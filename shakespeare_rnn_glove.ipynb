{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d028ef-83e9-4756-bc15-014ebb7331c3",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "* So let's start by importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0cd73c6-e4f1-41e4-8944-6a49e2d72e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk import tokenize\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13b4ac-ae35-4bf5-8470-dcc92a8b31f5",
   "metadata": {},
   "source": [
    "* Let's set random seed for grtting reproducible (deterministic) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800f9ef0-cc6d-4c7a-a769-af2ac1bc2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b449a9-132b-48ae-9b6f-f37cb025edc5",
   "metadata": {},
   "source": [
    "* Now let's open shakespeare file and see what are the unique characters in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfc0616-750b-4e1a-8898-9bfc7e9dfa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{';', '8', 'c', 'q', 'z', 'K', 'e', '0', '2', 'J', 'V', 'a', 'n', 'u', '.', 'o', 'g', 'U', 'Y', 'A', 'y', 'G', ']', '\\n', 'M', 'j', 'm', \"'\", 'F', 'W', 'I', 'E', 't', 'w', ',', '?', 'r', 'D', '-', '9', 'C', 'h', 'k', 'l', 'N', 'O', 'B', 'H', 'S', '5', '\"', 'f', 'R', 'd', 'v', 'p', '7', '[', 'P', '1', 'b', 'i', ' ', ':', 'L', 's', '4', 'x', '!', '6', '3', 'T'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/shakespeare-sonnets.txt\", \"r\") as f:\n",
    "    print(set(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e617aec-2c40-4e03-a383-4bc207802214",
   "metadata": {},
   "source": [
    "* As you can see from above result that there are many unwanted characters that should be removed\n",
    "* So now let's write a function to remove roman numerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ae6bb8-8a41-4877-a5ea-5745551cff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_roman_numerals(text):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    return re.sub(pattern, \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d07f8b-ef42-45cb-96a6-5ede34e928d9",
   "metadata": {},
   "source": [
    "* Let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1130ca41-201c-49a2-b080-06d235575e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' abcdefg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_roman_numerals(\"XI abcdefg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec79279-8d3c-4bc0-934d-11e233274af2",
   "metadata": {},
   "source": [
    "* As you can see that the function is working as expected\n",
    "* So now let's write a function to remove numerical numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddadba6f-c01e-48e4-b777-6862b12be076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numerical_numbers(text):\n",
    "    # Remove numbers\n",
    "    pattern = r'\\d+'\n",
    "    return re.sub(pattern, \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161c7f3-6956-4845-bd1f-95fb722d2a35",
   "metadata": {},
   "source": [
    "* Let's test this function now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "435cc586-f93d-41c4-8403-f372415206ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' abcdefg '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_numerical_numbers(\"1 abcdefg 1256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77e201-3b9a-48fb-b4fa-64c6b1034049",
   "metadata": {},
   "source": [
    "* As you can see from above results that the function is working as expected\n",
    "* Now let's write a function to remove spaces at start of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22441f1d-266e-438c-b166-7ae53ee28868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces_at_start(text):\n",
    "    return re.sub(r'^[ \\t]+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8cd95-0865-4e80-9f47-376d4356f8b3",
   "metadata": {},
   "source": [
    "* Let's test thei function now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc77522-4786-4aba-a948-7ddf714f89b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ananvbsbbs\\nshsbsnns'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_spaces_at_start(\"    ananvbsbbs\\n       shsbsnns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5b0d6-f293-450e-9464-61fc2fa7d44e",
   "metadata": {},
   "source": [
    "* As you can see the function is working as expected\n",
    "* Now let's write a function to remove square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b53a630-a753-4b26-b824-1a74e0dba8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_square_brackets(text):\n",
    "    return re.sub(r'\\[\\]', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9ba34-cf07-4353-9a38-3b9730ae6890",
   "metadata": {},
   "source": [
    "* Now let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5be954-6e2b-4c57-bc6c-32a762215360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' abcdefg \\n  annsjsnn'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_square_brackets(\"[] abcdefg \\n [] annsjsnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac032131-8937-4590-8d94-ff7dfcfb26fa",
   "metadata": {},
   "source": [
    "* As you can see that the function is correct\n",
    "* Shakespeare given corpus contains sonets (14 line poems)\n",
    "* We are going to add `<SONET_START>` token and `<SONET_END>` token at start and end of these poems (sonets) respectively\n",
    "* Adding these tokens will help our model to recognize these poem patterns\n",
    "* Now let's write a function to add sonet tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351fd10d-9719-459b-a0d7-5506c2f2e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sonet_tokens(text):\n",
    "    text = \"<SONET_START> \" + text[2:-1] + \"<SONET_END>\"\n",
    "    return re.sub(r'\\n\\n\\n\\n', \" <SONET_END>\\n<SONET_START> \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f7868-48f9-4e53-b8b9-5a8f8c832db5",
   "metadata": {},
   "source": [
    "* Now let's write a function which preprocesses data by using above functions, generate vocabulary and also generate sentence tokens broke down in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b2858f-0d7f-4960-91e7-f0e8471df367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(data_loc):\n",
    "    print( \"Reading txt file...\")\n",
    "    with open(f'{data_loc}', 'r') as f:\n",
    "        text = f.read()\n",
    "        text = clean_roman_numerals(text)\n",
    "        text = clean_numerical_numbers(text)\n",
    "        text = clean_square_brackets(text)\n",
    "        text = clean_spaces_at_start(text)\n",
    "        text = add_sonet_tokens(text)\n",
    "        pattern = r'''<[^>]+>|\\w+(?:'\\w+)?|\\n|[.,:;!?—()]'''\n",
    "        tokens = re.findall(pattern, text)   \n",
    "    # Count the word frequencies\n",
    "    word_freq = nltk.FreqDist(token.lower() for token in tokens)\n",
    "    print(f\"Found {len(word_freq.items())} unique words tokens.\")\n",
    "    # Get the most common words and build index_to_word and word_to_index vectors\n",
    "    vocabulary_size = len(word_freq.items())\n",
    "    vocab = word_freq.most_common(vocabulary_size)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    # index_to_word.append(unknown_token)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "    print(f\"The least frequent word in our vocabulary is '{vocab[-1][0]}' and appeared {vocab[-1][1]} times.\")\n",
    "    print(f\"Using vocabulary size {vocabulary_size}.\")\n",
    "    return vocab, vocabulary_size, index_to_word, word_to_index, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bb02d-7fba-4162-b649-d1e3967937f5",
   "metadata": {},
   "source": [
    "* Now let's apply above function on our shakespeare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0b92be0-41ee-482f-9322-dca0f98fcca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "Found 3195 unique words tokens.\n",
      "The least frequent word in our vocabulary is 'smothered' and appeared 1 times.\n",
      "Using vocabulary size 3195.\n"
     ]
    }
   ],
   "source": [
    "vocab, vocabulary_size, index_to_word, word_to_index, tokens = tokenize_sentences(\"./data/shakespeare-sonnets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c076f-66b1-44b3-8d90-d93a537dc8e8",
   "metadata": {},
   "source": [
    "* As you can see that we are getting 3195 unique words and we are using all (i.e 3195) words as vocabulary size\n",
    "* Let's print first 10 most common words from vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0e6cb4-009c-4fdc-bc45-9f4965924801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 2184),\n",
       " (',', 1642),\n",
       " ('.', 554),\n",
       " ('and', 495),\n",
       " ('the', 436),\n",
       " ('to', 414),\n",
       " ('my', 375),\n",
       " ('of', 372),\n",
       " ('that', 329),\n",
       " ('in', 329)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a634c-b48a-4d5b-ab0f-c3e5ad475a4a",
   "metadata": {},
   "source": [
    "* Let's see size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5771d6d-5123-4127-9606-72b75528aaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3195"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a1174-d0e0-4ff2-91e6-26cfc2c877ae",
   "metadata": {},
   "source": [
    "* Let's see first 10 words from index_to_word array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6085fa16-6d33-4a4f-b632-b77af7617b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', ',', '.', 'and', 'the', 'to', 'my', 'of', 'that', 'in']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f3ccf-0e36-4f8f-b75b-e83e9f14081e",
   "metadata": {},
   "source": [
    "* Now let's se word to index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18b7f47-7ab1-4b9d-b32e-365b9a3e1b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'and': 3,\n",
       " 'the': 4,\n",
       " 'to': 5,\n",
       " 'my': 6,\n",
       " 'of': 7,\n",
       " 'that': 8,\n",
       " 'in': 9,\n",
       " 'thy': 10,\n",
       " 'thou': 11,\n",
       " ';': 12,\n",
       " 'with': 13,\n",
       " 'love': 14,\n",
       " 'is': 15,\n",
       " 'for': 16,\n",
       " 'not': 17,\n",
       " 'me': 18,\n",
       " 'a': 19,\n",
       " 'but': 20,\n",
       " 'thee': 21,\n",
       " '<sonet_start>': 22,\n",
       " '<sonet_end>': 23,\n",
       " 'be': 24,\n",
       " 'so': 25,\n",
       " 'as': 26,\n",
       " 'all': 27,\n",
       " 'you': 28,\n",
       " 'his': 29,\n",
       " 'which': 30,\n",
       " 'when': 31,\n",
       " 'this': 32,\n",
       " 'it': 33,\n",
       " '?': 34,\n",
       " 'by': 35,\n",
       " 'your': 36,\n",
       " 'doth': 37,\n",
       " 'do': 38,\n",
       " 'from': 39,\n",
       " 'or': 40,\n",
       " 'on': 41,\n",
       " 'no': 42,\n",
       " 'have': 43,\n",
       " 'then': 44,\n",
       " 'are': 45,\n",
       " 'what': 46,\n",
       " 'if': 47,\n",
       " 'will': 48,\n",
       " 'more': 49,\n",
       " 'mine': 50,\n",
       " 'their': 51,\n",
       " 'shall': 52,\n",
       " 'sweet': 53,\n",
       " 'eyes': 54,\n",
       " 'time': 55,\n",
       " 'her': 56,\n",
       " 'they': 57,\n",
       " 'beauty': 58,\n",
       " 'yet': 59,\n",
       " 'nor': 60,\n",
       " 'o': 61,\n",
       " 'heart': 62,\n",
       " 'art': 63,\n",
       " 'than': 64,\n",
       " 'now': 65,\n",
       " 'fair': 66,\n",
       " 'should': 67,\n",
       " 'thine': 68,\n",
       " 'one': 69,\n",
       " 'can': 70,\n",
       " 'make': 71,\n",
       " 'he': 72,\n",
       " 'hath': 73,\n",
       " 'still': 74,\n",
       " 'how': 75,\n",
       " 'eye': 76,\n",
       " 'she': 77,\n",
       " 'him': 78,\n",
       " 'true': 79,\n",
       " 'where': 80,\n",
       " 'like': 81,\n",
       " 'am': 82,\n",
       " 'see': 83,\n",
       " 'though': 84,\n",
       " 'being': 85,\n",
       " 'those': 86,\n",
       " 'some': 87,\n",
       " 'such': 88,\n",
       " 'own': 89,\n",
       " 'were': 90,\n",
       " 'live': 91,\n",
       " 'may': 92,\n",
       " 'say': 93,\n",
       " 'dost': 94,\n",
       " 'who': 95,\n",
       " 'upon': 96,\n",
       " 'was': 97,\n",
       " ':': 98,\n",
       " \"love's\": 99,\n",
       " 'myself': 100,\n",
       " 'world': 101,\n",
       " 'praise': 102,\n",
       " 'might': 103,\n",
       " 'give': 104,\n",
       " 'most': 105,\n",
       " 'did': 106,\n",
       " 'let': 107,\n",
       " 'well': 108,\n",
       " 'at': 109,\n",
       " 'new': 110,\n",
       " 'why': 111,\n",
       " 'day': 112,\n",
       " 'best': 113,\n",
       " 'since': 114,\n",
       " 'truth': 115,\n",
       " 'even': 116,\n",
       " 'thus': 117,\n",
       " 'old': 118,\n",
       " 'show': 119,\n",
       " 'life': 120,\n",
       " 'self': 121,\n",
       " 'look': 122,\n",
       " 'every': 123,\n",
       " 'would': 124,\n",
       " 'night': 125,\n",
       " '!': 126,\n",
       " 'dear': 127,\n",
       " 'must': 128,\n",
       " 'false': 129,\n",
       " 'ill': 130,\n",
       " 'these': 131,\n",
       " 'better': 132,\n",
       " 'thyself': 133,\n",
       " 'worth': 134,\n",
       " 'made': 135,\n",
       " 'know': 136,\n",
       " 'part': 137,\n",
       " 'face': 138,\n",
       " 'whose': 139,\n",
       " 'nothing': 140,\n",
       " 'alone': 141,\n",
       " 'both': 142,\n",
       " 'our': 143,\n",
       " \"beauty's\": 144,\n",
       " 'too': 145,\n",
       " 'there': 146,\n",
       " 'hand': 147,\n",
       " 'thought': 148,\n",
       " 'away': 149,\n",
       " 'against': 150,\n",
       " 'therefore': 151,\n",
       " 'thoughts': 152,\n",
       " 'days': 153,\n",
       " 'much': 154,\n",
       " 'up': 155,\n",
       " 'sight': 156,\n",
       " 'hast': 157,\n",
       " 'them': 158,\n",
       " 'out': 159,\n",
       " 'tongue': 160,\n",
       " 'name': 161,\n",
       " 'never': 162,\n",
       " 'an': 163,\n",
       " 'tell': 164,\n",
       " 'age': 165,\n",
       " 'death': 166,\n",
       " 'each': 167,\n",
       " 'youth': 168,\n",
       " 'mind': 169,\n",
       " 'hate': 170,\n",
       " \"time's\": 171,\n",
       " 'other': 172,\n",
       " 'find': 173,\n",
       " 'had': 174,\n",
       " 'good': 175,\n",
       " 'muse': 176,\n",
       " 'far': 177,\n",
       " 'think': 178,\n",
       " 'dead': 179,\n",
       " 'we': 180,\n",
       " 'others': 181,\n",
       " 'men': 182,\n",
       " 'before': 183,\n",
       " 'verse': 184,\n",
       " 'come': 185,\n",
       " 'till': 186,\n",
       " 'poor': 187,\n",
       " 'friend': 188,\n",
       " 'proud': 189,\n",
       " 'gentle': 190,\n",
       " 'wilt': 191,\n",
       " 'state': 192,\n",
       " 'things': 193,\n",
       " 'lie': 194,\n",
       " 'lies': 195,\n",
       " 'use': 196,\n",
       " 'many': 197,\n",
       " 'none': 198,\n",
       " 'hold': 199,\n",
       " 'heaven': 200,\n",
       " 'whilst': 201,\n",
       " 'black': 202,\n",
       " 'full': 203,\n",
       " 'take': 204,\n",
       " 'die': 205,\n",
       " 'bear': 206,\n",
       " 'making': 207,\n",
       " 'hours': 208,\n",
       " 'looks': 209,\n",
       " 'prove': 210,\n",
       " 'change': 211,\n",
       " 'kind': 212,\n",
       " 'mayst': 213,\n",
       " 'whom': 214,\n",
       " 'long': 215,\n",
       " 'ever': 216,\n",
       " 'first': 217,\n",
       " 'seem': 218,\n",
       " 'tis': 219,\n",
       " 'pride': 220,\n",
       " 'woe': 221,\n",
       " 'desire': 222,\n",
       " 'bright': 223,\n",
       " 'within': 224,\n",
       " 'form': 225,\n",
       " 'shalt': 226,\n",
       " \"summer's\": 227,\n",
       " 'pleasure': 228,\n",
       " 'happy': 229,\n",
       " 'end': 230,\n",
       " 'rich': 231,\n",
       " 'knows': 232,\n",
       " 'earth': 233,\n",
       " 'sun': 234,\n",
       " 'grace': 235,\n",
       " 'seen': 236,\n",
       " 'two': 237,\n",
       " 'fire': 238,\n",
       " 'right': 239,\n",
       " 'spirit': 240,\n",
       " 'shame': 241,\n",
       " 'glass': 242,\n",
       " 'another': 243,\n",
       " 'great': 244,\n",
       " 'nature': 245,\n",
       " 'leave': 246,\n",
       " 'th': 247,\n",
       " 'place': 248,\n",
       " 'could': 249,\n",
       " 'any': 250,\n",
       " 'call': 251,\n",
       " 'past': 252,\n",
       " 'again': 253,\n",
       " 'after': 254,\n",
       " 'blessed': 255,\n",
       " 'pen': 256,\n",
       " 'write': 257,\n",
       " 'once': 258,\n",
       " 'words': 259,\n",
       " 'loving': 260,\n",
       " 'deeds': 261,\n",
       " 'faults': 262,\n",
       " 'although': 263,\n",
       " 'loves': 264,\n",
       " 'cannot': 265,\n",
       " 'found': 266,\n",
       " 'hell': 267,\n",
       " 'soul': 268,\n",
       " 'angel': 269,\n",
       " 'pity': 270,\n",
       " 'treasure': 271,\n",
       " 'back': 272,\n",
       " 'beauteous': 273,\n",
       " 'gone': 274,\n",
       " 'lives': 275,\n",
       " 'times': 276,\n",
       " 'strong': 277,\n",
       " 'keep': 278,\n",
       " 'without': 279,\n",
       " 'decay': 280,\n",
       " 'store': 281,\n",
       " 'yourself': 282,\n",
       " 'stay': 283,\n",
       " 'lose': 284,\n",
       " 'mistress': 285,\n",
       " 'thing': 286,\n",
       " 'power': 287,\n",
       " 'memory': 288,\n",
       " 'cruel': 289,\n",
       " 'brow': 290,\n",
       " 'deep': 291,\n",
       " 'child': 292,\n",
       " 'blood': 293,\n",
       " 'lovely': 294,\n",
       " 'gives': 295,\n",
       " 'summer': 296,\n",
       " 'leaves': 297,\n",
       " 'flowers': 298,\n",
       " 'joy': 299,\n",
       " 'fear': 300,\n",
       " 'grow': 301,\n",
       " 'save': 302,\n",
       " 'stand': 303,\n",
       " 'lines': 304,\n",
       " 'skill': 305,\n",
       " 'glory': 306,\n",
       " 'view': 307,\n",
       " 'makes': 308,\n",
       " 'disgrace': 309,\n",
       " 'll': 310,\n",
       " 'speak': 311,\n",
       " 'loss': 312,\n",
       " 'bring': 313,\n",
       " 'delight': 314,\n",
       " 'lest': 315,\n",
       " 'sake': 316,\n",
       " 'thence': 317,\n",
       " 'sin': 318,\n",
       " 'bad': 319,\n",
       " 'tender': 320,\n",
       " 'fresh': 321,\n",
       " 'waste': 322,\n",
       " 'excuse': 323,\n",
       " 'cold': 324,\n",
       " 'canst': 325,\n",
       " 'very': 326,\n",
       " \"o'er\": 327,\n",
       " 'ten': 328,\n",
       " 'living': 329,\n",
       " 'parts': 330,\n",
       " 'sing': 331,\n",
       " 'ah': 332,\n",
       " 'behold': 333,\n",
       " 'white': 334,\n",
       " 'honor': 335,\n",
       " 'outward': 336,\n",
       " 'less': 337,\n",
       " 'wide': 338,\n",
       " 'worst': 339,\n",
       " 'itself': 340,\n",
       " 'breast': 341,\n",
       " 'put': 342,\n",
       " 'rest': 343,\n",
       " 'shadow': 344,\n",
       " 'sad': 345,\n",
       " 'forth': 346,\n",
       " 'roses': 347,\n",
       " 'steal': 348,\n",
       " 'whether': 349,\n",
       " 'straight': 350,\n",
       " \"heart's\": 351,\n",
       " 'set': 352,\n",
       " 'breath': 353,\n",
       " 'virtue': 354,\n",
       " 'seeing': 355,\n",
       " 'three': 356,\n",
       " 'lips': 357,\n",
       " 'foul': 358,\n",
       " 'swear': 359,\n",
       " 'rose': 360,\n",
       " \"world's\": 361,\n",
       " 'only': 362,\n",
       " 'due': 363,\n",
       " 'through': 364,\n",
       " 'despite': 365,\n",
       " \"nature's\": 366,\n",
       " 'having': 367,\n",
       " 'dwell': 368,\n",
       " 'same': 369,\n",
       " 'ere': 370,\n",
       " 'shouldst': 371,\n",
       " 'light': 372,\n",
       " 'head': 373,\n",
       " 'under': 374,\n",
       " 'way': 375,\n",
       " 'unless': 376,\n",
       " 'music': 377,\n",
       " 'hear': 378,\n",
       " 'sweets': 379,\n",
       " 'war': 380,\n",
       " 'gainst': 381,\n",
       " 'go': 382,\n",
       " 'themselves': 383,\n",
       " 'hence': 384,\n",
       " 'longer': 385,\n",
       " 'eternal': 386,\n",
       " 'judgment': 387,\n",
       " 'fortune': 388,\n",
       " 'shows': 389,\n",
       " 'wherefore': 390,\n",
       " 'rhyme': 391,\n",
       " 'compare': 392,\n",
       " 'sometime': 393,\n",
       " 'swift': 394,\n",
       " 'wrong': 395,\n",
       " 'young': 396,\n",
       " 'man': 397,\n",
       " \"heaven's\": 398,\n",
       " 'dumb': 399,\n",
       " 'speaking': 400,\n",
       " 'writ': 401,\n",
       " 'wit': 402,\n",
       " 'book': 403,\n",
       " 'hope': 404,\n",
       " 'blind': 405,\n",
       " 'return': 406,\n",
       " 'wealth': 407,\n",
       " 'precious': 408,\n",
       " 'while': 409,\n",
       " 'enough': 410,\n",
       " 'argument': 411,\n",
       " 'heavy': 412,\n",
       " 'dull': 413,\n",
       " 'present': 414,\n",
       " 'says': 415,\n",
       " 'care': 416,\n",
       " 'side': 417,\n",
       " 'strange': 418,\n",
       " 'slave': 419,\n",
       " 'been': 420,\n",
       " 'painting': 421,\n",
       " 'taught': 422,\n",
       " 'fears': 423,\n",
       " 'born': 424,\n",
       " 'spent': 425,\n",
       " 'sick': 426,\n",
       " 'reason': 427,\n",
       " 'mad': 428,\n",
       " 'fairest': 429,\n",
       " 'ornament': 430,\n",
       " 'spring': 431,\n",
       " 'else': 432,\n",
       " 'tomb': 433,\n",
       " 'calls': 434,\n",
       " 'wrinkles': 435,\n",
       " 'golden': 436,\n",
       " 'spend': 437,\n",
       " 'lend': 438,\n",
       " 'play': 439,\n",
       " 'winter': 440,\n",
       " 'quite': 441,\n",
       " 'everywhere': 442,\n",
       " 'left': 443,\n",
       " 'gracious': 444,\n",
       " 'mortal': 445,\n",
       " \"lov'st\": 446,\n",
       " 'chide': 447,\n",
       " 'shape': 448,\n",
       " 'beloved': 449,\n",
       " 'least': 450,\n",
       " 'fast': 451,\n",
       " 'year': 452,\n",
       " 'gave': 453,\n",
       " 'gift': 454,\n",
       " 'barren': 455,\n",
       " 'green': 456,\n",
       " 'yours': 457,\n",
       " 'rage': 458,\n",
       " 'stars': 459,\n",
       " 'evil': 460,\n",
       " 'oft': 461,\n",
       " 'read': 462,\n",
       " 'doom': 463,\n",
       " 'date': 464,\n",
       " 'holds': 465,\n",
       " 'wish': 466,\n",
       " 'painted': 467,\n",
       " 'keeps': 468,\n",
       " 'believe': 469,\n",
       " 'graces': 470,\n",
       " 'antique': 471,\n",
       " 'short': 472,\n",
       " 'lived': 473,\n",
       " 'hue': 474,\n",
       " \"men's\": 475,\n",
       " 'fell': 476,\n",
       " 'purpose': 477,\n",
       " 'truly': 478,\n",
       " 'trust': 479,\n",
       " 'strength': 480,\n",
       " 'wherein': 481,\n",
       " 'done': 482,\n",
       " 'boast': 483,\n",
       " 'buried': 484,\n",
       " 'merit': 485,\n",
       " 'worthy': 486,\n",
       " 'scope': 487,\n",
       " 'moan': 488,\n",
       " \"stol'n\": 489,\n",
       " 'loved': 490,\n",
       " 'birth': 491,\n",
       " 'hide': 492,\n",
       " 'base': 493,\n",
       " 'grief': 494,\n",
       " 'sorrow': 495,\n",
       " 'tears': 496,\n",
       " 'canker': 497,\n",
       " 'sweetest': 498,\n",
       " 'needs': 499,\n",
       " 'spite': 500,\n",
       " 'comfort': 501,\n",
       " 'subject': 502,\n",
       " 'into': 503,\n",
       " 'invention': 504,\n",
       " 'pain': 505,\n",
       " 't': 506,\n",
       " 'absence': 507,\n",
       " 'kill': 508,\n",
       " 'large': 509,\n",
       " 'water': 510,\n",
       " 'slow': 511,\n",
       " 'motion': 512,\n",
       " 'down': 513,\n",
       " 'told': 514,\n",
       " 'took': 515,\n",
       " 'need': 516,\n",
       " 'cheek': 517,\n",
       " 'worse': 518,\n",
       " 'near': 519,\n",
       " 'win': 520,\n",
       " 'flower': 521,\n",
       " 'faith': 522,\n",
       " 'tied': 523,\n",
       " 'rank': 524,\n",
       " 'brain': 525,\n",
       " 'saw': 526,\n",
       " 'grew': 527,\n",
       " 'brand': 528,\n",
       " 'cure': 529,\n",
       " 'increase': 530,\n",
       " 'abundance': 531,\n",
       " 'sum': 532,\n",
       " 'count': 533,\n",
       " 'prime': 534,\n",
       " 'remembered': 535,\n",
       " 'single': 536,\n",
       " 'image': 537,\n",
       " 'lends': 538,\n",
       " 'free': 539,\n",
       " 'given': 540,\n",
       " 'unused': 541,\n",
       " 'used': 542,\n",
       " 'frame': 543,\n",
       " 'substance': 544,\n",
       " \"death's\": 545,\n",
       " 'worms': 546,\n",
       " 'weary': 547,\n",
       " 'mark': 548,\n",
       " 'song': 549,\n",
       " 'behind': 550,\n",
       " 'grant': 551,\n",
       " 'rude': 552,\n",
       " 'heat': 553,\n",
       " 'beauties': 554,\n",
       " 'scythe': 555,\n",
       " 'here': 556,\n",
       " 'lease': 557,\n",
       " 'pluck': 558,\n",
       " 'methinks': 559,\n",
       " 'minutes': 560,\n",
       " 'grows': 561,\n",
       " 'inward': 562,\n",
       " 'high': 563,\n",
       " 'numbers': 564,\n",
       " 'touches': 565,\n",
       " \"ne'er\": 566,\n",
       " 'buds': 567,\n",
       " 'shade': 568,\n",
       " 'crime': 569,\n",
       " 'woman': 570,\n",
       " 'rehearse': 571,\n",
       " 'rare': 572,\n",
       " 'air': 573,\n",
       " 'babe': 574,\n",
       " 'body': 575,\n",
       " 'turns': 576,\n",
       " 'want': 577,\n",
       " 'public': 578,\n",
       " 'forgot': 579,\n",
       " 'removed': 580,\n",
       " 'bare': 581,\n",
       " 'respect': 582,\n",
       " 'dare': 583,\n",
       " 'toil': 584,\n",
       " 'tired': 585,\n",
       " \"body's\": 586,\n",
       " 'jewel': 587,\n",
       " 'farther': 588,\n",
       " 'please': 589,\n",
       " 'clouds': 590,\n",
       " 'contented': 591,\n",
       " 'break': 592,\n",
       " 'lack': 593,\n",
       " 'account': 594,\n",
       " 'hearts': 595,\n",
       " 'holy': 596,\n",
       " 'appear': 597,\n",
       " 'grown': 598,\n",
       " 'sovereign': 599,\n",
       " 'shine': 600,\n",
       " 'hour': 601,\n",
       " 'wound': 602,\n",
       " 'sense': 603,\n",
       " 'thief': 604,\n",
       " 'help': 605,\n",
       " 'report': 606,\n",
       " 'crowned': 607,\n",
       " 'leisure': 608,\n",
       " 'blame': 609,\n",
       " 'taste': 610,\n",
       " 'absent': 611,\n",
       " 'years': 612,\n",
       " 'said': 613,\n",
       " \"know'st\": 614,\n",
       " 'gain': 615,\n",
       " 'sleep': 616,\n",
       " 'sea': 617,\n",
       " 'soon': 618,\n",
       " 'assured': 619,\n",
       " 'seek': 620,\n",
       " 'groan': 621,\n",
       " 'former': 622,\n",
       " 'second': 623,\n",
       " 'knife': 624,\n",
       " 'brass': 625,\n",
       " 'forsworn': 626,\n",
       " \"another's\": 627,\n",
       " 'tongues': 628,\n",
       " 'add': 629,\n",
       " 'smell': 630,\n",
       " 'weeds': 631,\n",
       " 'common': 632,\n",
       " 'suspect': 633,\n",
       " 'thinking': 634,\n",
       " 'lost': 635,\n",
       " 'knowing': 636,\n",
       " 'cheeks': 637,\n",
       " 'story': 638,\n",
       " 'praises': 639,\n",
       " 'above': 640,\n",
       " 'wretched': 641,\n",
       " 'errors': 642,\n",
       " 'lays': 643,\n",
       " 'despair': 644,\n",
       " 'red': 645,\n",
       " 'just': 646,\n",
       " 'catch': 647,\n",
       " 'laid': 648,\n",
       " 'oaths': 649,\n",
       " 'bath': 650,\n",
       " 'decease': 651,\n",
       " 'heir': 652,\n",
       " 'flame': 653,\n",
       " 'grave': 654,\n",
       " 'weed': 655,\n",
       " 'answer': 656,\n",
       " 'repair': 657,\n",
       " 'posterity': 658,\n",
       " \"mother's\": 659,\n",
       " 'april': 660,\n",
       " 'windows': 661,\n",
       " 'abuse': 662,\n",
       " 'audit': 663,\n",
       " 'work': 664,\n",
       " 'confounds': 665,\n",
       " 'effect': 666,\n",
       " 'distilled': 667,\n",
       " \"winter's\": 668,\n",
       " 'pay': 669,\n",
       " 'happier': 670,\n",
       " 'conquest': 671,\n",
       " 'lo': 672,\n",
       " 'heavenly': 673,\n",
       " 'son': 674,\n",
       " 'ear': 675,\n",
       " 'husband': 676,\n",
       " 'seeming': 677,\n",
       " 'weep': 678,\n",
       " 'kept': 679,\n",
       " \"murd'rous\": 680,\n",
       " 'fairer': 681,\n",
       " \"grow'st\": 682,\n",
       " 'brave': 683,\n",
       " 'borne': 684,\n",
       " 'among': 685,\n",
       " 'defense': 686,\n",
       " 'takes': 687,\n",
       " 'fall': 688,\n",
       " 'seasons': 689,\n",
       " 'brief': 690,\n",
       " 'rain': 691,\n",
       " 'princes': 692,\n",
       " 'knowledge': 693,\n",
       " 'constant': 694,\n",
       " 'wouldst': 695,\n",
       " 'nought': 696,\n",
       " 'whereon': 697,\n",
       " 'height': 698,\n",
       " 'wear': 699,\n",
       " 'conceit': 700,\n",
       " 'bloody': 701,\n",
       " 'tyrant': 702,\n",
       " 'means': 703,\n",
       " 'maiden': 704,\n",
       " 'virtuous': 705,\n",
       " 'drawn': 706,\n",
       " 'filled': 707,\n",
       " 'half': 708,\n",
       " 'number': 709,\n",
       " 'shake': 710,\n",
       " 'hot': 711,\n",
       " 'complexion': 712,\n",
       " 'course': 713,\n",
       " 'blunt': 714,\n",
       " 'burn': 715,\n",
       " 'forbid': 716,\n",
       " 'draw': 717,\n",
       " \"woman's\": 718,\n",
       " \"women's\": 719,\n",
       " 'wert': 720,\n",
       " 'moon': 721,\n",
       " 'fixed': 722,\n",
       " 'cover': 723,\n",
       " 'bearing': 724,\n",
       " \"gav'st\": 725,\n",
       " 'burden': 726,\n",
       " 'expressed': 727,\n",
       " 'therein': 728,\n",
       " 'cunning': 729,\n",
       " 'favor': 730,\n",
       " 'triumph': 731,\n",
       " 'frown': 732,\n",
       " 'thousand': 733,\n",
       " 'razed': 734,\n",
       " 'duty': 735,\n",
       " 'witness': 736,\n",
       " 'wanting': 737,\n",
       " 'star': 738,\n",
       " 'haste': 739,\n",
       " 'bed': 740,\n",
       " 'looking': 741,\n",
       " 'blot': 742,\n",
       " 'flatter': 743,\n",
       " 'daily': 744,\n",
       " 'friends': 745,\n",
       " 'almost': 746,\n",
       " 'haply': 747,\n",
       " 'expense': 748,\n",
       " 'growing': 749,\n",
       " 'brought': 750,\n",
       " 'style': 751,\n",
       " 'basest': 752,\n",
       " 'ride': 753,\n",
       " 'west': 754,\n",
       " 'alack': 755,\n",
       " 'masked': 756,\n",
       " 'stain': 757,\n",
       " 'didst': 758,\n",
       " 'bears': 759,\n",
       " 'cross': 760,\n",
       " 'thorns': 761,\n",
       " 'amiss': 762,\n",
       " 'sins': 763,\n",
       " 'fault': 764,\n",
       " 'lawful': 765,\n",
       " 'plea': 766,\n",
       " 'twain': 767,\n",
       " 'remain': 768,\n",
       " 'evermore': 769,\n",
       " \"fortune's\": 770,\n",
       " 'dearest': 771,\n",
       " 'despised': 772,\n",
       " 'vulgar': 773,\n",
       " 'outlive': 774,\n",
       " 'manners': 775,\n",
       " 'torment': 776,\n",
       " 'greater': 777,\n",
       " 'injury': 778,\n",
       " 'foes': 779,\n",
       " 'pretty': 780,\n",
       " 'because': 781,\n",
       " 'approve': 782,\n",
       " 'losing': 783,\n",
       " 'flattery': 784,\n",
       " 'dark': 785,\n",
       " 'shadows': 786,\n",
       " 'clear': 787,\n",
       " 'flesh': 788,\n",
       " 'matter': 789,\n",
       " 'foot': 790,\n",
       " 'picture': 791,\n",
       " 'sure': 792,\n",
       " 'chest': 793,\n",
       " 'whence': 794,\n",
       " 'prize': 795,\n",
       " 'pass': 796,\n",
       " 'greet': 797,\n",
       " 'reasons': 798,\n",
       " 'desert': 799,\n",
       " 'cause': 800,\n",
       " 'teach': 801,\n",
       " 'speed': 802,\n",
       " 'pace': 803,\n",
       " 'tend': 804,\n",
       " 'odor': 805,\n",
       " 'hang': 806,\n",
       " 'gilded': 807,\n",
       " 'stone': 808,\n",
       " 'quick': 809,\n",
       " 'record': 810,\n",
       " 'edge': 811,\n",
       " 'appetite': 812,\n",
       " 'today': 813,\n",
       " 'feeding': 814,\n",
       " 'tomorrow': 815,\n",
       " 'fill': 816,\n",
       " 'ocean': 817,\n",
       " 'shore': 818,\n",
       " 'thrice': 819,\n",
       " 'fool': 820,\n",
       " 'thinks': 821,\n",
       " 'god': 822,\n",
       " 'control': 823,\n",
       " 'doing': 824,\n",
       " 'five': 825,\n",
       " 'character': 826,\n",
       " 'wonder': 827,\n",
       " 'main': 828,\n",
       " 'stands': 829,\n",
       " 'mock': 830,\n",
       " 'home': 831,\n",
       " 'elsewhere': 832,\n",
       " 'cost': 833,\n",
       " 'advantage': 834,\n",
       " 'action': 835,\n",
       " 'steel': 836,\n",
       " 'simple': 837,\n",
       " 'infection': 838,\n",
       " 'bastard': 839,\n",
       " 'mend': 840,\n",
       " 'guess': 841,\n",
       " 'flies': 842,\n",
       " 'mourn': 843,\n",
       " 'vile': 844,\n",
       " 'line': 845,\n",
       " 'birds': 846,\n",
       " 'ground': 847,\n",
       " 'pine': 848,\n",
       " 'eternity': 849,\n",
       " 'acquaintance': 850,\n",
       " 'got': 851,\n",
       " 'double': 852,\n",
       " 'works': 853,\n",
       " 'fame': 854,\n",
       " 'sail': 855,\n",
       " 'anew': 856,\n",
       " 'silence': 857,\n",
       " 'spirits': 858,\n",
       " 'bonds': 859,\n",
       " 'vow': 860,\n",
       " 'general': 861,\n",
       " \"what's\": 862,\n",
       " 'turn': 863,\n",
       " 'seemed': 864,\n",
       " 'growth': 865,\n",
       " 'color': 866,\n",
       " 'sinful': 867,\n",
       " 'turned': 868,\n",
       " 'constancy': 869,\n",
       " 'confined': 870,\n",
       " 'boy': 871,\n",
       " 'proved': 872,\n",
       " 'proof': 873,\n",
       " 'receives': 874,\n",
       " 'plague': 875,\n",
       " 'minds': 876,\n",
       " 'taken': 877,\n",
       " 'whereto': 878,\n",
       " 'unkind': 879,\n",
       " 'fingers': 880,\n",
       " 'kiss': 881,\n",
       " 'perjured': 882,\n",
       " 'came': 883,\n",
       " 'corrupt': 884,\n",
       " 'vainly': 885,\n",
       " 'fiend': 886,\n",
       " 'sworn': 887,\n",
       " 'conscience': 888,\n",
       " 'creatures': 889,\n",
       " 'thereby': 890,\n",
       " 'riper': 891,\n",
       " 'contracted': 892,\n",
       " 'bud': 893,\n",
       " 'content': 894,\n",
       " 'churl': 895,\n",
       " \"mak'st\": 896,\n",
       " 'eat': 897,\n",
       " 'winters': 898,\n",
       " 'besiege': 899,\n",
       " 'tattered': 900,\n",
       " 'small': 901,\n",
       " 'held': 902,\n",
       " 'lusty': 903,\n",
       " 'mother': 904,\n",
       " 'womb': 905,\n",
       " 'husbandry': 906,\n",
       " 'fond': 907,\n",
       " 'stop': 908,\n",
       " 'niggard': 909,\n",
       " 'bounteous': 910,\n",
       " 'usurer': 911,\n",
       " 'deceive': 912,\n",
       " 'gaze': 913,\n",
       " 'tyrants': 914,\n",
       " 'leads': 915,\n",
       " 'hideous': 916,\n",
       " 'sap': 917,\n",
       " 'checked': 918,\n",
       " 'bareness': 919,\n",
       " 'pent': 920,\n",
       " 'walls': 921,\n",
       " 'remembrance': 922,\n",
       " 'meet': 923,\n",
       " 'willing': 924,\n",
       " 'breed': 925,\n",
       " 'depart': 926,\n",
       " 'serving': 927,\n",
       " 'sacred': 928,\n",
       " 'majesty': 929,\n",
       " 'steep': 930,\n",
       " 'resembling': 931,\n",
       " 'attending': 932,\n",
       " 'pilgrimage': 933,\n",
       " 'pitch': 934,\n",
       " 'fore': 935,\n",
       " 'converted': 936,\n",
       " 'unlooked': 937,\n",
       " 'delights': 938,\n",
       " \"receiv'st\": 939,\n",
       " 'concord': 940,\n",
       " 'sounds': 941,\n",
       " 'married': 942,\n",
       " 'sweetly': 943,\n",
       " 'mutual': 944,\n",
       " 'pleasing': 945,\n",
       " 'note': 946,\n",
       " 'speechless': 947,\n",
       " 'sings': 948,\n",
       " 'wail': 949,\n",
       " 'widow': 950,\n",
       " 'bosom': 951,\n",
       " 'himself': 952,\n",
       " 'commits': 953,\n",
       " 'deny': 954,\n",
       " 'possessed': 955,\n",
       " 'chief': 956,\n",
       " 'presence': 957,\n",
       " 'folly': 958,\n",
       " 'bounty': 959,\n",
       " 'copy': 960,\n",
       " 'clock': 961,\n",
       " 'tells': 962,\n",
       " 'violet': 963,\n",
       " 'lofty': 964,\n",
       " 'canopy': 965,\n",
       " 'question': 966,\n",
       " 'wastes': 967,\n",
       " 'forsake': 968,\n",
       " 'coming': 969,\n",
       " 'prepare': 970,\n",
       " 'issue': 971,\n",
       " 'father': 972,\n",
       " 'wind': 973,\n",
       " 'thrive': 974,\n",
       " 'everything': 975,\n",
       " 'perfection': 976,\n",
       " 'little': 977,\n",
       " 'huge': 978,\n",
       " 'stage': 979,\n",
       " 'influence': 980,\n",
       " 'comment': 981,\n",
       " 'youthful': 982,\n",
       " 'inconstant': 983,\n",
       " 'sets': 984,\n",
       " 'wasteful': 985,\n",
       " 'fortify': 986,\n",
       " 'counterfeit': 987,\n",
       " 'pencil': 988,\n",
       " 'neither': 989,\n",
       " 'deserts': 990,\n",
       " 'poet': 991,\n",
       " 'faces': 992,\n",
       " \"poet's\": 993,\n",
       " 'alive': 994,\n",
       " 'twice': 995,\n",
       " 'winds': 996,\n",
       " 'shines': 997,\n",
       " 'often': 998,\n",
       " 'gold': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cf859-6351-4613-8aea-6a57b80e2f39",
   "metadata": {},
   "source": [
    "* Let's see first 10 tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e1dcda-54ad-4f19-a8df-48b6821986a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SONET_START>',\n",
       " 'From',\n",
       " 'fairest',\n",
       " 'creatures',\n",
       " 'we',\n",
       " 'desire',\n",
       " 'increase',\n",
       " ',',\n",
       " '\\n',\n",
       " 'That']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bb980-7865-4f03-a22d-f1e44ab662e2",
   "metadata": {},
   "source": [
    "* To generate sequences of data to train and test on we are going to use n_grams\n",
    "* So let's write a function which generates n sequence n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4137fc3-3f0b-4e60-b6d3-43f2c94d689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences_n_grams(word_tokens, n):\n",
    "    slices = [word_tokens[i:] for i in range(n)]\n",
    "    return [list(i) for i in zip(*slices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e5e7b-9bd9-4d09-852b-673e59c1824d",
   "metadata": {},
   "source": [
    "* Now let's test this function\n",
    "* Let's first test it with n_grams = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a688fe6-6918-4e5f-9a0a-a4a5db91d0ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<SONET_START>', 'From'],\n",
       " ['From', 'fairest'],\n",
       " ['fairest', 'creatures'],\n",
       " ['creatures', 'we'],\n",
       " ['we', 'desire'],\n",
       " ['desire', 'increase'],\n",
       " ['increase', ','],\n",
       " [',', '\\n'],\n",
       " ['\\n', 'That'],\n",
       " ['That', 'thereby']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequences_n_grams(tokens, 2)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c8d83-f5f6-4623-a41c-bd14b4d893c2",
   "metadata": {},
   "source": [
    "* We are getting correct results\n",
    "* Now let's also test it with n_grams = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a138996-7f1d-45a8-81d7-0dcfcac77d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<SONET_START>', 'From', 'fairest'],\n",
       " ['From', 'fairest', 'creatures'],\n",
       " ['fairest', 'creatures', 'we'],\n",
       " ['creatures', 'we', 'desire'],\n",
       " ['we', 'desire', 'increase'],\n",
       " ['desire', 'increase', ','],\n",
       " ['increase', ',', '\\n'],\n",
       " [',', '\\n', 'That'],\n",
       " ['\\n', 'That', 'thereby'],\n",
       " ['That', 'thereby', \"beauty's\"]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sequences_n_grams(tokens, 3)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f850b9c-ad0f-48b2-88f3-d25ddcb103d6",
   "metadata": {},
   "source": [
    "* As you can see that we are getting correct results\n",
    "* Note that we are adding padding at start of an array instead of end\n",
    "* Now let's write a function to generate feature and label data containing n_grams of length 2, 8, 16, 32 and 48 generated array having 10000 samples per n_gram\n",
    "* Here we are creating data with different n_gram length because with n_gram = 2 model will be able to learn word mappings then with n_grams = 8 model will be able to learn relation ship between phrases then with n_gram = 16 model will be abe to learn relationship between sentences and with ngrams = 32 and 48 model will be able to learn relatonship between poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ff46f6a-4d84-4180-a385-94e5a83791ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_n_grams(word_token, max_seq_len_per_n_gram=10000, n_grams=[2, 8, 16, 32, 48]):\n",
    "    features_data = []\n",
    "    target_data = []\n",
    "    for current_n_gram in n_grams:\n",
    "        n_gram_data = generate_sequences_n_grams(word_token, current_n_gram)\n",
    "        selected_seuences = np.random.choice(len(n_gram_data), size=max_seq_len_per_n_gram)\n",
    "        for sequence_number in selected_seuences:\n",
    "            features_data.append(n_gram_data[sequence_number][:-1])\n",
    "            target_data.append(n_gram_data[sequence_number][1:])\n",
    "    return features_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1773f-6648-4a99-8d98-3e4792d659f1",
   "metadata": {},
   "source": [
    "* Let's now generate data with above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55db1546-6941-45e2-bc90-12b04e913d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_features_data, generated_target_data = generate_data_n_grams(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d11bf9-0e1d-42c5-9600-b29a872ae4a1",
   "metadata": {},
   "source": [
    "* Let's see first 10  and last 10 generated features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bce6d3e0-c7ba-4d07-aa5f-16c791b5e762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['of'],\n",
       "  ['<SONET_END>'],\n",
       "  [','],\n",
       "  ['deemed'],\n",
       "  ['will'],\n",
       "  ['living'],\n",
       "  ['wish'],\n",
       "  ['?'],\n",
       "  ['besiege'],\n",
       "  ['me']],\n",
       " [[\"war's\",\n",
       "   'quick',\n",
       "   'fire',\n",
       "   'shall',\n",
       "   'burn',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'living',\n",
       "   'record',\n",
       "   'of',\n",
       "   'your',\n",
       "   'memory',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Gainst',\n",
       "   'death',\n",
       "   'and',\n",
       "   'all',\n",
       "   'oblivious',\n",
       "   'enmity',\n",
       "   '\\n',\n",
       "   'Shall',\n",
       "   'you',\n",
       "   'pace',\n",
       "   'forth',\n",
       "   ';',\n",
       "   'your',\n",
       "   'praise',\n",
       "   'shall',\n",
       "   'still',\n",
       "   'find',\n",
       "   'room',\n",
       "   '\\n',\n",
       "   'Even',\n",
       "   'in',\n",
       "   'the',\n",
       "   'eyes',\n",
       "   'of',\n",
       "   'all',\n",
       "   'posterity',\n",
       "   '\\n',\n",
       "   'That',\n",
       "   'wear',\n",
       "   'this',\n",
       "   'world',\n",
       "   'out',\n",
       "   'to'],\n",
       "  ['thy',\n",
       "   'love',\n",
       "   ',',\n",
       "   'and',\n",
       "   'love',\n",
       "   'that',\n",
       "   'still',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'And',\n",
       "   'then',\n",
       "   'thou',\n",
       "   'lovest',\n",
       "   'me',\n",
       "   ',',\n",
       "   'for',\n",
       "   'my',\n",
       "   'name',\n",
       "   'is',\n",
       "   'Will',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'Thou',\n",
       "   'blind',\n",
       "   'fool',\n",
       "   ',',\n",
       "   'Love',\n",
       "   ',',\n",
       "   'what',\n",
       "   'dost',\n",
       "   'thou',\n",
       "   'to',\n",
       "   'mine',\n",
       "   'eyes',\n",
       "   '\\n',\n",
       "   'That',\n",
       "   'they',\n",
       "   'behold',\n",
       "   'and',\n",
       "   'see',\n",
       "   'not',\n",
       "   'what',\n",
       "   'they',\n",
       "   'see',\n",
       "   '?'],\n",
       "  ['And',\n",
       "   'worse',\n",
       "   'essays',\n",
       "   'proved',\n",
       "   'thee',\n",
       "   'my',\n",
       "   'best',\n",
       "   'of',\n",
       "   'love',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Now',\n",
       "   'all',\n",
       "   'is',\n",
       "   'done',\n",
       "   ',',\n",
       "   'have',\n",
       "   'what',\n",
       "   'shall',\n",
       "   'have',\n",
       "   'no',\n",
       "   'end',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Mine',\n",
       "   'appetite',\n",
       "   'never',\n",
       "   'more',\n",
       "   'will',\n",
       "   'grind',\n",
       "   '\\n',\n",
       "   'On',\n",
       "   'newer',\n",
       "   'proof',\n",
       "   ',',\n",
       "   'to',\n",
       "   'try',\n",
       "   'an',\n",
       "   'older',\n",
       "   'friend',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'A',\n",
       "   'god',\n",
       "   'in',\n",
       "   'love',\n",
       "   ','],\n",
       "  ['luck',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Of',\n",
       "   'plagues',\n",
       "   ',',\n",
       "   'of',\n",
       "   'dearths',\n",
       "   ',',\n",
       "   'or',\n",
       "   'seasons',\n",
       "   'quality',\n",
       "   ';',\n",
       "   '\\n',\n",
       "   'Nor',\n",
       "   'can',\n",
       "   'fortune',\n",
       "   'to',\n",
       "   'brief',\n",
       "   'minutes',\n",
       "   'tell',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Pointing',\n",
       "   'to',\n",
       "   'each',\n",
       "   'his',\n",
       "   'thunder',\n",
       "   ',',\n",
       "   'rain',\n",
       "   ',',\n",
       "   'and',\n",
       "   'wind',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Or',\n",
       "   'say',\n",
       "   'with',\n",
       "   'princes',\n",
       "   'if',\n",
       "   'it',\n",
       "   'shall',\n",
       "   'go',\n",
       "   'well',\n",
       "   '\\n',\n",
       "   'By',\n",
       "   'oft'],\n",
       "  ['babe',\n",
       "   'from',\n",
       "   'faring',\n",
       "   'ill',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Presume',\n",
       "   'not',\n",
       "   'on',\n",
       "   'thy',\n",
       "   'heart',\n",
       "   'when',\n",
       "   'mine',\n",
       "   'is',\n",
       "   'slain',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Thou',\n",
       "   \"gav'st\",\n",
       "   'me',\n",
       "   'thine',\n",
       "   'not',\n",
       "   'to',\n",
       "   'give',\n",
       "   'back',\n",
       "   'again',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'As',\n",
       "   'an',\n",
       "   'unperfect',\n",
       "   'actor',\n",
       "   'on',\n",
       "   'the',\n",
       "   'stage',\n",
       "   '\\n',\n",
       "   'Who',\n",
       "   'with',\n",
       "   'his',\n",
       "   'fear',\n",
       "   'is',\n",
       "   'put',\n",
       "   'beside',\n",
       "   'his',\n",
       "   'part'],\n",
       "  ['That',\n",
       "   'due',\n",
       "   'to',\n",
       "   'thee',\n",
       "   'which',\n",
       "   'thou',\n",
       "   \"deserv'st\",\n",
       "   'alone',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'O',\n",
       "   'absence',\n",
       "   ',',\n",
       "   'what',\n",
       "   'a',\n",
       "   'torment',\n",
       "   'wouldst',\n",
       "   'thou',\n",
       "   'prove',\n",
       "   '\\n',\n",
       "   'Were',\n",
       "   'it',\n",
       "   'not',\n",
       "   'thy',\n",
       "   'sour',\n",
       "   'leisure',\n",
       "   'gave',\n",
       "   'sweet',\n",
       "   'leave',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'entertain',\n",
       "   'the',\n",
       "   'time',\n",
       "   'with',\n",
       "   'thoughts',\n",
       "   'of',\n",
       "   'love',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Which',\n",
       "   'time',\n",
       "   'and',\n",
       "   'thoughts',\n",
       "   'so',\n",
       "   'sweetly',\n",
       "   'doth'],\n",
       "  ['nor',\n",
       "   'grows',\n",
       "   'with',\n",
       "   'heat',\n",
       "   'nor',\n",
       "   'drowns',\n",
       "   'with',\n",
       "   'showers',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'this',\n",
       "   'witness',\n",
       "   'call',\n",
       "   'the',\n",
       "   'fools',\n",
       "   'of',\n",
       "   'time',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Which',\n",
       "   'die',\n",
       "   'for',\n",
       "   'goodness',\n",
       "   'who',\n",
       "   'have',\n",
       "   'lived',\n",
       "   'for',\n",
       "   'crime',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'Were',\n",
       "   't',\n",
       "   'aught',\n",
       "   'to',\n",
       "   'me',\n",
       "   'bore',\n",
       "   'the',\n",
       "   'canopy',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'With',\n",
       "   'my',\n",
       "   'extern',\n",
       "   'the'],\n",
       "  ['\\n',\n",
       "   'How',\n",
       "   'would',\n",
       "   'thy',\n",
       "   \"shadow's\",\n",
       "   'form',\n",
       "   'form',\n",
       "   'happy',\n",
       "   'show',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'the',\n",
       "   'clear',\n",
       "   'day',\n",
       "   'with',\n",
       "   'thy',\n",
       "   'much',\n",
       "   'clearer',\n",
       "   'light',\n",
       "   '\\n',\n",
       "   'When',\n",
       "   'to',\n",
       "   'unseeing',\n",
       "   'eyes',\n",
       "   'thy',\n",
       "   'shade',\n",
       "   'shines',\n",
       "   'so',\n",
       "   '!',\n",
       "   '\\n',\n",
       "   'How',\n",
       "   'would',\n",
       "   ',',\n",
       "   'say',\n",
       "   ',',\n",
       "   'mine',\n",
       "   'eyes',\n",
       "   'be',\n",
       "   'blessed',\n",
       "   'made',\n",
       "   '\\n',\n",
       "   'By',\n",
       "   'looking',\n",
       "   'on',\n",
       "   'thee',\n",
       "   'in',\n",
       "   'the'],\n",
       "  ['Of',\n",
       "   'him',\n",
       "   ',',\n",
       "   'myself',\n",
       "   ',',\n",
       "   'and',\n",
       "   'thee',\n",
       "   'am',\n",
       "   'forsaken',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'A',\n",
       "   'torment',\n",
       "   'thrice',\n",
       "   'threefold',\n",
       "   'thus',\n",
       "   'to',\n",
       "   'be',\n",
       "   'crossed',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Prison',\n",
       "   'my',\n",
       "   'heart',\n",
       "   'in',\n",
       "   'thy',\n",
       "   'steel',\n",
       "   \"bosom's\",\n",
       "   'ward',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'But',\n",
       "   'then',\n",
       "   'my',\n",
       "   \"friend's\",\n",
       "   'heart',\n",
       "   'let',\n",
       "   'my',\n",
       "   'poor',\n",
       "   'heart',\n",
       "   'bail',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   \"Whoe'er\",\n",
       "   'keeps',\n",
       "   'me',\n",
       "   ','],\n",
       "  ['And',\n",
       "   'live',\n",
       "   'no',\n",
       "   'more',\n",
       "   'to',\n",
       "   'shame',\n",
       "   'nor',\n",
       "   'me',\n",
       "   'nor',\n",
       "   'you',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'For',\n",
       "   'am',\n",
       "   'shamed',\n",
       "   'by',\n",
       "   'that',\n",
       "   'which',\n",
       "   'bring',\n",
       "   'forth',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'And',\n",
       "   'so',\n",
       "   'should',\n",
       "   'you',\n",
       "   ',',\n",
       "   'to',\n",
       "   'love',\n",
       "   'things',\n",
       "   'nothing',\n",
       "   'worth',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'That',\n",
       "   'time',\n",
       "   'of',\n",
       "   'year',\n",
       "   'thou',\n",
       "   'mayst',\n",
       "   'in',\n",
       "   'me',\n",
       "   'behold',\n",
       "   '\\n',\n",
       "   'When']])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_features_data[:10], generated_features_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcb563-098e-45d8-acb3-0d86f55c1f2c",
   "metadata": {},
   "source": [
    "* Let's see first 10 and last 10 generated target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18835d16-286b-45e3-a29e-423afbd48f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['mine'],\n",
       "  ['\\n'],\n",
       "  ['\\n'],\n",
       "  ['\\n'],\n",
       "  ['give'],\n",
       "  ['record'],\n",
       "  [','],\n",
       "  ['\\n'],\n",
       "  ['thy'],\n",
       "  ['\\n']],\n",
       " [['quick',\n",
       "   'fire',\n",
       "   'shall',\n",
       "   'burn',\n",
       "   '\\n',\n",
       "   'The',\n",
       "   'living',\n",
       "   'record',\n",
       "   'of',\n",
       "   'your',\n",
       "   'memory',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Gainst',\n",
       "   'death',\n",
       "   'and',\n",
       "   'all',\n",
       "   'oblivious',\n",
       "   'enmity',\n",
       "   '\\n',\n",
       "   'Shall',\n",
       "   'you',\n",
       "   'pace',\n",
       "   'forth',\n",
       "   ';',\n",
       "   'your',\n",
       "   'praise',\n",
       "   'shall',\n",
       "   'still',\n",
       "   'find',\n",
       "   'room',\n",
       "   '\\n',\n",
       "   'Even',\n",
       "   'in',\n",
       "   'the',\n",
       "   'eyes',\n",
       "   'of',\n",
       "   'all',\n",
       "   'posterity',\n",
       "   '\\n',\n",
       "   'That',\n",
       "   'wear',\n",
       "   'this',\n",
       "   'world',\n",
       "   'out',\n",
       "   'to',\n",
       "   'the'],\n",
       "  ['love',\n",
       "   ',',\n",
       "   'and',\n",
       "   'love',\n",
       "   'that',\n",
       "   'still',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'And',\n",
       "   'then',\n",
       "   'thou',\n",
       "   'lovest',\n",
       "   'me',\n",
       "   ',',\n",
       "   'for',\n",
       "   'my',\n",
       "   'name',\n",
       "   'is',\n",
       "   'Will',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'Thou',\n",
       "   'blind',\n",
       "   'fool',\n",
       "   ',',\n",
       "   'Love',\n",
       "   ',',\n",
       "   'what',\n",
       "   'dost',\n",
       "   'thou',\n",
       "   'to',\n",
       "   'mine',\n",
       "   'eyes',\n",
       "   '\\n',\n",
       "   'That',\n",
       "   'they',\n",
       "   'behold',\n",
       "   'and',\n",
       "   'see',\n",
       "   'not',\n",
       "   'what',\n",
       "   'they',\n",
       "   'see',\n",
       "   '?',\n",
       "   '\\n'],\n",
       "  ['worse',\n",
       "   'essays',\n",
       "   'proved',\n",
       "   'thee',\n",
       "   'my',\n",
       "   'best',\n",
       "   'of',\n",
       "   'love',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Now',\n",
       "   'all',\n",
       "   'is',\n",
       "   'done',\n",
       "   ',',\n",
       "   'have',\n",
       "   'what',\n",
       "   'shall',\n",
       "   'have',\n",
       "   'no',\n",
       "   'end',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Mine',\n",
       "   'appetite',\n",
       "   'never',\n",
       "   'more',\n",
       "   'will',\n",
       "   'grind',\n",
       "   '\\n',\n",
       "   'On',\n",
       "   'newer',\n",
       "   'proof',\n",
       "   ',',\n",
       "   'to',\n",
       "   'try',\n",
       "   'an',\n",
       "   'older',\n",
       "   'friend',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'A',\n",
       "   'god',\n",
       "   'in',\n",
       "   'love',\n",
       "   ',',\n",
       "   'to'],\n",
       "  [',',\n",
       "   '\\n',\n",
       "   'Of',\n",
       "   'plagues',\n",
       "   ',',\n",
       "   'of',\n",
       "   'dearths',\n",
       "   ',',\n",
       "   'or',\n",
       "   'seasons',\n",
       "   'quality',\n",
       "   ';',\n",
       "   '\\n',\n",
       "   'Nor',\n",
       "   'can',\n",
       "   'fortune',\n",
       "   'to',\n",
       "   'brief',\n",
       "   'minutes',\n",
       "   'tell',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Pointing',\n",
       "   'to',\n",
       "   'each',\n",
       "   'his',\n",
       "   'thunder',\n",
       "   ',',\n",
       "   'rain',\n",
       "   ',',\n",
       "   'and',\n",
       "   'wind',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Or',\n",
       "   'say',\n",
       "   'with',\n",
       "   'princes',\n",
       "   'if',\n",
       "   'it',\n",
       "   'shall',\n",
       "   'go',\n",
       "   'well',\n",
       "   '\\n',\n",
       "   'By',\n",
       "   'oft',\n",
       "   'predict'],\n",
       "  ['from',\n",
       "   'faring',\n",
       "   'ill',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Presume',\n",
       "   'not',\n",
       "   'on',\n",
       "   'thy',\n",
       "   'heart',\n",
       "   'when',\n",
       "   'mine',\n",
       "   'is',\n",
       "   'slain',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Thou',\n",
       "   \"gav'st\",\n",
       "   'me',\n",
       "   'thine',\n",
       "   'not',\n",
       "   'to',\n",
       "   'give',\n",
       "   'back',\n",
       "   'again',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'As',\n",
       "   'an',\n",
       "   'unperfect',\n",
       "   'actor',\n",
       "   'on',\n",
       "   'the',\n",
       "   'stage',\n",
       "   '\\n',\n",
       "   'Who',\n",
       "   'with',\n",
       "   'his',\n",
       "   'fear',\n",
       "   'is',\n",
       "   'put',\n",
       "   'beside',\n",
       "   'his',\n",
       "   'part',\n",
       "   ','],\n",
       "  ['due',\n",
       "   'to',\n",
       "   'thee',\n",
       "   'which',\n",
       "   'thou',\n",
       "   \"deserv'st\",\n",
       "   'alone',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'O',\n",
       "   'absence',\n",
       "   ',',\n",
       "   'what',\n",
       "   'a',\n",
       "   'torment',\n",
       "   'wouldst',\n",
       "   'thou',\n",
       "   'prove',\n",
       "   '\\n',\n",
       "   'Were',\n",
       "   'it',\n",
       "   'not',\n",
       "   'thy',\n",
       "   'sour',\n",
       "   'leisure',\n",
       "   'gave',\n",
       "   'sweet',\n",
       "   'leave',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'entertain',\n",
       "   'the',\n",
       "   'time',\n",
       "   'with',\n",
       "   'thoughts',\n",
       "   'of',\n",
       "   'love',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Which',\n",
       "   'time',\n",
       "   'and',\n",
       "   'thoughts',\n",
       "   'so',\n",
       "   'sweetly',\n",
       "   'doth',\n",
       "   'deceive'],\n",
       "  ['grows',\n",
       "   'with',\n",
       "   'heat',\n",
       "   'nor',\n",
       "   'drowns',\n",
       "   'with',\n",
       "   'showers',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'this',\n",
       "   'witness',\n",
       "   'call',\n",
       "   'the',\n",
       "   'fools',\n",
       "   'of',\n",
       "   'time',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'Which',\n",
       "   'die',\n",
       "   'for',\n",
       "   'goodness',\n",
       "   'who',\n",
       "   'have',\n",
       "   'lived',\n",
       "   'for',\n",
       "   'crime',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'Were',\n",
       "   't',\n",
       "   'aught',\n",
       "   'to',\n",
       "   'me',\n",
       "   'bore',\n",
       "   'the',\n",
       "   'canopy',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'With',\n",
       "   'my',\n",
       "   'extern',\n",
       "   'the',\n",
       "   'outward'],\n",
       "  ['How',\n",
       "   'would',\n",
       "   'thy',\n",
       "   \"shadow's\",\n",
       "   'form',\n",
       "   'form',\n",
       "   'happy',\n",
       "   'show',\n",
       "   '\\n',\n",
       "   'To',\n",
       "   'the',\n",
       "   'clear',\n",
       "   'day',\n",
       "   'with',\n",
       "   'thy',\n",
       "   'much',\n",
       "   'clearer',\n",
       "   'light',\n",
       "   '\\n',\n",
       "   'When',\n",
       "   'to',\n",
       "   'unseeing',\n",
       "   'eyes',\n",
       "   'thy',\n",
       "   'shade',\n",
       "   'shines',\n",
       "   'so',\n",
       "   '!',\n",
       "   '\\n',\n",
       "   'How',\n",
       "   'would',\n",
       "   ',',\n",
       "   'say',\n",
       "   ',',\n",
       "   'mine',\n",
       "   'eyes',\n",
       "   'be',\n",
       "   'blessed',\n",
       "   'made',\n",
       "   '\\n',\n",
       "   'By',\n",
       "   'looking',\n",
       "   'on',\n",
       "   'thee',\n",
       "   'in',\n",
       "   'the',\n",
       "   'living'],\n",
       "  ['him',\n",
       "   ',',\n",
       "   'myself',\n",
       "   ',',\n",
       "   'and',\n",
       "   'thee',\n",
       "   'am',\n",
       "   'forsaken',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'A',\n",
       "   'torment',\n",
       "   'thrice',\n",
       "   'threefold',\n",
       "   'thus',\n",
       "   'to',\n",
       "   'be',\n",
       "   'crossed',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'Prison',\n",
       "   'my',\n",
       "   'heart',\n",
       "   'in',\n",
       "   'thy',\n",
       "   'steel',\n",
       "   \"bosom's\",\n",
       "   'ward',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'But',\n",
       "   'then',\n",
       "   'my',\n",
       "   \"friend's\",\n",
       "   'heart',\n",
       "   'let',\n",
       "   'my',\n",
       "   'poor',\n",
       "   'heart',\n",
       "   'bail',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   \"Whoe'er\",\n",
       "   'keeps',\n",
       "   'me',\n",
       "   ',',\n",
       "   'let'],\n",
       "  ['live',\n",
       "   'no',\n",
       "   'more',\n",
       "   'to',\n",
       "   'shame',\n",
       "   'nor',\n",
       "   'me',\n",
       "   'nor',\n",
       "   'you',\n",
       "   '.',\n",
       "   '\\n',\n",
       "   'For',\n",
       "   'am',\n",
       "   'shamed',\n",
       "   'by',\n",
       "   'that',\n",
       "   'which',\n",
       "   'bring',\n",
       "   'forth',\n",
       "   ',',\n",
       "   '\\n',\n",
       "   'And',\n",
       "   'so',\n",
       "   'should',\n",
       "   'you',\n",
       "   ',',\n",
       "   'to',\n",
       "   'love',\n",
       "   'things',\n",
       "   'nothing',\n",
       "   'worth',\n",
       "   '.',\n",
       "   '<SONET_END>',\n",
       "   '\\n',\n",
       "   '<SONET_START>',\n",
       "   'That',\n",
       "   'time',\n",
       "   'of',\n",
       "   'year',\n",
       "   'thou',\n",
       "   'mayst',\n",
       "   'in',\n",
       "   'me',\n",
       "   'behold',\n",
       "   '\\n',\n",
       "   'When',\n",
       "   'yellow']])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_target_data[:10], generated_target_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3540b8-b380-4b00-96a9-aea3b92d279f",
   "metadata": {},
   "source": [
    "* As we already have vocabulary, now let's write a function to tokenize generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a92f15-7c1e-48df-8269-8069fe0e1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_generated_data(data, word_to_index):\n",
    "    tokenized_data = []\n",
    "    for sequence in data:\n",
    "        tokenized_data.append([word_to_index[i.lower()] for i in sequence])\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2925181-80d2-4a8b-98ce-4bd0e9bdc839",
   "metadata": {},
   "source": [
    "* Now let's tokenize features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "381e38cf-547b-4e37-a3a4-6be6e8a4f4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_features_data = tokenize_generated_data(generated_features_data, word_to_index)\n",
    "len(tokenized_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4181890-8238-4f87-bed6-e0aed0a621f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[7], [23], [1], [1299], [48], [329], [466], [34], [899], [18]],\n",
       " [[2048,\n",
       "   809,\n",
       "   238,\n",
       "   52,\n",
       "   715,\n",
       "   0,\n",
       "   4,\n",
       "   329,\n",
       "   810,\n",
       "   7,\n",
       "   36,\n",
       "   288,\n",
       "   2,\n",
       "   0,\n",
       "   381,\n",
       "   166,\n",
       "   3,\n",
       "   27,\n",
       "   2049,\n",
       "   2050,\n",
       "   0,\n",
       "   52,\n",
       "   28,\n",
       "   803,\n",
       "   346,\n",
       "   12,\n",
       "   36,\n",
       "   102,\n",
       "   52,\n",
       "   74,\n",
       "   173,\n",
       "   2051,\n",
       "   0,\n",
       "   116,\n",
       "   9,\n",
       "   4,\n",
       "   54,\n",
       "   7,\n",
       "   27,\n",
       "   658,\n",
       "   0,\n",
       "   8,\n",
       "   699,\n",
       "   32,\n",
       "   101,\n",
       "   159,\n",
       "   5],\n",
       "  [10,\n",
       "   14,\n",
       "   1,\n",
       "   3,\n",
       "   14,\n",
       "   8,\n",
       "   74,\n",
       "   1,\n",
       "   0,\n",
       "   3,\n",
       "   44,\n",
       "   11,\n",
       "   2995,\n",
       "   18,\n",
       "   1,\n",
       "   16,\n",
       "   6,\n",
       "   161,\n",
       "   15,\n",
       "   48,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   11,\n",
       "   405,\n",
       "   820,\n",
       "   1,\n",
       "   14,\n",
       "   1,\n",
       "   46,\n",
       "   94,\n",
       "   11,\n",
       "   5,\n",
       "   50,\n",
       "   54,\n",
       "   0,\n",
       "   8,\n",
       "   57,\n",
       "   333,\n",
       "   3,\n",
       "   83,\n",
       "   17,\n",
       "   46,\n",
       "   57,\n",
       "   83,\n",
       "   34],\n",
       "  [3,\n",
       "   518,\n",
       "   2660,\n",
       "   872,\n",
       "   21,\n",
       "   6,\n",
       "   113,\n",
       "   7,\n",
       "   14,\n",
       "   2,\n",
       "   0,\n",
       "   65,\n",
       "   27,\n",
       "   15,\n",
       "   482,\n",
       "   1,\n",
       "   43,\n",
       "   46,\n",
       "   52,\n",
       "   43,\n",
       "   42,\n",
       "   230,\n",
       "   2,\n",
       "   0,\n",
       "   50,\n",
       "   812,\n",
       "   162,\n",
       "   49,\n",
       "   48,\n",
       "   2661,\n",
       "   0,\n",
       "   41,\n",
       "   1318,\n",
       "   873,\n",
       "   1,\n",
       "   5,\n",
       "   2662,\n",
       "   163,\n",
       "   2663,\n",
       "   188,\n",
       "   1,\n",
       "   0,\n",
       "   19,\n",
       "   822,\n",
       "   9,\n",
       "   14,\n",
       "   1],\n",
       "  [1568,\n",
       "   1,\n",
       "   0,\n",
       "   7,\n",
       "   1569,\n",
       "   1,\n",
       "   7,\n",
       "   1570,\n",
       "   1,\n",
       "   40,\n",
       "   689,\n",
       "   1571,\n",
       "   12,\n",
       "   0,\n",
       "   60,\n",
       "   70,\n",
       "   388,\n",
       "   5,\n",
       "   690,\n",
       "   560,\n",
       "   164,\n",
       "   1,\n",
       "   0,\n",
       "   1572,\n",
       "   5,\n",
       "   167,\n",
       "   29,\n",
       "   1573,\n",
       "   1,\n",
       "   691,\n",
       "   1,\n",
       "   3,\n",
       "   973,\n",
       "   1,\n",
       "   0,\n",
       "   40,\n",
       "   93,\n",
       "   13,\n",
       "   692,\n",
       "   47,\n",
       "   33,\n",
       "   52,\n",
       "   382,\n",
       "   108,\n",
       "   0,\n",
       "   35,\n",
       "   461],\n",
       "  [574,\n",
       "   39,\n",
       "   1668,\n",
       "   130,\n",
       "   2,\n",
       "   0,\n",
       "   1669,\n",
       "   17,\n",
       "   41,\n",
       "   10,\n",
       "   62,\n",
       "   31,\n",
       "   50,\n",
       "   15,\n",
       "   1022,\n",
       "   2,\n",
       "   0,\n",
       "   11,\n",
       "   725,\n",
       "   18,\n",
       "   68,\n",
       "   17,\n",
       "   5,\n",
       "   104,\n",
       "   272,\n",
       "   253,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   26,\n",
       "   163,\n",
       "   1670,\n",
       "   1671,\n",
       "   41,\n",
       "   4,\n",
       "   979,\n",
       "   0,\n",
       "   95,\n",
       "   13,\n",
       "   29,\n",
       "   300,\n",
       "   15,\n",
       "   342,\n",
       "   1023,\n",
       "   29,\n",
       "   137],\n",
       "  [8,\n",
       "   363,\n",
       "   5,\n",
       "   21,\n",
       "   30,\n",
       "   11,\n",
       "   1866,\n",
       "   141,\n",
       "   2,\n",
       "   0,\n",
       "   61,\n",
       "   507,\n",
       "   1,\n",
       "   46,\n",
       "   19,\n",
       "   776,\n",
       "   695,\n",
       "   11,\n",
       "   210,\n",
       "   0,\n",
       "   90,\n",
       "   33,\n",
       "   17,\n",
       "   10,\n",
       "   1113,\n",
       "   608,\n",
       "   453,\n",
       "   53,\n",
       "   246,\n",
       "   0,\n",
       "   5,\n",
       "   1867,\n",
       "   4,\n",
       "   55,\n",
       "   13,\n",
       "   152,\n",
       "   7,\n",
       "   14,\n",
       "   1,\n",
       "   0,\n",
       "   30,\n",
       "   55,\n",
       "   3,\n",
       "   152,\n",
       "   25,\n",
       "   943,\n",
       "   37],\n",
       "  [60,\n",
       "   561,\n",
       "   13,\n",
       "   553,\n",
       "   60,\n",
       "   2864,\n",
       "   13,\n",
       "   1239,\n",
       "   2,\n",
       "   0,\n",
       "   5,\n",
       "   32,\n",
       "   736,\n",
       "   251,\n",
       "   4,\n",
       "   2865,\n",
       "   7,\n",
       "   55,\n",
       "   1,\n",
       "   0,\n",
       "   30,\n",
       "   205,\n",
       "   16,\n",
       "   1341,\n",
       "   95,\n",
       "   43,\n",
       "   473,\n",
       "   16,\n",
       "   569,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   90,\n",
       "   506,\n",
       "   1110,\n",
       "   5,\n",
       "   18,\n",
       "   1353,\n",
       "   4,\n",
       "   965,\n",
       "   1,\n",
       "   0,\n",
       "   13,\n",
       "   6,\n",
       "   2866,\n",
       "   4],\n",
       "  [0,\n",
       "   75,\n",
       "   124,\n",
       "   10,\n",
       "   1897,\n",
       "   225,\n",
       "   225,\n",
       "   229,\n",
       "   119,\n",
       "   0,\n",
       "   5,\n",
       "   4,\n",
       "   787,\n",
       "   112,\n",
       "   13,\n",
       "   10,\n",
       "   154,\n",
       "   1135,\n",
       "   372,\n",
       "   0,\n",
       "   31,\n",
       "   5,\n",
       "   1898,\n",
       "   54,\n",
       "   10,\n",
       "   568,\n",
       "   997,\n",
       "   25,\n",
       "   126,\n",
       "   0,\n",
       "   75,\n",
       "   124,\n",
       "   1,\n",
       "   93,\n",
       "   1,\n",
       "   50,\n",
       "   54,\n",
       "   24,\n",
       "   255,\n",
       "   135,\n",
       "   0,\n",
       "   35,\n",
       "   741,\n",
       "   41,\n",
       "   21,\n",
       "   9,\n",
       "   4],\n",
       "  [7,\n",
       "   78,\n",
       "   1,\n",
       "   100,\n",
       "   1,\n",
       "   3,\n",
       "   21,\n",
       "   82,\n",
       "   2961,\n",
       "   1,\n",
       "   0,\n",
       "   19,\n",
       "   776,\n",
       "   819,\n",
       "   2962,\n",
       "   117,\n",
       "   5,\n",
       "   24,\n",
       "   2963,\n",
       "   2,\n",
       "   0,\n",
       "   2964,\n",
       "   6,\n",
       "   62,\n",
       "   9,\n",
       "   10,\n",
       "   836,\n",
       "   1033,\n",
       "   2965,\n",
       "   1,\n",
       "   0,\n",
       "   20,\n",
       "   44,\n",
       "   6,\n",
       "   1081,\n",
       "   62,\n",
       "   107,\n",
       "   6,\n",
       "   187,\n",
       "   62,\n",
       "   1238,\n",
       "   2,\n",
       "   0,\n",
       "   2966,\n",
       "   468,\n",
       "   18,\n",
       "   1],\n",
       "  [3,\n",
       "   91,\n",
       "   42,\n",
       "   49,\n",
       "   5,\n",
       "   241,\n",
       "   60,\n",
       "   18,\n",
       "   60,\n",
       "   28,\n",
       "   2,\n",
       "   0,\n",
       "   16,\n",
       "   82,\n",
       "   2233,\n",
       "   35,\n",
       "   8,\n",
       "   30,\n",
       "   313,\n",
       "   346,\n",
       "   1,\n",
       "   0,\n",
       "   3,\n",
       "   25,\n",
       "   67,\n",
       "   28,\n",
       "   1,\n",
       "   5,\n",
       "   14,\n",
       "   193,\n",
       "   140,\n",
       "   134,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   8,\n",
       "   55,\n",
       "   7,\n",
       "   452,\n",
       "   11,\n",
       "   213,\n",
       "   9,\n",
       "   18,\n",
       "   333,\n",
       "   0,\n",
       "   31]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_features_data[:10], tokenized_features_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5dfb22-e9e0-47d7-8784-6115e80cec95",
   "metadata": {},
   "source": [
    "* As you can see that we are getting correct tokenized features data\n",
    "* Now let's do the same for target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fbbf218-b370-4913-92a6-ead9404b9c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_target_data = tokenize_generated_data(generated_target_data, word_to_index)\n",
    "len(tokenized_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecf6cb37-efc6-49d3-9bd9-eaeb7b20f994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[50], [0], [0], [0], [104], [810], [1], [0], [10], [0]],\n",
       " [[809,\n",
       "   238,\n",
       "   52,\n",
       "   715,\n",
       "   0,\n",
       "   4,\n",
       "   329,\n",
       "   810,\n",
       "   7,\n",
       "   36,\n",
       "   288,\n",
       "   2,\n",
       "   0,\n",
       "   381,\n",
       "   166,\n",
       "   3,\n",
       "   27,\n",
       "   2049,\n",
       "   2050,\n",
       "   0,\n",
       "   52,\n",
       "   28,\n",
       "   803,\n",
       "   346,\n",
       "   12,\n",
       "   36,\n",
       "   102,\n",
       "   52,\n",
       "   74,\n",
       "   173,\n",
       "   2051,\n",
       "   0,\n",
       "   116,\n",
       "   9,\n",
       "   4,\n",
       "   54,\n",
       "   7,\n",
       "   27,\n",
       "   658,\n",
       "   0,\n",
       "   8,\n",
       "   699,\n",
       "   32,\n",
       "   101,\n",
       "   159,\n",
       "   5,\n",
       "   4],\n",
       "  [14,\n",
       "   1,\n",
       "   3,\n",
       "   14,\n",
       "   8,\n",
       "   74,\n",
       "   1,\n",
       "   0,\n",
       "   3,\n",
       "   44,\n",
       "   11,\n",
       "   2995,\n",
       "   18,\n",
       "   1,\n",
       "   16,\n",
       "   6,\n",
       "   161,\n",
       "   15,\n",
       "   48,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   11,\n",
       "   405,\n",
       "   820,\n",
       "   1,\n",
       "   14,\n",
       "   1,\n",
       "   46,\n",
       "   94,\n",
       "   11,\n",
       "   5,\n",
       "   50,\n",
       "   54,\n",
       "   0,\n",
       "   8,\n",
       "   57,\n",
       "   333,\n",
       "   3,\n",
       "   83,\n",
       "   17,\n",
       "   46,\n",
       "   57,\n",
       "   83,\n",
       "   34,\n",
       "   0],\n",
       "  [518,\n",
       "   2660,\n",
       "   872,\n",
       "   21,\n",
       "   6,\n",
       "   113,\n",
       "   7,\n",
       "   14,\n",
       "   2,\n",
       "   0,\n",
       "   65,\n",
       "   27,\n",
       "   15,\n",
       "   482,\n",
       "   1,\n",
       "   43,\n",
       "   46,\n",
       "   52,\n",
       "   43,\n",
       "   42,\n",
       "   230,\n",
       "   2,\n",
       "   0,\n",
       "   50,\n",
       "   812,\n",
       "   162,\n",
       "   49,\n",
       "   48,\n",
       "   2661,\n",
       "   0,\n",
       "   41,\n",
       "   1318,\n",
       "   873,\n",
       "   1,\n",
       "   5,\n",
       "   2662,\n",
       "   163,\n",
       "   2663,\n",
       "   188,\n",
       "   1,\n",
       "   0,\n",
       "   19,\n",
       "   822,\n",
       "   9,\n",
       "   14,\n",
       "   1,\n",
       "   5],\n",
       "  [1,\n",
       "   0,\n",
       "   7,\n",
       "   1569,\n",
       "   1,\n",
       "   7,\n",
       "   1570,\n",
       "   1,\n",
       "   40,\n",
       "   689,\n",
       "   1571,\n",
       "   12,\n",
       "   0,\n",
       "   60,\n",
       "   70,\n",
       "   388,\n",
       "   5,\n",
       "   690,\n",
       "   560,\n",
       "   164,\n",
       "   1,\n",
       "   0,\n",
       "   1572,\n",
       "   5,\n",
       "   167,\n",
       "   29,\n",
       "   1573,\n",
       "   1,\n",
       "   691,\n",
       "   1,\n",
       "   3,\n",
       "   973,\n",
       "   1,\n",
       "   0,\n",
       "   40,\n",
       "   93,\n",
       "   13,\n",
       "   692,\n",
       "   47,\n",
       "   33,\n",
       "   52,\n",
       "   382,\n",
       "   108,\n",
       "   0,\n",
       "   35,\n",
       "   461,\n",
       "   1574],\n",
       "  [39,\n",
       "   1668,\n",
       "   130,\n",
       "   2,\n",
       "   0,\n",
       "   1669,\n",
       "   17,\n",
       "   41,\n",
       "   10,\n",
       "   62,\n",
       "   31,\n",
       "   50,\n",
       "   15,\n",
       "   1022,\n",
       "   2,\n",
       "   0,\n",
       "   11,\n",
       "   725,\n",
       "   18,\n",
       "   68,\n",
       "   17,\n",
       "   5,\n",
       "   104,\n",
       "   272,\n",
       "   253,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   26,\n",
       "   163,\n",
       "   1670,\n",
       "   1671,\n",
       "   41,\n",
       "   4,\n",
       "   979,\n",
       "   0,\n",
       "   95,\n",
       "   13,\n",
       "   29,\n",
       "   300,\n",
       "   15,\n",
       "   342,\n",
       "   1023,\n",
       "   29,\n",
       "   137,\n",
       "   1],\n",
       "  [363,\n",
       "   5,\n",
       "   21,\n",
       "   30,\n",
       "   11,\n",
       "   1866,\n",
       "   141,\n",
       "   2,\n",
       "   0,\n",
       "   61,\n",
       "   507,\n",
       "   1,\n",
       "   46,\n",
       "   19,\n",
       "   776,\n",
       "   695,\n",
       "   11,\n",
       "   210,\n",
       "   0,\n",
       "   90,\n",
       "   33,\n",
       "   17,\n",
       "   10,\n",
       "   1113,\n",
       "   608,\n",
       "   453,\n",
       "   53,\n",
       "   246,\n",
       "   0,\n",
       "   5,\n",
       "   1867,\n",
       "   4,\n",
       "   55,\n",
       "   13,\n",
       "   152,\n",
       "   7,\n",
       "   14,\n",
       "   1,\n",
       "   0,\n",
       "   30,\n",
       "   55,\n",
       "   3,\n",
       "   152,\n",
       "   25,\n",
       "   943,\n",
       "   37,\n",
       "   912],\n",
       "  [561,\n",
       "   13,\n",
       "   553,\n",
       "   60,\n",
       "   2864,\n",
       "   13,\n",
       "   1239,\n",
       "   2,\n",
       "   0,\n",
       "   5,\n",
       "   32,\n",
       "   736,\n",
       "   251,\n",
       "   4,\n",
       "   2865,\n",
       "   7,\n",
       "   55,\n",
       "   1,\n",
       "   0,\n",
       "   30,\n",
       "   205,\n",
       "   16,\n",
       "   1341,\n",
       "   95,\n",
       "   43,\n",
       "   473,\n",
       "   16,\n",
       "   569,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   90,\n",
       "   506,\n",
       "   1110,\n",
       "   5,\n",
       "   18,\n",
       "   1353,\n",
       "   4,\n",
       "   965,\n",
       "   1,\n",
       "   0,\n",
       "   13,\n",
       "   6,\n",
       "   2866,\n",
       "   4,\n",
       "   336],\n",
       "  [75,\n",
       "   124,\n",
       "   10,\n",
       "   1897,\n",
       "   225,\n",
       "   225,\n",
       "   229,\n",
       "   119,\n",
       "   0,\n",
       "   5,\n",
       "   4,\n",
       "   787,\n",
       "   112,\n",
       "   13,\n",
       "   10,\n",
       "   154,\n",
       "   1135,\n",
       "   372,\n",
       "   0,\n",
       "   31,\n",
       "   5,\n",
       "   1898,\n",
       "   54,\n",
       "   10,\n",
       "   568,\n",
       "   997,\n",
       "   25,\n",
       "   126,\n",
       "   0,\n",
       "   75,\n",
       "   124,\n",
       "   1,\n",
       "   93,\n",
       "   1,\n",
       "   50,\n",
       "   54,\n",
       "   24,\n",
       "   255,\n",
       "   135,\n",
       "   0,\n",
       "   35,\n",
       "   741,\n",
       "   41,\n",
       "   21,\n",
       "   9,\n",
       "   4,\n",
       "   329],\n",
       "  [78,\n",
       "   1,\n",
       "   100,\n",
       "   1,\n",
       "   3,\n",
       "   21,\n",
       "   82,\n",
       "   2961,\n",
       "   1,\n",
       "   0,\n",
       "   19,\n",
       "   776,\n",
       "   819,\n",
       "   2962,\n",
       "   117,\n",
       "   5,\n",
       "   24,\n",
       "   2963,\n",
       "   2,\n",
       "   0,\n",
       "   2964,\n",
       "   6,\n",
       "   62,\n",
       "   9,\n",
       "   10,\n",
       "   836,\n",
       "   1033,\n",
       "   2965,\n",
       "   1,\n",
       "   0,\n",
       "   20,\n",
       "   44,\n",
       "   6,\n",
       "   1081,\n",
       "   62,\n",
       "   107,\n",
       "   6,\n",
       "   187,\n",
       "   62,\n",
       "   1238,\n",
       "   2,\n",
       "   0,\n",
       "   2966,\n",
       "   468,\n",
       "   18,\n",
       "   1,\n",
       "   107],\n",
       "  [91,\n",
       "   42,\n",
       "   49,\n",
       "   5,\n",
       "   241,\n",
       "   60,\n",
       "   18,\n",
       "   60,\n",
       "   28,\n",
       "   2,\n",
       "   0,\n",
       "   16,\n",
       "   82,\n",
       "   2233,\n",
       "   35,\n",
       "   8,\n",
       "   30,\n",
       "   313,\n",
       "   346,\n",
       "   1,\n",
       "   0,\n",
       "   3,\n",
       "   25,\n",
       "   67,\n",
       "   28,\n",
       "   1,\n",
       "   5,\n",
       "   14,\n",
       "   193,\n",
       "   140,\n",
       "   134,\n",
       "   2,\n",
       "   23,\n",
       "   0,\n",
       "   22,\n",
       "   8,\n",
       "   55,\n",
       "   7,\n",
       "   452,\n",
       "   11,\n",
       "   213,\n",
       "   9,\n",
       "   18,\n",
       "   333,\n",
       "   0,\n",
       "   31,\n",
       "   1235]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_target_data[:10], tokenized_target_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0effc05f-d9cc-4e33-9970-b72f995f8e74",
   "metadata": {},
   "source": [
    "* As you can see that we are getting correct tokenized target data\n",
    "* Now let's use glove embedding to embed this features token data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8d83e04-fb65-4160-beb9-b62a3cb57159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = \"./data/glove.6B.100d\"\n",
    "\n",
    "embeddings_index = {} #initialize dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
    "try:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "except:\n",
    "    print(line)\n",
    "f.close()\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5072989-01f0-4c86-81c9-f3014bc78d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3195"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca4c2e09-b54f-4a7f-b12d-67558db1cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word, i in vocab:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < vocabulary_size:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6935c55-32ad-4dbd-b65d-3fff142bc134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3195, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dacd5142-fb04-4aca-8051-94691c530f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 436)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d45851ea-dda2-4c81-b0db-8680b5088ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24839   ,  0.41580001, -0.50120997, -0.46527001, -0.3831    ,\n",
       "        0.46129   ,  0.37305999,  0.81476998, -0.6559    ,  0.094987  ,\n",
       "        0.28314   , -0.79750001, -0.1247    ,  1.36459994,  0.11573   ,\n",
       "       -0.60711998, -0.20911001, -0.20287   , -0.43491   ,  0.037515  ,\n",
       "        0.054566  , -0.026389  , -0.39265001, -0.23852   , -0.29278001,\n",
       "       -0.042224  , -0.39368001,  0.1583    , -0.64696997, -0.10887   ,\n",
       "       -0.19221   ,  0.026955  , -0.39243999, -0.67449999, -0.24213   ,\n",
       "        0.44674   , -0.31722999,  0.43742999, -0.32921001, -0.72364998,\n",
       "        0.22341   , -0.46348   , -0.31382   , -1.59000003,  0.47584   ,\n",
       "        0.84485   , -0.92957997,  0.2406    ,  1.09159994, -0.61738998,\n",
       "        0.22025999, -0.08839   ,  0.20354   ,  0.43571001, -0.67427999,\n",
       "       -2.23149991, -0.49112001, -0.54070997, -0.22719   , -0.011102  ,\n",
       "        0.44363001,  0.17513999, -0.1183    ,  1.05610001,  0.73207003,\n",
       "        0.77508003,  0.90759999, -0.59994   , -0.42039001, -0.84024   ,\n",
       "       -0.38319999,  0.40546   ,  0.55716002, -0.093883  , -0.30089   ,\n",
       "       -0.24208   ,  0.46267   , -0.013453  ,  0.096436  , -0.18233   ,\n",
       "        0.15469   , -0.10463   ,  0.88780999,  1.02190006,  0.19005001,\n",
       "        0.17207   ,  0.33601999, -0.10644   ,  0.90478998, -0.14224   ,\n",
       "       -0.17444   , -0.45822001,  0.85189998,  0.35418001, -0.18489   ,\n",
       "        0.044011  ,  0.41060999,  0.030981  ,  0.42032   ,  0.035697  ])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf5640-0995-4214-925d-653c91ad92c1",
   "metadata": {},
   "source": [
    "* Now let's split the data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b751dea1-be0b-4b53-9bd0-35770d0f2ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000, 40000, 10000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tokenized_features_data, tokenized_target_data, test_size=0.2)\n",
    "len(X_train), len(X_test), len(Y_train), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "991c9a9e-4431-46f9-9b8d-ca4eb506b5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  5,\n",
       "  913,\n",
       "  728,\n",
       "  41,\n",
       "  21,\n",
       "  2,\n",
       "  0,\n",
       "  59,\n",
       "  54,\n",
       "  32,\n",
       "  729,\n",
       "  577,\n",
       "  5,\n",
       "  235,\n",
       "  51,\n",
       "  63,\n",
       "  98,\n",
       "  0,\n",
       "  57,\n",
       "  717,\n",
       "  20,\n",
       "  46,\n",
       "  57,\n",
       "  83,\n",
       "  1,\n",
       "  136,\n",
       "  17,\n",
       "  4,\n",
       "  62,\n",
       "  2,\n",
       "  23,\n",
       "  0,\n",
       "  22,\n",
       "  107,\n",
       "  86,\n",
       "  95,\n",
       "  45,\n",
       "  9,\n",
       "  730,\n",
       "  13,\n",
       "  51,\n",
       "  459,\n",
       "  0,\n",
       "  7,\n",
       "  578,\n",
       "  335],\n",
       " [0, 31, 1730, 1731, 15, 17, 1732, 35, 125, 1, 0, 20, 112, 35, 125],\n",
       " [3,\n",
       "  57,\n",
       "  1357,\n",
       "  218,\n",
       "  0,\n",
       "  109,\n",
       "  88,\n",
       "  95,\n",
       "  1,\n",
       "  17,\n",
       "  424,\n",
       "  66,\n",
       "  1,\n",
       "  42,\n",
       "  58,\n",
       "  593,\n",
       "  1,\n",
       "  0,\n",
       "  2907,\n",
       "  1292,\n",
       "  13,\n",
       "  19,\n",
       "  129,\n",
       "  1305,\n",
       "  2,\n",
       "  0,\n",
       "  59,\n",
       "  25,\n",
       "  57,\n",
       "  843,\n",
       "  1,\n",
       "  1358,\n",
       "  7,\n",
       "  51,\n",
       "  221,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  123,\n",
       "  160,\n",
       "  415,\n",
       "  58,\n",
       "  67,\n",
       "  122,\n",
       "  25,\n",
       "  2,\n",
       "  23],\n",
       " [9, 29, 568, 1, 0, 31, 9],\n",
       " [1, 0, 30, 15, 17, 2880, 13, 2881, 1, 232, 42, 63, 0, 20, 944],\n",
       " [124, 884, 6, 1387, 5, 24, 19, 1388, 1, 0, 1389, 29, 1390, 13, 56],\n",
       " [606, 2, 23, 0, 22, 26, 19],\n",
       " [0,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  2411,\n",
       "  272,\n",
       "  253,\n",
       "  15,\n",
       "  2412,\n",
       "  2,\n",
       "  0,\n",
       "  10,\n",
       "  121,\n",
       "  11,\n",
       "  725,\n",
       "  1,\n",
       "  10,\n",
       "  89,\n",
       "  134,\n",
       "  44,\n",
       "  17,\n",
       "  636,\n",
       "  1,\n",
       "  0,\n",
       "  40,\n",
       "  18,\n",
       "  1,\n",
       "  5,\n",
       "  214,\n",
       "  11,\n",
       "  725],\n",
       " [127,\n",
       "  2,\n",
       "  23,\n",
       "  0,\n",
       "  22,\n",
       "  150,\n",
       "  8,\n",
       "  55,\n",
       "  1,\n",
       "  47,\n",
       "  216,\n",
       "  8,\n",
       "  55,\n",
       "  185,\n",
       "  1,\n",
       "  0,\n",
       "  31,\n",
       "  52,\n",
       "  83,\n",
       "  21,\n",
       "  732,\n",
       "  41,\n",
       "  6,\n",
       "  1960,\n",
       "  1,\n",
       "  0,\n",
       "  1961,\n",
       "  10,\n",
       "  14,\n",
       "  73,\n",
       "  1157,\n",
       "  29,\n",
       "  1962,\n",
       "  532,\n",
       "  1,\n",
       "  0,\n",
       "  1158,\n",
       "  5,\n",
       "  8,\n",
       "  663,\n",
       "  35,\n",
       "  1963,\n",
       "  1964,\n",
       "  12,\n",
       "  0,\n",
       "  150,\n",
       "  8],\n",
       " [1393, 6, 192, 2, 0, 47, 10, 3139, 3140, 14, 9, 18, 1, 0, 49]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8480ccc5-3fc1-4ef5-8362-fa7f51fa86b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5,\n",
       "  913,\n",
       "  728,\n",
       "  41,\n",
       "  21,\n",
       "  2,\n",
       "  0,\n",
       "  59,\n",
       "  54,\n",
       "  32,\n",
       "  729,\n",
       "  577,\n",
       "  5,\n",
       "  235,\n",
       "  51,\n",
       "  63,\n",
       "  98,\n",
       "  0,\n",
       "  57,\n",
       "  717,\n",
       "  20,\n",
       "  46,\n",
       "  57,\n",
       "  83,\n",
       "  1,\n",
       "  136,\n",
       "  17,\n",
       "  4,\n",
       "  62,\n",
       "  2,\n",
       "  23,\n",
       "  0,\n",
       "  22,\n",
       "  107,\n",
       "  86,\n",
       "  95,\n",
       "  45,\n",
       "  9,\n",
       "  730,\n",
       "  13,\n",
       "  51,\n",
       "  459,\n",
       "  0,\n",
       "  7,\n",
       "  578,\n",
       "  335,\n",
       "  3],\n",
       " [31, 1730, 1731, 15, 17, 1732, 35, 125, 1, 0, 20, 112, 35, 125, 3],\n",
       " [57,\n",
       "  1357,\n",
       "  218,\n",
       "  0,\n",
       "  109,\n",
       "  88,\n",
       "  95,\n",
       "  1,\n",
       "  17,\n",
       "  424,\n",
       "  66,\n",
       "  1,\n",
       "  42,\n",
       "  58,\n",
       "  593,\n",
       "  1,\n",
       "  0,\n",
       "  2907,\n",
       "  1292,\n",
       "  13,\n",
       "  19,\n",
       "  129,\n",
       "  1305,\n",
       "  2,\n",
       "  0,\n",
       "  59,\n",
       "  25,\n",
       "  57,\n",
       "  843,\n",
       "  1,\n",
       "  1358,\n",
       "  7,\n",
       "  51,\n",
       "  221,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  123,\n",
       "  160,\n",
       "  415,\n",
       "  58,\n",
       "  67,\n",
       "  122,\n",
       "  25,\n",
       "  2,\n",
       "  23,\n",
       "  0],\n",
       " [29, 568, 1, 0, 31, 9, 386],\n",
       " [0, 30, 15, 17, 2880, 13, 2881, 1, 232, 42, 63, 0, 20, 944, 1354],\n",
       " [884, 6, 1387, 5, 24, 19, 1388, 1, 0, 1389, 29, 1390, 13, 56, 66],\n",
       " [2, 23, 0, 22, 26, 19, 1848],\n",
       " [3,\n",
       "  25,\n",
       "  6,\n",
       "  2411,\n",
       "  272,\n",
       "  253,\n",
       "  15,\n",
       "  2412,\n",
       "  2,\n",
       "  0,\n",
       "  10,\n",
       "  121,\n",
       "  11,\n",
       "  725,\n",
       "  1,\n",
       "  10,\n",
       "  89,\n",
       "  134,\n",
       "  44,\n",
       "  17,\n",
       "  636,\n",
       "  1,\n",
       "  0,\n",
       "  40,\n",
       "  18,\n",
       "  1,\n",
       "  5,\n",
       "  214,\n",
       "  11,\n",
       "  725,\n",
       "  33],\n",
       " [2,\n",
       "  23,\n",
       "  0,\n",
       "  22,\n",
       "  150,\n",
       "  8,\n",
       "  55,\n",
       "  1,\n",
       "  47,\n",
       "  216,\n",
       "  8,\n",
       "  55,\n",
       "  185,\n",
       "  1,\n",
       "  0,\n",
       "  31,\n",
       "  52,\n",
       "  83,\n",
       "  21,\n",
       "  732,\n",
       "  41,\n",
       "  6,\n",
       "  1960,\n",
       "  1,\n",
       "  0,\n",
       "  1961,\n",
       "  10,\n",
       "  14,\n",
       "  73,\n",
       "  1157,\n",
       "  29,\n",
       "  1962,\n",
       "  532,\n",
       "  1,\n",
       "  0,\n",
       "  1158,\n",
       "  5,\n",
       "  8,\n",
       "  663,\n",
       "  35,\n",
       "  1963,\n",
       "  1964,\n",
       "  12,\n",
       "  0,\n",
       "  150,\n",
       "  8,\n",
       "  55],\n",
       " [6, 192, 2, 0, 47, 10, 3139, 3140, 14, 9, 18, 1, 0, 49, 486]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd729ebe-31bc-40ce-9691-1537485fb6a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10,\n",
       "  294,\n",
       "  411,\n",
       "  0,\n",
       "  2314,\n",
       "  4,\n",
       "  2315,\n",
       "  7,\n",
       "  19,\n",
       "  2316,\n",
       "  256,\n",
       "  12,\n",
       "  0,\n",
       "  59,\n",
       "  46,\n",
       "  7,\n",
       "  21,\n",
       "  10,\n",
       "  991,\n",
       "  37,\n",
       "  1109,\n",
       "  0,\n",
       "  72,\n",
       "  1104,\n",
       "  21,\n",
       "  7,\n",
       "  3,\n",
       "  1256,\n",
       "  33,\n",
       "  21,\n",
       "  253],\n",
       " [12, 0, 25, 3062, 11, 254, 8],\n",
       " [390, 415, 77, 17, 77, 15, 3010],\n",
       " [1],\n",
       " [42, 49, 52, 368, 1, 0, 315],\n",
       " [3],\n",
       " [0],\n",
       " [178,\n",
       "  18,\n",
       "  87,\n",
       "  1373,\n",
       "  168,\n",
       "  1,\n",
       "  0,\n",
       "  3190,\n",
       "  9,\n",
       "  4,\n",
       "  361,\n",
       "  129,\n",
       "  3191,\n",
       "  2,\n",
       "  0,\n",
       "  117,\n",
       "  885,\n",
       "  634,\n",
       "  8,\n",
       "  77,\n",
       "  821,\n",
       "  18,\n",
       "  396,\n",
       "  1,\n",
       "  0,\n",
       "  263,\n",
       "  136,\n",
       "  6,\n",
       "  612,\n",
       "  24,\n",
       "  252,\n",
       "  4,\n",
       "  113,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1351,\n",
       "  1,\n",
       "  1374,\n",
       "  56,\n",
       "  129,\n",
       "  400,\n",
       "  160,\n",
       "  1,\n",
       "  0,\n",
       "  3192,\n",
       "  262],\n",
       " [62],\n",
       " [0,\n",
       "  59,\n",
       "  38,\n",
       "  17,\n",
       "  25,\n",
       "  12,\n",
       "  20,\n",
       "  114,\n",
       "  82,\n",
       "  519,\n",
       "  1022,\n",
       "  1,\n",
       "  0,\n",
       "  508,\n",
       "  18,\n",
       "  3017,\n",
       "  13,\n",
       "  209,\n",
       "  1,\n",
       "  3,\n",
       "  3018,\n",
       "  6,\n",
       "  505,\n",
       "  2,\n",
       "  23,\n",
       "  0,\n",
       "  22,\n",
       "  24,\n",
       "  1232,\n",
       "  26,\n",
       "  11]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d22b8b65-a38f-4b12-80a8-64620ba6d6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[294,\n",
       "  411,\n",
       "  0,\n",
       "  2314,\n",
       "  4,\n",
       "  2315,\n",
       "  7,\n",
       "  19,\n",
       "  2316,\n",
       "  256,\n",
       "  12,\n",
       "  0,\n",
       "  59,\n",
       "  46,\n",
       "  7,\n",
       "  21,\n",
       "  10,\n",
       "  991,\n",
       "  37,\n",
       "  1109,\n",
       "  0,\n",
       "  72,\n",
       "  1104,\n",
       "  21,\n",
       "  7,\n",
       "  3,\n",
       "  1256,\n",
       "  33,\n",
       "  21,\n",
       "  253,\n",
       "  2],\n",
       " [0, 25, 3062, 11, 254, 8, 30],\n",
       " [415, 77, 17, 77, 15, 3010, 34],\n",
       " [3],\n",
       " [49, 52, 368, 1, 0, 315, 1],\n",
       " [308],\n",
       " [22],\n",
       " [18,\n",
       "  87,\n",
       "  1373,\n",
       "  168,\n",
       "  1,\n",
       "  0,\n",
       "  3190,\n",
       "  9,\n",
       "  4,\n",
       "  361,\n",
       "  129,\n",
       "  3191,\n",
       "  2,\n",
       "  0,\n",
       "  117,\n",
       "  885,\n",
       "  634,\n",
       "  8,\n",
       "  77,\n",
       "  821,\n",
       "  18,\n",
       "  396,\n",
       "  1,\n",
       "  0,\n",
       "  263,\n",
       "  136,\n",
       "  6,\n",
       "  612,\n",
       "  24,\n",
       "  252,\n",
       "  4,\n",
       "  113,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1351,\n",
       "  1,\n",
       "  1374,\n",
       "  56,\n",
       "  129,\n",
       "  400,\n",
       "  160,\n",
       "  1,\n",
       "  0,\n",
       "  3192,\n",
       "  262,\n",
       "  9],\n",
       " [107],\n",
       " [59,\n",
       "  38,\n",
       "  17,\n",
       "  25,\n",
       "  12,\n",
       "  20,\n",
       "  114,\n",
       "  82,\n",
       "  519,\n",
       "  1022,\n",
       "  1,\n",
       "  0,\n",
       "  508,\n",
       "  18,\n",
       "  3017,\n",
       "  13,\n",
       "  209,\n",
       "  1,\n",
       "  3,\n",
       "  3018,\n",
       "  6,\n",
       "  505,\n",
       "  2,\n",
       "  23,\n",
       "  0,\n",
       "  22,\n",
       "  24,\n",
       "  1232,\n",
       "  26,\n",
       "  11,\n",
       "  63]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96498779-4d95-497e-8807-735588e1735d",
   "metadata": {},
   "source": [
    "* Now that we have splitted our data now let's write custom RNN with single layer which uses glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dae2045-43ec-416c-ad7d-41488cf3042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, embedding_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.G = embedding_matrix\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # embedding of x[t]:\n",
    "            e_t = self.G[x[t]]                   \n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            #s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            s[t] = np.tanh(self.U.dot(e_t) + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                #dLdU[:,x[bptt_step]] += delta_t\n",
    "                dLdU += np.outer(delta_t, self.G[x[bptt_step]]) \n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "\n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "        bptt_gradients = model.bptt(x, y)\n",
    "        # List of all parameters we want to check.\n",
    "        model_parameters = ['U', 'V', 'W']\n",
    "        # Gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # Get the actual parameter value from the mode, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))     \n",
    "            # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "            it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index   \n",
    "                # Save the original value so we can reset it later\n",
    "                original_value = parameter[ix]   \n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = model.calculate_total_loss([x],[y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = model.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)    \n",
    "                # Reset parameter to original value\n",
    "                parameter[ix] = original_value  \n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix] \n",
    "                # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient) / (\n",
    "                                    np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                   # If the error is to large fail the gradient check\n",
    "                if relative_error > error_threshold:\n",
    "                    print( \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                    print( \"+h Loss: %f\" % gradplus)\n",
    "                    print( \"-h Loss: %f\" % gradminus)\n",
    "                    print( \"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                    print( \"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                    print( \"Relative Error: %f\" % relative_error)\n",
    "                    return \n",
    "                it.iternext()     \n",
    "            print( \"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        \n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc45235-3015-43da-8094-b35037fd02e3",
   "metadata": {},
   "source": [
    "* Let's write a softmax activation function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77ca2958-86d8-4608-96be-ebb41441086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # sometimes, may want to do this first:\n",
    "    #x = np.vectorize(round)(x)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a3ec8-044a-4108-803e-0109b9a6bf2e",
   "metadata": {},
   "source": [
    "* Let's test written model for one sample in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f51070f-0317-4e86-ac5e-2e6a99dfa07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4426d8a2-8774-47a3-a0a2-73fe96ec230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 3195) [[0.00031299 0.00031299 0.00031299 ... 0.00031299 0.00031299 0.00031299]\n",
      " [0.00032618 0.00031194 0.00029823 ... 0.00029698 0.00031307 0.00031164]\n",
      " [0.00030632 0.00031649 0.00030686 ... 0.00030714 0.00031204 0.0003208 ]\n",
      " ...\n",
      " [0.00031068 0.00032478 0.0003044  ... 0.00032265 0.00031396 0.00031794]\n",
      " [0.0003034  0.00032446 0.00030852 ... 0.00031352 0.00031806 0.00032889]\n",
      " [0.00033368 0.00032757 0.00032311 ... 0.00032843 0.00032218 0.00031164]]\n"
     ]
    }
   ],
   "source": [
    "print (o.shape, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97ba4f40-682e-4033-8c20-b32d2b6660e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'birds'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word[int(np.argmax(o[-1], axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72723a20-5e6d-4ae5-8b69-55a9a2e13746",
   "metadata": {},
   "source": [
    "* As you can see that the output is generated as expected\n",
    "* Now let's see if model's prediction is equal to random predictions as model is not trained yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06655202-14a7-4d9e-ad0b-81049f7e7161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.069342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14176/1748315108.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 8.068450\n"
     ]
    }
   ],
   "source": [
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], Y_train[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50ffd2-f5c0-4fa7-ac8b-cdff58274465",
   "metadata": {},
   "source": [
    "* As you can see that they are similar and our model is correct\n",
    "* Let's also check model by doing gradient check which will check our backpropogation implementation is correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc3bd63e-40c0-4e93-b0e7-3e50e62a508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNN(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c839dc4-bc87-4bb7-b5e8-7029988152ab",
   "metadata": {},
   "source": [
    "* Now let's write a function for training model using stochastic gradient decent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "220e461a-e45e-4b4f-ad51-6379b2194ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5, validation_data=None):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            if validation_data:\n",
    "                val_loss = model.calculate_loss(validation_data[0], validation_data[1])\n",
    "                losses.append((num_examples_seen, loss, val_loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (f\"{time}: Loss after num_examples_seen={num_examples_seen} epoch={epoch}: Training - {loss}  Testing - {val_loss}\")\n",
    "            else:\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (f\"{time}: Loss after num_examples_seen={num_examples_seen} epoch={epoch}: {loss}\")\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print (f\"Setting learning rate to {learning_rate}\")\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bc38d-e0bd-43c5-9aac-cba8603c3109",
   "metadata": {},
   "source": [
    "* Now let's train model on 50 epochs first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5191249b-3ef8-423e-a7f0-bdfa62d6c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14176/1748315108.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 01:03:30: Loss after num_examples_seen=0 epoch=0: Training - 8.06890918113278  Testing - 8.068799183842096\n",
      "2025-11-02 01:12:22: Loss after num_examples_seen=40000 epoch=1: Training - 3.9312908122854484  Testing - 3.9894089560439787\n",
      "2025-11-02 01:21:18: Loss after num_examples_seen=80000 epoch=2: Training - 3.433425034186288  Testing - 3.491797464779423\n",
      "2025-11-02 01:29:55: Loss after num_examples_seen=120000 epoch=3: Training - 3.296033095089618  Testing - 3.354550309528791\n",
      "2025-11-02 01:38:33: Loss after num_examples_seen=160000 epoch=4: Training - 3.296267204255384  Testing - 3.3514713273130767\n",
      "Setting learning rate to 0.0025\n",
      "2025-11-02 01:47:12: Loss after num_examples_seen=200000 epoch=5: Training - 2.9181946229283517  Testing - 2.97943247377824\n",
      "2025-11-02 01:55:49: Loss after num_examples_seen=240000 epoch=6: Training - 2.854960863256655  Testing - 2.9164611647447325\n",
      "2025-11-02 01:04:29: Loss after num_examples_seen=280000 epoch=7: Training - 2.8053729800011813  Testing - 2.8652804069436972\n",
      "2025-11-02 01:13:09: Loss after num_examples_seen=320000 epoch=8: Training - 2.80043961494381  Testing - 2.8638291731694108\n",
      "2025-11-02 01:21:46: Loss after num_examples_seen=360000 epoch=9: Training - 2.7675678178193577  Testing - 2.833236628800843\n",
      "2025-11-02 01:30:22: Loss after num_examples_seen=400000 epoch=10: Training - 2.7431229210804347  Testing - 2.8016351490083653\n",
      "2025-11-02 01:39:00: Loss after num_examples_seen=440000 epoch=11: Training - 2.7155156060738164  Testing - 2.7770628208687462\n",
      "2025-11-02 01:47:39: Loss after num_examples_seen=480000 epoch=12: Training - 2.715225187209085  Testing - 2.778095547759106\n",
      "2025-11-02 01:56:15: Loss after num_examples_seen=520000 epoch=13: Training - 2.7254539043247035  Testing - 2.789718198762594\n",
      "Setting learning rate to 0.00125\n",
      "2025-11-02 02:04:53: Loss after num_examples_seen=560000 epoch=14: Training - 2.500528607515873  Testing - 2.5711819997428633\n",
      "2025-11-02 02:13:28: Loss after num_examples_seen=600000 epoch=15: Training - 2.479699218848834  Testing - 2.5500914622407342\n",
      "2025-11-02 02:22:03: Loss after num_examples_seen=640000 epoch=16: Training - 2.4578144201627423  Testing - 2.5285973025200317\n",
      "2025-11-02 02:30:38: Loss after num_examples_seen=680000 epoch=17: Training - 2.4460512255119404  Testing - 2.51575334353708\n",
      "2025-11-02 02:39:14: Loss after num_examples_seen=720000 epoch=18: Training - 2.4293418172777623  Testing - 2.500446115018611\n",
      "2025-11-02 02:47:52: Loss after num_examples_seen=760000 epoch=19: Training - 2.423244555752266  Testing - 2.494177400707946\n",
      "2025-11-02 02:56:30: Loss after num_examples_seen=800000 epoch=20: Training - 2.407849200199204  Testing - 2.4782143622947945\n",
      "2025-11-02 03:05:07: Loss after num_examples_seen=840000 epoch=21: Training - 2.4001295429918796  Testing - 2.4683172022906823\n",
      "2025-11-02 03:13:46: Loss after num_examples_seen=880000 epoch=22: Training - 2.385086102605895  Testing - 2.4573713115144895\n",
      "2025-11-02 03:22:23: Loss after num_examples_seen=920000 epoch=23: Training - 2.3865770943170763  Testing - 2.459041243032659\n",
      "Setting learning rate to 0.000625\n",
      "2025-11-02 03:31:02: Loss after num_examples_seen=960000 epoch=24: Training - 2.2759440528024015  Testing - 2.350994935435814\n",
      "2025-11-02 03:39:41: Loss after num_examples_seen=1000000 epoch=25: Training - 2.2784719453322126  Testing - 2.3529713667196597\n",
      "Setting learning rate to 0.0003125\n",
      "2025-11-02 03:48:19: Loss after num_examples_seen=1040000 epoch=26: Training - 2.2261959546484387  Testing - 2.304745060201601\n",
      "2025-11-02 03:56:58: Loss after num_examples_seen=1080000 epoch=27: Training - 2.2227424028512464  Testing - 2.302083029076806\n",
      "2025-11-02 04:05:35: Loss after num_examples_seen=1120000 epoch=28: Training - 2.2186160294778037  Testing - 2.298965338507368\n",
      "2025-11-02 04:14:13: Loss after num_examples_seen=1160000 epoch=29: Training - 2.215009318272303  Testing - 2.295593509134595\n",
      "2025-11-02 04:22:50: Loss after num_examples_seen=1200000 epoch=30: Training - 2.2117904245074946  Testing - 2.2920606397946397\n",
      "2025-11-02 04:31:28: Loss after num_examples_seen=1240000 epoch=31: Training - 2.206761209804756  Testing - 2.287696977777832\n",
      "2025-11-02 04:40:05: Loss after num_examples_seen=1280000 epoch=32: Training - 2.2058815246548993  Testing - 2.2854345242045655\n",
      "2025-11-02 04:48:40: Loss after num_examples_seen=1320000 epoch=33: Training - 2.2043959361169247  Testing - 2.284199354915485\n",
      "2025-11-02 04:57:14: Loss after num_examples_seen=1360000 epoch=34: Training - 2.1991182418901376  Testing - 2.279395968306727\n",
      "2025-11-02 05:05:49: Loss after num_examples_seen=1400000 epoch=35: Training - 2.2004231279451205  Testing - 2.2805186231250882\n",
      "Setting learning rate to 0.00015625\n",
      "2025-11-02 05:14:26: Loss after num_examples_seen=1440000 epoch=36: Training - 2.1720243682612876  Testing - 2.253123147774974\n",
      "2025-11-02 05:23:03: Loss after num_examples_seen=1480000 epoch=37: Training - 2.170594580450177  Testing - 2.2520558996394238\n",
      "2025-11-02 05:31:40: Loss after num_examples_seen=1520000 epoch=38: Training - 2.168364917039844  Testing - 2.2505558286889715\n",
      "2025-11-02 05:40:18: Loss after num_examples_seen=1560000 epoch=39: Training - 2.167709777691987  Testing - 2.249692079461835\n",
      "2025-11-02 05:48:56: Loss after num_examples_seen=1600000 epoch=40: Training - 2.1668144500172524  Testing - 2.249626188091216\n",
      "2025-11-02 05:57:34: Loss after num_examples_seen=1640000 epoch=41: Training - 2.1638900922928563  Testing - 2.246930633429918\n",
      "2025-11-02 06:06:11: Loss after num_examples_seen=1680000 epoch=42: Training - 2.1638161277986403  Testing - 2.246407552209051\n",
      "2025-11-02 06:14:51: Loss after num_examples_seen=1720000 epoch=43: Training - 2.162968881515377  Testing - 2.246053411376954\n",
      "2025-11-02 06:23:27: Loss after num_examples_seen=1760000 epoch=44: Training - 2.161804963239263  Testing - 2.244728025003188\n",
      "2025-11-02 06:32:05: Loss after num_examples_seen=1800000 epoch=45: Training - 2.1600155948587814  Testing - 2.2426338234099683\n",
      "2025-11-02 06:40:44: Loss after num_examples_seen=1840000 epoch=46: Training - 2.159065179186127  Testing - 2.2428323457292754\n",
      "2025-11-02 06:49:22: Loss after num_examples_seen=1880000 epoch=47: Training - 2.1573958079724833  Testing - 2.240892318057664\n",
      "2025-11-02 06:58:00: Loss after num_examples_seen=1920000 epoch=48: Training - 2.1578457308249908  Testing - 2.241103344533901\n",
      "Setting learning rate to 7.8125e-05\n",
      "2025-11-02 07:06:42: Loss after num_examples_seen=1960000 epoch=49: Training - 2.144908647281431  Testing - 2.228730398776007\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train, Y_train, nepoch=50, evaluate_loss_after=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05a196ac-ecdc-47e3-a06a-3617a4812a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7047db206f60>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_epochs = range(1, 51)\n",
    "plt.plot(no_epochs, [loss[1] for loss in losses], label=\"Train loss\")\n",
    "plt.plot(no_epochs, [loss[2] for loss in losses], label=\"Test loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb136352-f138-4366-a839-feabfe106648",
   "metadata": {},
   "source": [
    "* Now let's first save the weights of our model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "334833fd-b36d-491e-b8c1-068aa3419a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weights/U.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.U, file)\n",
    "with open('./weights/W.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.W, file)\n",
    "with open('./weights/V.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.V, file)\n",
    "with open('./weights/G.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.G, file)\n",
    "    \n",
    "with open('./weights/word_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.word_dim, file)\n",
    "with open('./weights/hidden_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.hidden_dim, file)\n",
    "with open('./weights/bptt_truncate.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.bptt_truncate, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670f90a-e29e-4c51-a895-60bf85386290",
   "metadata": {},
   "source": [
    "* Now let's write a function to generate poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff38cc66-d975-47fb-bb32-a965741366ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(start_sent, length=0):\n",
    "    start_sent = clean_roman_numerals(start_sent)\n",
    "    start_sent = clean_numerical_numbers(start_sent)\n",
    "    start_sent = clean_square_brackets(start_sent)\n",
    "    start_sent = clean_spaces_at_start(start_sent)\n",
    "    # start_sent = add_sonet_tokens(start_sent)\n",
    "    pattern = r'''<[^>]+>|\\w+(?:'\\w+)?|\\n|[.,:;!?—()]'''\n",
    "    tokens = re.findall(pattern, start_sent)\n",
    "    starting_sent = [tokens]\n",
    "    tokenized_starting_sent = tokenize_generated_data(starting_sent, word_to_index)\n",
    "    generated_poem = [tokens[0]]\n",
    "    for i in tokens[1:]:\n",
    "        if (i != \"<sonet_start>\" and i != \"<sonet_end>\"):\n",
    "            if i not in \".,?;!\\n\":\n",
    "                generated_poem.append(\" \")\n",
    "            generated_poem.append(i)\n",
    "    if length:\n",
    "        for i in range(length):\n",
    "            next_pred_word = int(np.argmax(model.forward_propagation(tokenized_starting_sent[0])[0][-1], axis=0))\n",
    "            if index_to_word[next_pred_word] != \"<sonet_end>\" and index_to_word[next_pred_word] != \"<sonet_start>\":\n",
    "                if index_to_word[next_pred_word] not in \".,?;!\\n\" and generated_poem[-1] != \"\\n\":\n",
    "                    generated_poem.append(\" \")\n",
    "                generated_poem.append(index_to_word[next_pred_word])\n",
    "            tokenized_starting_sent[0].append(next_pred_word)\n",
    "    else:\n",
    "        next_pred_word = \"<sonet_start>\"\n",
    "        while next_pred_word != word_to_index[\"<sonet_end>\"]:\n",
    "            next_pred_word = int(np.argmax(model.forward_propagation(tokenized_starting_sent[0])[0][-1], axis=0))\n",
    "            if index_to_word[next_pred_word] != \"<sonet_end>\" and index_to_word[next_pred_word] != \"<sonet_start>\":\n",
    "                if index_to_word[next_pred_word] not in \".,?;!\\n\" and generated_poem[-1] != \"\\n\":\n",
    "                    generated_poem.append(\" \")\n",
    "                generated_poem.append(index_to_word[next_pred_word])\n",
    "            tokenized_starting_sent[0].append(next_pred_word)\n",
    "    return generated_poem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22a981-fb8c-410c-97b8-2c4aa8bc679e",
   "metadata": {},
   "source": [
    "* Let's test this function by generating poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d6e94ab-18c0-4118-b26c-5efc6bdb9c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world of thy husbandry?\n",
      "who, why, fearing friend, found for it is hate fair,\n",
      "these the object burden should the pebbled shore.\n",
      "pity this abundant issue seemed to me\n",
      "or wrong them\n",
      "hath impart as your sweet self resemble,\n",
      "but with the day they on the pebbled,\n",
      "summer's hath choirs, even for their eyes were, and look must\n",
      ";\n",
      "yet both with a bastard shame,\n",
      "and age in love loves not to have years told.\n",
      "therefore ll\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate_poem(\"The\", 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee4919-9e03-467d-94ab-54b45532189b",
   "metadata": {},
   "source": [
    "* As you can see from above that our model is still improving (i.e loss is reducing) so let's do another 50 epochs of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6355d386-5ae0-48de-9a22-5dfd136bab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14176/1748315108.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 07:21:54: Loss after num_examples_seen=0 epoch=0: Training - 2.1442368578981013  Testing - 2.2278772079124334\n",
      "2025-11-02 07:30:57: Loss after num_examples_seen=40000 epoch=1: Training - 2.143395833586676  Testing - 2.2273594649309865\n",
      "2025-11-02 07:39:53: Loss after num_examples_seen=80000 epoch=2: Training - 2.1429928624361163  Testing - 2.227803468490943\n",
      "2025-11-02 07:48:51: Loss after num_examples_seen=120000 epoch=3: Training - 2.1419507007768304  Testing - 2.226448777196625\n",
      "2025-11-02 07:57:56: Loss after num_examples_seen=160000 epoch=4: Training - 2.1414492926995607  Testing - 2.225720194751631\n",
      "2025-11-02 08:07:26: Loss after num_examples_seen=200000 epoch=5: Training - 2.140418157192288  Testing - 2.2259688678285388\n",
      "2025-11-02 08:18:22: Loss after num_examples_seen=240000 epoch=6: Training - 2.1397371525651145  Testing - 2.2250210769752825\n",
      "2025-11-02 08:29:12: Loss after num_examples_seen=280000 epoch=7: Training - 2.1398960675387166  Testing - 2.2248821320616767\n",
      "Setting learning rate to 3.90625e-05\n",
      "2025-11-02 08:40:21: Loss after num_examples_seen=320000 epoch=8: Training - 2.133633663564398  Testing - 2.2192451039523236\n",
      "2025-11-02 08:50:09: Loss after num_examples_seen=360000 epoch=9: Training - 2.13306781189112  Testing - 2.218487068117679\n",
      "2025-11-02 09:00:20: Loss after num_examples_seen=400000 epoch=10: Training - 2.1328617537055927  Testing - 2.218296168230132\n",
      "2025-11-02 09:11:26: Loss after num_examples_seen=440000 epoch=11: Training - 2.132486997975129  Testing - 2.2180142725139724\n",
      "2025-11-02 09:23:40: Loss after num_examples_seen=480000 epoch=12: Training - 2.1322377073914214  Testing - 2.2176220410272434\n",
      "2025-11-02 09:35:22: Loss after num_examples_seen=520000 epoch=13: Training - 2.1317140987327563  Testing - 2.217646364906351\n",
      "2025-11-02 09:47:11: Loss after num_examples_seen=560000 epoch=14: Training - 2.1313356045289646  Testing - 2.217290543258744\n",
      "2025-11-02 09:58:22: Loss after num_examples_seen=600000 epoch=15: Training - 2.1310836898263044  Testing - 2.2171383102846427\n",
      "2025-11-02 10:08:35: Loss after num_examples_seen=640000 epoch=16: Training - 2.130559037891573  Testing - 2.216606811906974\n",
      "2025-11-02 10:18:21: Loss after num_examples_seen=680000 epoch=17: Training - 2.1303300665929243  Testing - 2.2159419036073276\n",
      "2025-11-02 10:28:10: Loss after num_examples_seen=720000 epoch=18: Training - 2.1298493480629843  Testing - 2.215706328489533\n",
      "2025-11-02 10:37:12: Loss after num_examples_seen=760000 epoch=19: Training - 2.1296656120189867  Testing - 2.2152591328760938\n",
      "2025-11-02 10:46:15: Loss after num_examples_seen=800000 epoch=20: Training - 2.129047421977866  Testing - 2.2150123954651586\n",
      "2025-11-02 10:55:20: Loss after num_examples_seen=840000 epoch=21: Training - 2.1286254619171037  Testing - 2.2149462112773275\n",
      "2025-11-02 11:04:22: Loss after num_examples_seen=880000 epoch=22: Training - 2.1285932687093845  Testing - 2.2149465907583927\n",
      "2025-11-02 11:13:09: Loss after num_examples_seen=920000 epoch=23: Training - 2.1282209530495204  Testing - 2.215223612232556\n",
      "2025-11-02 11:21:58: Loss after num_examples_seen=960000 epoch=24: Training - 2.1276788247622442  Testing - 2.214894396847484\n",
      "2025-11-02 11:30:52: Loss after num_examples_seen=1000000 epoch=25: Training - 2.1274398751009937  Testing - 2.214514123426287\n",
      "2025-11-02 11:39:44: Loss after num_examples_seen=1040000 epoch=26: Training - 2.127041440139203  Testing - 2.214501690960911\n",
      "2025-11-02 11:48:47: Loss after num_examples_seen=1080000 epoch=27: Training - 2.1270100537361127  Testing - 2.214763525168103\n",
      "2025-11-02 11:57:43: Loss after num_examples_seen=1120000 epoch=28: Training - 2.1266411737730118  Testing - 2.2141832095104106\n",
      "2025-11-02 12:06:36: Loss after num_examples_seen=1160000 epoch=29: Training - 2.1267118126225095  Testing - 2.214190807333844\n",
      "Setting learning rate to 1.953125e-05\n",
      "2025-11-02 12:15:31: Loss after num_examples_seen=1200000 epoch=30: Training - 2.1239260326583738  Testing - 2.211751035907443\n",
      "2025-11-02 12:24:16: Loss after num_examples_seen=1240000 epoch=31: Training - 2.123653422281067  Testing - 2.2112796155696035\n",
      "2025-11-02 12:32:59: Loss after num_examples_seen=1280000 epoch=32: Training - 2.1233267326787715  Testing - 2.2108351752276043\n",
      "2025-11-02 12:42:39: Loss after num_examples_seen=1320000 epoch=33: Training - 2.1231272565136523  Testing - 2.2104573956259226\n",
      "2025-11-02 12:53:25: Loss after num_examples_seen=1360000 epoch=34: Training - 2.123060462805625  Testing - 2.210587300893705\n",
      "2025-11-02 13:03:08: Loss after num_examples_seen=1400000 epoch=35: Training - 2.1231230923672095  Testing - 2.2104401847510005\n",
      "Setting learning rate to 9.765625e-06\n",
      "2025-11-02 13:11:59: Loss after num_examples_seen=1440000 epoch=36: Training - 2.121769812372671  Testing - 2.2092809697869877\n",
      "2025-11-02 13:21:50: Loss after num_examples_seen=1480000 epoch=37: Training - 2.1215744301238635  Testing - 2.209129002561784\n",
      "2025-11-02 13:32:18: Loss after num_examples_seen=1520000 epoch=38: Training - 2.121511693132904  Testing - 2.2092490153075763\n",
      "2025-11-02 13:43:11: Loss after num_examples_seen=1560000 epoch=39: Training - 2.121422210927607  Testing - 2.2093329042909406\n",
      "2025-11-02 13:53:02: Loss after num_examples_seen=1600000 epoch=40: Training - 2.121376520186177  Testing - 2.209240970973071\n",
      "2025-11-02 14:02:10: Loss after num_examples_seen=1640000 epoch=41: Training - 2.121351145231704  Testing - 2.209146973193124\n",
      "2025-11-02 14:11:00: Loss after num_examples_seen=1680000 epoch=42: Training - 2.1214042971889353  Testing - 2.2090386251119134\n",
      "Setting learning rate to 4.8828125e-06\n",
      "2025-11-02 14:19:50: Loss after num_examples_seen=1720000 epoch=43: Training - 2.1207480414503532  Testing - 2.208435047336135\n",
      "2025-11-02 14:28:36: Loss after num_examples_seen=1760000 epoch=44: Training - 2.1205801795533383  Testing - 2.208157228323719\n",
      "2025-11-02 14:37:32: Loss after num_examples_seen=1800000 epoch=45: Training - 2.1204442884563264  Testing - 2.208150571612833\n",
      "2025-11-02 14:46:21: Loss after num_examples_seen=1840000 epoch=46: Training - 2.120385753494243  Testing - 2.208166350535428\n",
      "2025-11-02 14:55:15: Loss after num_examples_seen=1880000 epoch=47: Training - 2.120359465839315  Testing - 2.2080967488149184\n",
      "2025-11-02 15:05:32: Loss after num_examples_seen=1920000 epoch=48: Training - 2.1202698984542807  Testing - 2.208095493828485\n",
      "2025-11-02 15:16:40: Loss after num_examples_seen=1960000 epoch=49: Training - 2.120228723172393  Testing - 2.208096313344629\n"
     ]
    }
   ],
   "source": [
    "losses_next_50 = train_with_sgd(model, X_train, Y_train, nepoch=50, evaluate_loss_after=1, validation_data=(X_test, Y_test), learning_rate=7.8125e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df8ca8-42ac-4c8d-9070-39a5388c058a",
   "metadata": {},
   "source": [
    "* Now let's plot the losses combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bf7e895-885b-49df-a79e-1db91547b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_combined = losses + losses_next_50\n",
    "len(losses_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d65b0c8c-a3cb-4a3f-b7fe-b0a264181f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7047db2ff380>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_epochs = range(1, 101)\n",
    "plt.plot(no_epochs, [loss[1] for loss in losses_combined], label=\"Train loss\")\n",
    "plt.plot(no_epochs, [loss[2] for loss in losses_combined], label=\"Test loss\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Number of epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b17b0b-6cb0-4c61-83e0-6bb01258cfe9",
   "metadata": {},
   "source": [
    "* As you can see that there is no overfitting yer and model is improving but very slowly\n",
    "* Let's first save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e2f1084-e737-4cc8-8e13-69d189120940",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weights/U.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.U, file)\n",
    "with open('./weights/W.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.W, file)\n",
    "with open('./weights/V.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.V, file)\n",
    "with open('./weights/G.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.G, file)\n",
    "    \n",
    "with open('./weights/word_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.word_dim, file)\n",
    "with open('./weights/hidden_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.hidden_dim, file)\n",
    "with open('./weights/bptt_truncate.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.bptt_truncate, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fbec1-8492-47ff-a6ee-8eaaa9886195",
   "metadata": {},
   "source": [
    "* Let's again generate a poem with updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e2c232b-74f7-46b2-9a1d-5200ab6d1a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world of thy husbandry?\n",
      "who, why, fearing friend, found for it is hate fair,\n",
      "these is contented thy heart drudge should the sun,\n",
      "and it that mine eye loves,\n",
      "and, frantic mad, to die thou gav'st the glowing of laws,\n",
      "though absence seemed, blunt so as foes commend,\n",
      "but those same tongues hours wail night as any enjoyer,\n",
      "applying on and cheeks,\n",
      "not once vouchsafe thee;\n",
      "then can drown best that thou mayst it thou spend\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate_poem(\"The\", 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcea952-295c-4c9d-a9ed-2b5a44468178",
   "metadata": {},
   "source": [
    "* As you can see that poem is better generated now compared to first model trained on just 50 epochs\n",
    "* Let's train it on another 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e196eea6-318c-42e8-9bdf-265a4c441a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14176/1748315108.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 15:38:17: Loss after num_examples_seen=0 epoch=0: Training - 2.12019894922026  Testing - 2.208031997762855\n",
      "2025-11-02 15:47:23: Loss after num_examples_seen=40000 epoch=1: Training - 2.120144668092074  Testing - 2.2079974168177627\n",
      "2025-11-02 15:57:22: Loss after num_examples_seen=80000 epoch=2: Training - 2.1201973051278564  Testing - 2.2079861600439954\n",
      "Setting learning rate to 2.44140625e-06\n",
      "2025-11-02 16:06:50: Loss after num_examples_seen=120000 epoch=3: Training - 2.1199325365704413  Testing - 2.207880923146609\n",
      "2025-11-02 16:15:50: Loss after num_examples_seen=160000 epoch=4: Training - 2.1198608116546214  Testing - 2.207854088121415\n",
      "2025-11-02 16:24:44: Loss after num_examples_seen=200000 epoch=5: Training - 2.1198278590645634  Testing - 2.2078989095344763\n",
      "2025-11-02 16:33:24: Loss after num_examples_seen=240000 epoch=6: Training - 2.1197902328675373  Testing - 2.2079163681408\n",
      "2025-11-02 16:42:03: Loss after num_examples_seen=280000 epoch=7: Training - 2.119761536279852  Testing - 2.2078906995012195\n",
      "2025-11-02 16:50:45: Loss after num_examples_seen=320000 epoch=8: Training - 2.1197632448725807  Testing - 2.207843559475835\n",
      "Setting learning rate to 1.220703125e-06\n",
      "2025-11-02 16:59:21: Loss after num_examples_seen=360000 epoch=9: Training - 2.1196562954180207  Testing - 2.2078486274656797\n",
      "2025-11-02 17:07:59: Loss after num_examples_seen=400000 epoch=10: Training - 2.119614577104715  Testing - 2.2078409604951905\n",
      "2025-11-02 17:16:35: Loss after num_examples_seen=440000 epoch=11: Training - 2.1195873698506222  Testing - 2.20782903549686\n",
      "2025-11-02 17:25:16: Loss after num_examples_seen=480000 epoch=12: Training - 2.1195824830811953  Testing - 2.2078297288783877\n",
      "2025-11-02 17:33:51: Loss after num_examples_seen=520000 epoch=13: Training - 2.119566528574364  Testing - 2.2078371772179164\n",
      "2025-11-02 17:42:30: Loss after num_examples_seen=560000 epoch=14: Training - 2.119544571516451  Testing - 2.207833558412124\n",
      "2025-11-02 17:51:10: Loss after num_examples_seen=600000 epoch=15: Training - 2.119524692203698  Testing - 2.2078354241646974\n",
      "2025-11-02 17:59:46: Loss after num_examples_seen=640000 epoch=16: Training - 2.1195095065136  Testing - 2.2078447183042798\n",
      "2025-11-02 18:08:25: Loss after num_examples_seen=680000 epoch=17: Training - 2.1194930666433716  Testing - 2.207845267786517\n",
      "2025-11-02 18:17:01: Loss after num_examples_seen=720000 epoch=18: Training - 2.1194754262081283  Testing - 2.207853406688845\n",
      "2025-11-02 18:25:42: Loss after num_examples_seen=760000 epoch=19: Training - 2.11945987644097  Testing - 2.2078687685277316\n",
      "2025-11-02 18:34:18: Loss after num_examples_seen=800000 epoch=20: Training - 2.119443332242705  Testing - 2.207888598116859\n",
      "2025-11-02 18:42:58: Loss after num_examples_seen=840000 epoch=21: Training - 2.119431743109243  Testing - 2.2078925827954814\n",
      "2025-11-02 18:51:36: Loss after num_examples_seen=880000 epoch=22: Training - 2.119444784373961  Testing - 2.207894315053651\n",
      "Setting learning rate to 6.103515625e-07\n",
      "2025-11-02 19:00:17: Loss after num_examples_seen=920000 epoch=23: Training - 2.1194036429253047  Testing - 2.207876519226941\n",
      "2025-11-02 19:08:54: Loss after num_examples_seen=960000 epoch=24: Training - 2.1193977887877447  Testing - 2.2078655731478674\n",
      "2025-11-02 19:17:34: Loss after num_examples_seen=1000000 epoch=25: Training - 2.1193997234394715  Testing - 2.207857934372341\n",
      "Setting learning rate to 3.0517578125e-07\n",
      "2025-11-02 19:26:11: Loss after num_examples_seen=1040000 epoch=26: Training - 2.1193921460907204  Testing - 2.20785463768463\n",
      "2025-11-02 19:34:50: Loss after num_examples_seen=1080000 epoch=27: Training - 2.1193871023156907  Testing - 2.2078537674498118\n",
      "2025-11-02 19:43:26: Loss after num_examples_seen=1120000 epoch=28: Training - 2.1193860423732223  Testing - 2.2078525786398058\n",
      "2025-11-02 19:52:05: Loss after num_examples_seen=1160000 epoch=29: Training - 2.119382719432764  Testing - 2.2078527490034956\n",
      "2025-11-02 20:00:41: Loss after num_examples_seen=1200000 epoch=30: Training - 2.119379191154013  Testing - 2.2078532859215017\n",
      "2025-11-02 20:09:21: Loss after num_examples_seen=1240000 epoch=31: Training - 2.119377261719714  Testing - 2.2078531576987452\n",
      "2025-11-02 20:17:58: Loss after num_examples_seen=1280000 epoch=32: Training - 2.1193765441384502  Testing - 2.207856601923103\n",
      "2025-11-02 20:26:35: Loss after num_examples_seen=1320000 epoch=33: Training - 2.1193747526395854  Testing - 2.207862954804328\n",
      "2025-11-02 20:35:16: Loss after num_examples_seen=1360000 epoch=34: Training - 2.1193718746549712  Testing - 2.207871629558317\n",
      "2025-11-02 20:43:58: Loss after num_examples_seen=1400000 epoch=35: Training - 2.1193694720703125  Testing - 2.207876016662726\n",
      "2025-11-02 20:52:37: Loss after num_examples_seen=1440000 epoch=36: Training - 2.119366999449564  Testing - 2.207876498774818\n",
      "2025-11-02 21:01:12: Loss after num_examples_seen=1480000 epoch=37: Training - 2.11936488715756  Testing - 2.2078763232728518\n",
      "2025-11-02 21:09:50: Loss after num_examples_seen=1520000 epoch=38: Training - 2.1193629596774017  Testing - 2.2078724449374776\n",
      "2025-11-02 21:18:37: Loss after num_examples_seen=1560000 epoch=39: Training - 2.1193609710539136  Testing - 2.2078669155862958\n",
      "2025-11-02 21:27:25: Loss after num_examples_seen=1600000 epoch=40: Training - 2.1193587653561923  Testing - 2.207871884730779\n",
      "2025-11-02 21:36:50: Loss after num_examples_seen=1640000 epoch=41: Training - 2.1193564100601208  Testing - 2.2078665693845294\n",
      "2025-11-02 21:47:02: Loss after num_examples_seen=1680000 epoch=42: Training - 2.1193542570833386  Testing - 2.207863308965936\n",
      "2025-11-02 21:56:15: Loss after num_examples_seen=1720000 epoch=43: Training - 2.1193525139857843  Testing - 2.2078706311546723\n",
      "2025-11-02 22:05:02: Loss after num_examples_seen=1760000 epoch=44: Training - 2.119351516333453  Testing - 2.2078778179730874\n",
      "2025-11-02 22:14:44: Loss after num_examples_seen=1800000 epoch=45: Training - 2.1193507278414407  Testing - 2.2078817636650307\n",
      "2025-11-02 22:24:49: Loss after num_examples_seen=1840000 epoch=46: Training - 2.1193500065675868  Testing - 2.2078836642859656\n",
      "2025-11-02 22:34:34: Loss after num_examples_seen=1880000 epoch=47: Training - 2.1193487238300586  Testing - 2.207884836594765\n",
      "2025-11-02 22:43:25: Loss after num_examples_seen=1920000 epoch=48: Training - 2.119347073546279  Testing - 2.2078855237911044\n",
      "2025-11-02 22:52:13: Loss after num_examples_seen=1960000 epoch=49: Training - 2.1193449169932546  Testing - 2.207886643132601\n"
     ]
    }
   ],
   "source": [
    "losses_next_50 = train_with_sgd(model, X_train, Y_train, nepoch=50, evaluate_loss_after=1, validation_data=(X_test, Y_test), learning_rate=4.8828125e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf8646-95e7-4452-918b-1353c7217383",
   "metadata": {},
   "source": [
    "* As you can see from above that testing loss is stable and training loss is decreasing by very small value so let's see for another 50 epochs and see if there is any improvement\n",
    "* Let's save weights first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e2ac871-4364-4c54-9fd4-1c51ea6204d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weights/U.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.U, file)\n",
    "with open('./weights/W.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.W, file)\n",
    "with open('./weights/V.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.V, file)\n",
    "with open('./weights/G.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.G, file)\n",
    "    \n",
    "with open('./weights/word_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.word_dim, file)\n",
    "with open('./weights/hidden_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.hidden_dim, file)\n",
    "with open('./weights/bptt_truncate.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.bptt_truncate, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eae36f-3929-4547-a21e-146055443fd9",
   "metadata": {},
   "source": [
    "* Let's generate poem with updated model (Trained with 150 number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e97405b-4e50-4290-9167-b9a9b6e579b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world of thy husbandry?\n",
      "who, why, fearing, and true as wantonly and upon that loves may\n",
      ",\n",
      "shall neigh no more flesh my verses must,\n",
      "and chide,\n",
      "not once,\n",
      "or at your hand th account,\n",
      "and all those beauties whereof, and meant thereby thee,\n",
      "weeds in the world's of thy worth days keep may o,\n",
      "towards with golden face of shame,\n",
      "dumb absence seemed to yellow autumn of alt'ring heart,\n",
      "and like,\n",
      "like,\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate_poem(\"The\", 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b15ef244-42bc-4e7a-9a8f-426851f486ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14176/1748315108.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  N = np.sum((len(y_i) for y_i in y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 23:24:06: Loss after num_examples_seen=0 epoch=0: Training - 2.1193413362857756  Testing - 2.207886956399361\n",
      "2025-11-02 23:34:23: Loss after num_examples_seen=40000 epoch=1: Training - 2.11933705334177  Testing - 2.20788693703879\n",
      "2025-11-02 23:44:13: Loss after num_examples_seen=80000 epoch=2: Training - 2.1193319835655458  Testing - 2.20788765514084\n",
      "2025-11-02 23:54:00: Loss after num_examples_seen=120000 epoch=3: Training - 2.119329669709507  Testing - 2.207889245205973\n",
      "2025-11-03 00:03:44: Loss after num_examples_seen=160000 epoch=4: Training - 2.119322982983664  Testing - 2.2078943467890952\n",
      "2025-11-03 00:13:42: Loss after num_examples_seen=200000 epoch=5: Training - 2.1193138099648166  Testing - 2.207902403750235\n",
      "2025-11-03 00:23:09: Loss after num_examples_seen=240000 epoch=6: Training - 2.1193092624022882  Testing - 2.2079106023838486\n",
      "2025-11-03 00:31:51: Loss after num_examples_seen=280000 epoch=7: Training - 2.119304072619418  Testing - 2.207910457522989\n",
      "2025-11-03 00:40:36: Loss after num_examples_seen=320000 epoch=8: Training - 2.119299061589193  Testing - 2.2079071444360148\n",
      "2025-11-03 00:49:28: Loss after num_examples_seen=360000 epoch=9: Training - 2.119292713904659  Testing - 2.207900686904181\n",
      "2025-11-03 00:58:05: Loss after num_examples_seen=400000 epoch=10: Training - 2.1192856752534874  Testing - 2.207889719025316\n",
      "2025-11-03 01:06:46: Loss after num_examples_seen=440000 epoch=11: Training - 2.119276179289342  Testing - 2.2078763787146665\n",
      "2025-11-03 01:15:23: Loss after num_examples_seen=480000 epoch=12: Training - 2.1192687920261224  Testing - 2.20786689959922\n",
      "2025-11-03 01:23:58: Loss after num_examples_seen=520000 epoch=13: Training - 2.11925928561832  Testing - 2.2078621161199785\n",
      "2025-11-03 01:32:50: Loss after num_examples_seen=560000 epoch=14: Training - 2.1192501199510207  Testing - 2.207858981763375\n",
      "2025-11-03 01:41:34: Loss after num_examples_seen=600000 epoch=15: Training - 2.1192448666047143  Testing - 2.207856394703901\n",
      "2025-11-03 01:50:22: Loss after num_examples_seen=640000 epoch=16: Training - 2.11924581326979  Testing - 2.2078535976148914\n",
      "Setting learning rate to 1.52587890625e-07\n",
      "2025-11-03 01:59:15: Loss after num_examples_seen=680000 epoch=17: Training - 2.1192475457618665  Testing - 2.2078509326520743\n",
      "Setting learning rate to 7.62939453125e-08\n",
      "2025-11-03 02:08:02: Loss after num_examples_seen=720000 epoch=18: Training - 2.119248910889476  Testing - 2.2078497937401416\n",
      "Setting learning rate to 3.814697265625e-08\n",
      "2025-11-03 02:16:52: Loss after num_examples_seen=760000 epoch=19: Training - 2.1192491100628694  Testing - 2.2078491917878047\n",
      "Setting learning rate to 1.9073486328125e-08\n",
      "2025-11-03 02:25:53: Loss after num_examples_seen=800000 epoch=20: Training - 2.1192491271044522  Testing - 2.207848887306127\n",
      "Setting learning rate to 9.5367431640625e-09\n",
      "2025-11-03 02:34:55: Loss after num_examples_seen=840000 epoch=21: Training - 2.1192491182276463  Testing - 2.2078487336929014\n",
      "2025-11-03 02:43:53: Loss after num_examples_seen=880000 epoch=22: Training - 2.1192491026155627  Testing - 2.2078485835679555\n",
      "2025-11-03 02:52:26: Loss after num_examples_seen=920000 epoch=23: Training - 2.119249081410175  Testing - 2.207848436926627\n",
      "2025-11-03 03:01:00: Loss after num_examples_seen=960000 epoch=24: Training - 2.1192490556658905  Testing - 2.207848293752989\n",
      "2025-11-03 03:09:32: Loss after num_examples_seen=1000000 epoch=25: Training - 2.1192490263527906  Testing - 2.2078481540214625\n",
      "2025-11-03 03:18:05: Loss after num_examples_seen=1040000 epoch=26: Training - 2.1192489943587476  Testing - 2.2078480176984825\n",
      "2025-11-03 03:26:35: Loss after num_examples_seen=1080000 epoch=27: Training - 2.119248960490999  Testing - 2.207847884743797\n",
      "2025-11-03 03:35:06: Loss after num_examples_seen=1120000 epoch=28: Training - 2.1192489254781735  Testing - 2.207847755111626\n",
      "2025-11-03 03:43:37: Loss after num_examples_seen=1160000 epoch=29: Training - 2.1192488899711415  Testing - 2.2078476287516637\n",
      "2025-11-03 03:52:07: Loss after num_examples_seen=1200000 epoch=30: Training - 2.1192488545451402  Testing - 2.207847505609742\n",
      "2025-11-03 04:00:35: Loss after num_examples_seen=1240000 epoch=31: Training - 2.1192488197007604  Testing - 2.207847385628543\n",
      "2025-11-03 04:09:05: Loss after num_examples_seen=1280000 epoch=32: Training - 2.119248785865307  Testing - 2.207847268747853\n",
      "2025-11-03 04:17:32: Loss after num_examples_seen=1320000 epoch=33: Training - 2.1192487533936797  Testing - 2.2078471549047713\n",
      "2025-11-03 04:26:06: Loss after num_examples_seen=1360000 epoch=34: Training - 2.119248722568547  Testing - 2.207847044033701\n",
      "2025-11-03 04:34:54: Loss after num_examples_seen=1400000 epoch=35: Training - 2.119248693600062  Testing - 2.207846936066138\n",
      "2025-11-03 04:44:05: Loss after num_examples_seen=1440000 epoch=36: Training - 2.119248666624277  Testing - 2.2078468309302246\n",
      "2025-11-03 04:53:55: Loss after num_examples_seen=1480000 epoch=37: Training - 2.1192486417010588  Testing - 2.20784672855009\n",
      "2025-11-03 05:04:00: Loss after num_examples_seen=1520000 epoch=38: Training - 2.119248618810612  Testing - 2.207846628845189\n",
      "2025-11-03 05:12:53: Loss after num_examples_seen=1560000 epoch=39: Training - 2.1192485978497575  Testing - 2.2078465317291833\n",
      "2025-11-03 05:22:28: Loss after num_examples_seen=1600000 epoch=40: Training - 2.119248578627172  Testing - 2.207846437109035\n",
      "2025-11-03 05:31:23: Loss after num_examples_seen=1640000 epoch=41: Training - 2.1192485608591234  Testing - 2.2078463448838983\n",
      "2025-11-03 05:40:13: Loss after num_examples_seen=1680000 epoch=42: Training - 2.1192485441662074  Testing - 2.2078462549442577\n",
      "2025-11-03 05:49:25: Loss after num_examples_seen=1720000 epoch=43: Training - 2.1192485280719797  Testing - 2.207846167171328\n",
      "2025-11-03 05:58:24: Loss after num_examples_seen=1760000 epoch=44: Training - 2.1192485120057394  Testing - 2.207846081437058\n",
      "2025-11-03 06:07:11: Loss after num_examples_seen=1800000 epoch=45: Training - 2.1192484953107127  Testing - 2.2078459976048315\n",
      "2025-11-03 06:15:57: Loss after num_examples_seen=1840000 epoch=46: Training - 2.1192484772591773  Testing - 2.207845915531376\n",
      "2025-11-03 06:24:39: Loss after num_examples_seen=1880000 epoch=47: Training - 2.119248457076678  Testing - 2.2078458350699854\n",
      "2025-11-03 06:33:23: Loss after num_examples_seen=1920000 epoch=48: Training - 2.1192484339745716  Testing - 2.2078457560753435\n",
      "2025-11-03 06:42:05: Loss after num_examples_seen=1960000 epoch=49: Training - 2.1192484071900046  Testing - 2.207845678409811\n"
     ]
    }
   ],
   "source": [
    "losses_next_50 = train_with_sgd(model, X_train, Y_train, nepoch=50, evaluate_loss_after=1, validation_data=(X_test, Y_test), learning_rate=3.0517578125e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6776c-88bf-4214-abf9-5619d74d0280",
   "metadata": {},
   "source": [
    "* As you can see from above that the loss is stablized for both training and testing set further training will make no improvement\n",
    "* So let's save our model weights (trained on 200 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61ae7f82-8678-4d7b-883b-70c9007d6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./weights/U.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.U, file)\n",
    "with open('./weights/W.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.W, file)\n",
    "with open('./weights/V.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.V, file)\n",
    "with open('./weights/G.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.G, file)\n",
    "    \n",
    "with open('./weights/word_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.word_dim, file)\n",
    "with open('./weights/hidden_dim.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.hidden_dim, file)\n",
    "with open('./weights/bptt_truncate.pkl', 'wb') as file:    \n",
    "    pickle.dump(model.bptt_truncate, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486acae-bcaa-49b3-b608-093d99b2c089",
   "metadata": {},
   "source": [
    "* Lets generate poem with one word supplied and of 100 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "39ceb4f3-48b8-4dcb-8fde-85a7f13e5218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world of thy husbandry?\n",
      "who, why, fearing, and true as wantonly and upon that loves may\n",
      ",\n",
      "shall neigh no more flesh my verses must,\n",
      "and chide,\n",
      "not once,\n",
      "or at your hand th account,\n",
      "and all those beauties whereof, and meant thereby thee,\n",
      "weeds in the world's of thy worth days keep may o,\n",
      "towards with golden face of shame,\n",
      "dumb absence seemed to yellow autumn of alt'ring heart,\n",
      "and like,\n",
      "like,\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate_poem(\"The\", 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26a31e-eab7-4d6a-a0a7-6cc10b0679c3",
   "metadata": {},
   "source": [
    "* Now lets generate poem giving multiple words as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0cc368b-217c-4556-ac12-8f763a431d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or say your bounty.\n",
      "o, let, and true, barrenly to leap words lengths upon my brow name\n",
      "ill in inward of thy will,\n",
      "and, frantic fair, so not spend when others fear thee hence?\n",
      "o, know best, to hope is till they\n",
      "the lovely of the shore\n",
      ",\n",
      "and in our faults by lies eye hand as after sunset,\n",
      "chiding my appeal says\n",
      "where must grew;\n",
      "but that tongue music burdens say make grow;\n",
      "and other gainst time's scythe,\n",
      "and captive good attending captain ill,\n",
      "which shall not love to thy friend?\n",
      "or whether shall you with fortune alone\n",
      ",\n",
      "that time and beauty so sweetly\n",
      ";\n",
      "and he of marjoram are save tell from thee,\n",
      "who ll beds art, even for all the day of mine own love's might.\n",
      "and therefore say of my love's,\n",
      "and, constant stars, that she mine\n",
      "thee :,\n",
      "fair, whom fortune, now you had those form spent thou dost created thou wilt wilt prove,\n",
      "which shall above me to my bed,\n",
      "and to his palate doth prepare should fill\n",
      "from the world of all posterity;\n",
      "how, like, and this, thou, dear thy heart in me behold\n",
      "if,\n",
      "serving think should for ornament,\n",
      "which proves still, if the world intents.\n",
      "pity, what a mansion title which the blind be live\n",
      "in lease of thy love, when nature my friend,\n",
      "finding thy heart, which should no such matter.\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate_poem(\"to be or\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
