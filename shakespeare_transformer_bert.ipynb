{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a8a9d0-2e2d-4db7-bf44-e8538a71ca0e",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer model\n",
    "\n",
    "* So let's start by importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a2f435-34b2-4c2c-83ee-7cf7503393b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:51:05.736857: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 04:51:05.794918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 04:51:08.636803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1071c2dd-3621-4c40-9e18-755b01fdc680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed for reproducability\n",
    "tf.random.set_seed(1234)\n",
    "# Setting autotune which automatically let's tensorflow to tune data pipeline performance\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# Printing tensorflow version\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0ec21-a53c-424b-a052-3446e7b02188",
   "metadata": {},
   "source": [
    "### Transformer Hyperparameters\n",
    "\n",
    "* To keep this example small and relatively fast, we reduce values for *num_layers, d_model, and units*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6ef8ba-09c1-40a9-b334-52671c2a1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence length\n",
    "MAX_LENGTH = 20\n",
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 50000\n",
    "# For tf.data.Dataset\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "# For Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a02cc0-ce96-4662-8356-0b2d72e2efae",
   "metadata": {},
   "source": [
    "### Shakespeare sonnets preprocessing\n",
    "\n",
    "* Let's write a function for cleaning numbers and hyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6f1b12-22ba-4eb5-811d-2755efe668ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    \"\"\"Function for cleaning numbers\"\"\"\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a95e4-a434-45c2-9aff-1883c2a7732d",
   "metadata": {},
   "source": [
    "* Now let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b40a6104-bfc6-439b-9bb3-e6ab70d23eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "print( \"Reading txt file...\")\n",
    "with open('./data/shakespeare-sonnets.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e077d9e-4592-444a-9ad0-6ae8a257bb81",
   "metadata": {},
   "source": [
    "* Now let's replace punctuations with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a30a39d-1833-4726-9647-6d1069316cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_  \")\n",
    "text = text.replace(\":\", \" _comma_  \")\n",
    "text = text.replace(\";\", \" _comma_  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc890e8-d87e-46de-823f-7686f70591b1",
   "metadata": {},
   "source": [
    "* Now let's replace ending of lines with some punctuations with full stop\n",
    "* Remove comma and tabs and extra spaces\n",
    "* Use written clean numbers to remove numbers\n",
    "* Expand contractions\n",
    "* Convert all text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebe8595-86f7-4398-868a-a4c9171ea402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace line endings with full stop\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "# Remove comma\n",
    "text = text.replace('\"',\"\")\n",
    "# Remove tabs\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "# Remove extra spaces\n",
    "text = text.replace(\"  \", \"\")\n",
    "# Remove numbers\n",
    "text = clean_numbers(text)\n",
    "# Remove soace\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "text = _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "# Expanding contractions\n",
    "text = re.sub(r\"i'm\", \"i am\", text)\n",
    "text = re.sub(r\"he's\", \"he is\", text)\n",
    "text = re.sub(r\"she's\", \"she is\", text)\n",
    "text = re.sub(r\"it's\", \"it is\", text)\n",
    "text = re.sub(r\"that's\", \"that is\", text)\n",
    "text = re.sub(r\"what's\", \"that is\", text)\n",
    "text = re.sub(r\"where's\", \"where is\", text)\n",
    "text = re.sub(r\"how's\", \"how is\", text)\n",
    "text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "text = re.sub(r\"\\'re\", \" are\", text)\n",
    "text = re.sub(r\"\\'d\", \" would\", text)\n",
    "text = re.sub(r\"\\'re\", \" are\", text)\n",
    "text = re.sub(r\"won't\", \"will not\", text)\n",
    "text = re.sub(r\"can't\", \"cannot\", text)\n",
    "text = re.sub(r\"n't\", \" not\", text)\n",
    "text = re.sub(r\"n'\", \"ng\", text)\n",
    "text = re.sub(r\"'bout\", \"about\", text)\n",
    "# Converting all text to lowercase\n",
    "text = text.lower()\n",
    "# Replacing lowercase i to uppercase I\n",
    "text = text.replace('i ', 'I ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5fceb-e8ff-444d-beb6-728734a06234",
   "metadata": {},
   "source": [
    "* Let's now check the length of text and few lines of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd41ea9b-133e-4be2-8402-1cde2353c302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106278"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6f80c5-113f-4a78-b693-84bc6cf8e386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from fairest creatures we desire increase _eol_ that thereby beauty's rose might never die _eol_ but _comma_ as the riper should by time decease _eol_ his tender heir might bear his memory. but thou _comma_ contracted to thine own bright eyes _eol_ feed'st thy light's flame with selfsubstantial fuel _eol_ making a famine where abundance lies _eol_ thyself thy foe _comma_ to thy sweet self too cruel. thou that art now the world's fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and _comma_ tender churl _comma_ mak'st waste in niggarding. pity the world _comma_ or else this glutton be to eat the world's due _comma_ by the grave and thee. when forty winters shall besiege thy brow and dig deep trenches in thy beauty's field _eol_ thy youth's proud livery _comma_ so gazed on now _eol_ will be a tattered weed of small worth held. then being asked where all thy beauty lies _eol_ where all the treasure of thy lusty days _eol_ to say within thine own deepsunken eyes were an alleating shame and thriftless praise. how much more praise deserved thy beauty's use if thou couldst answer this fair child of mine shall sum my count and make my old excuse _comma_ proving his beauty by succession thine. this were to be new made when thou art old and see thy blood warm when thou feel'st it cold. look in thy glass and tell the face thou viewest now is the time that face should form another _eol_ whose fresh repair if now thou not renewest _eol_ thou dost beguile the world _comma_ unbless some mother. for where is she so fair whose uneared womb disdains the tillage of thy husbandry. or who is he so fond will be the tomb of his selflove _comma_ to stop posterity. thou art thy mother's glass _comma_ and she in thee calls back the lovely april of her prime _comma_ so thou through windows of thine age shalt see _eol_ despite of wrinkles _comma_ this thy golden time. but if thou live remembered not to be _eol_ die single _comma_ and thine image dies w\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71281ce5-ccdc-466c-81d4-b551361fab92",
   "metadata": {},
   "source": [
    "* Let's harvest all ngrams up to 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d511e96c-4cec-4601-8def-59004ddae7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram- 2 length: 13985\n",
      "ngram- 3 length: 18762\n",
      "ngram- 4 length: 19415\n",
      "ngram- 5 length: 19497\n",
      "ngram- 6 length: 19522\n",
      "ngram- 7 length: 19537\n",
      "ngram- 8 length: 19550\n",
      "ngram- 9 length: 19561\n",
      "ngram- 10 length: 19571\n",
      "ngram- 11 length: 19581\n",
      "ngram- 12 length: 19588\n",
      "ngram- 13 length: 19595\n",
      "ngram- 14 length: 19602\n",
      "ngram- 15 length: 19608\n",
      "ngram- 16 length: 19613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 130.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram- 17 length: 19616\n",
      "ngram- 18 length: 19618\n",
      "ngram- 19 length: 19620\n",
      "ngram- 20 length: 19621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ngrams_up_to_20 = []\n",
    "for i in tqdm(range(2, 21)):\n",
    "    ngram_counts = Counter(ngrams(text.split(), i))\n",
    "    print('ngram-', i, 'length:', len(ngram_counts))\n",
    "    ngrams_up_to_20.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3510d0-a79f-4445-a96b-56c671d86bc2",
   "metadata": {},
   "source": [
    "* As you canm see that we have generated ngrams up to 20 so now let's view most common n grams\n",
    "* Let's view most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e9984f-6259-436c-8294-6a0fe7ebde3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('_eol_', 'and'), 150),\n",
       " (('_comma_', 'and'), 107),\n",
       " (('o', '_comma_'), 44),\n",
       " (('_comma_', 'but'), 39),\n",
       " (('love', '_comma_'), 37),\n",
       " (('my', 'love'), 37),\n",
       " (('_eol_', 'that'), 36),\n",
       " (('thou', 'art'), 31),\n",
       " (('in', 'the'), 31),\n",
       " (('_comma_', 'I'), 30)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_up_to_20[0].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8bb89-6bb2-4be8-9deb-fb7f900496ee",
   "metadata": {},
   "source": [
    "* Now let's view most common trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495cf9c5-5f16-4997-8d3d-7ce32f74285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('_comma_', 'love', '_comma_'), 8),\n",
       " (('my', 'love', '_comma_'), 7),\n",
       " (('_eol_', 'and', 'in'), 7),\n",
       " (('_eol_', 'and', 'all'), 6),\n",
       " (('_comma_', 'and', 'I'), 6),\n",
       " (('some', 'in', 'their'), 6),\n",
       " (('_comma_', 'when', 'I'), 5),\n",
       " (('_comma_', 'and', '_comma_'), 5),\n",
       " (('_eol_', 'and', 'you'), 5),\n",
       " (('thy', 'love', '_comma_'), 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_up_to_20[1].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09eb62-ee73-44fd-af8f-04d0861b97b5",
   "metadata": {},
   "source": [
    "* Now let's view most common 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea119bde-598f-4cef-9b85-e73dd8c7e082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('_comma_', 'why', 'dost', 'thou'), 3),\n",
       " (('when', 'I', 'have', 'seen'), 3),\n",
       " (('_comma_', 'some', 'in', 'their'), 3),\n",
       " (('_eol_', 'some', 'in', 'their'), 3),\n",
       " (('fair', '_comma_', 'kind', '_comma_'), 3),\n",
       " (('_comma_', 'kind', '_comma_', 'and'), 3),\n",
       " (('kind', '_comma_', 'and', 'true'), 3),\n",
       " (('_comma_', 'that', 'I', 'have'), 3),\n",
       " (('for', 'I', 'have', 'sworn'), 3),\n",
       " (('the', 'treasure', 'of', 'thy'), 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_up_to_20[2].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2620af",
   "metadata": {},
   "source": [
    "### Tokenization of words into integers\n",
    "\n",
    "* Let's now split sentences and view first four sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c13d2bec-2556-4ffe-8792-f9497848467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"from fairest creatures we desire increase _eol_ that thereby beauty's rose might never die _eol_ but _comma_ as the riper should by time decease _eol_ his tender heir might bear his memory\",\n",
       " \" but thou _comma_ contracted to thine own bright eyes _eol_ feed'st thy light's flame with selfsubstantial fuel _eol_ making a famine where abundance lies _eol_ thyself thy foe _comma_ to thy sweet self too cruel\",\n",
       " \" thou that art now the world's fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and _comma_ tender churl _comma_ mak'st waste in niggarding\",\n",
       " \" pity the world _comma_ or else this glutton be to eat the world's due _comma_ by the grave and thee\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = text.split('.')\n",
    "training_data[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002d4be-7863-4ce3-b535-c7f1a9a8b3cd",
   "metadata": {},
   "source": [
    "* Now let's remove spaces at start and end from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62476fc-cbb5-48c1-bd84-000be87c0030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"from fairest creatures we desire increase _eol_ that thereby beauty's rose might never die _eol_ but _comma_ as the riper should by time decease _eol_ his tender heir might bear his memory\",\n",
       " \"but thou _comma_ contracted to thine own bright eyes _eol_ feed'st thy light's flame with selfsubstantial fuel _eol_ making a famine where abundance lies _eol_ thyself thy foe _comma_ to thy sweet self too cruel\",\n",
       " \"thou that art now the world's fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and _comma_ tender churl _comma_ mak'st waste in niggarding\",\n",
       " \"pity the world _comma_ or else this glutton be to eat the world's due _comma_ by the grave and thee\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(training_data)):\n",
    "    training_data[i] = training_data[i].strip()\n",
    "training_data[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d66647",
   "metadata": {},
   "source": [
    "### Building Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a30b9f-b636-4c7a-9f42-08a3a4d0ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(training_data)\n",
    "# Convert the training data into sequences of tokens\n",
    "sequences = tokenizer.texts_to_sequences(training_data)\n",
    "# Pad the sequences to have the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e2129-9cb2-4414-bbe9-96d4bfed6b93",
   "metadata": {},
   "source": [
    "* Now let's see length of tokenizer word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "628de141-71f5-4fde-8a96-99d85f2b5bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bd1c5-9359-43f3-aeec-358e3bcdfa69",
   "metadata": {},
   "source": [
    "* Let's see the token numbers of comma and eol in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "178723be-60df-4618-97e9-7e409dc79417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['comma'], tokenizer.word_index['eol']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a9b11-44e6-4eb9-ac75-38e6e678adf9",
   "metadata": {},
   "source": [
    "* Now let's define our start token, end token and vocabulary size\n",
    "* Note that vocabulary size will be tokenizer word count + 3 as vocab will include start token, end token and padding as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284b73e9-160d-4fee-b29f-5eb62fa0ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = len(tokenizer.word_index) + 1\n",
    "END_TOKEN = len(tokenizer.word_index) + 2\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3cc17b0-fb7f-43c5-9486-f4b74e8598fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3228, 3229, 3230)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN, VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20276a3f",
   "metadata": {},
   "source": [
    "* Let's write a function which takes ngrams and returns true or false based on whether it includes . or ‘ or ’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7d3392-c9d8-456b-a5b7-3dcdb0a163f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_periods(ngram):\n",
    "    \"\"\"Function which returns false if given n gram contains . or ’ or ‘\"\"\"\n",
    "    for wrd in ngram[0]:\n",
    "        if '.' in wrd or \"’\" in wrd or \"‘\" in wrd:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79caaba1-b56a-41f5-993f-bfa4d7968380",
   "metadata": {},
   "source": [
    "* Let's now write a filter function which uses above function over ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e4318c-5491-4ec0-a72e-e6cb00e798dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(ngrams):\n",
    "    \"\"\"Filter function which uses remove_periods function for filtering\"\"\"\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f0639",
   "metadata": {},
   "source": [
    "* Let's write a funcion for shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87f2abf9-dea4-49fd-b24a-b77f13dd0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_yates (arr1, arr2):\n",
    "    # We will Start from the last element\n",
    "    # and swap one by one.\n",
    "    n = len(arr1)\n",
    "    if n != len(arr2):\n",
    "        return None\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        # Pick a random index from 0 to i\n",
    "        j = random.randint(0, i)\n",
    "        # Swap arr[i] with the element at random index\n",
    "        arr1[i], arr1[j] = arr1[j], arr1[i]\n",
    "        arr2[i], arr2[j] = arr2[j], arr2[i]\n",
    "    return arr1, arr2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82904f45",
   "metadata": {},
   "source": [
    "* Let's now get the first words that Shakespeare usually begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0abe501-1460-41e4-a869-5ae89129f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_1299065/1809355186.py:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  first_word_counts = Counter([p.replace('. ', '') for p in re.findall('\\..[^\" \"]*', text.lower())])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('o', 48),\n",
       " ('but', 41),\n",
       " ('for', 31),\n",
       " ('then', 25),\n",
       " ('so', 24),\n",
       " ('if', 23),\n",
       " ('the', 23),\n",
       " ('i', 18),\n",
       " ('and', 17),\n",
       " ('how', 15)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_counts = Counter([p.replace('. ', '') for p in re.findall('\\..[^\" \"]*', text.lower())])\n",
    "first_word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11e5f8-859a-4fe6-b528-817533223a2c",
   "metadata": {},
   "source": [
    "* Let's start preparing x_train and y_train datasets\n",
    "* So let's first add the these first words in y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fce71f0b-61c8-4b30-a8fe-7df88a37a862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[58], [20], [15], [41], [23], [45], [4], [8], [3], [72]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [[tokenizer.word_index[w[0]]] for w in first_word_counts.most_common() if w[0] != '[]']\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012f02c-e044-4009-ba7e-351da986e502",
   "metadata": {},
   "source": [
    "* Now let's add start token in x_train as these y_train words are the start words so x_train should have start tokens as these tokens follow start words\n",
    "* So now x_train and y_train contains structure `start token -> start word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a42a12a-85c3-45f5-9c10-9c44507112c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228],\n",
       " [3228]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [[START_TOKEN] for i in range(len(y_train))]\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab8764",
   "metadata": {},
   "source": [
    "* Now let's proceed to bigrams\n",
    "* In bigrams x_train will contain first word and y_train will contain second word as first word is followed by second word\n",
    "* So structure of x_train and y_train in bigram is `first word -> second word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f318e15f-8822-4703-a0b9-ea7ed7916268",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_to_learn = ngrams_up_to_20[0]\n",
    "X_train_2 = [[tokenizer.word_index[sent[0][0]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in tokenizer.word_index and sent[0][1] in tokenizer.word_index]\n",
    "y_train_2 = [[tokenizer.word_index[sent[0][1]]] for sent in my_filter(bigrams_to_learn.most_common())\n",
    "                  if sent[0][0] in tokenizer.word_index and sent[0][1] in tokenizer.word_index]\n",
    "X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03c89d26-b7b2-45d8-bbf2-e1e945a47880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2076], [34], [28], [1275], [1161]], [[26], [208], [26], [13], [382]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2[:5], y_train_2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1805747-0c8e-4366-9078-eca8d0bd0ee4",
   "metadata": {},
   "source": [
    "* Let's extend our x_train and y_train dataset with bigram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "498c28cb-9968-407f-85a7-0510c353d358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10756, 10756)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_2), len(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a28ddef-f38f-4532-9228-523d40afbf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164, 164)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ae1b824-d62e-478c-aca6-389271d7e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_2)\n",
    "y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8609e79-276a-4594-89e4-adf3e39709b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10920, 10920)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89731d63",
   "metadata": {},
   "source": [
    "* Now let's proceed with remaining ngrams up to 20 with same process and extend our x_train and y_train\n",
    "* Here with ngrams three and above x_train will not have last word and y_train will not have first word so it follows next word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06f19651-b7ca-4496-a1de-b23d018698d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 22.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1, len(ngrams_up_to_20))):\n",
    "    ngrams_to_learn = ngrams_up_to_20[i]\n",
    "    X_train_2 = [[tokenizer.word_index[w] for w in sent[0][:-1]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "                   if all([w in tokenizer.word_index for w in sent[0]])]\n",
    "    y_train_2 = [[tokenizer.word_index[w] for w in sent[0][1:]] for sent in my_filter(ngrams_to_learn.most_common())\n",
    "                   if all([w in tokenizer.word_index for w in sent[0]])]\n",
    "    # Shuffling data\n",
    "    X_train_2, y_train_2 = fisher_yates(X_train_2, y_train_2)\n",
    "    # Extending training data with current data\n",
    "    X_train.extend(X_train_2)\n",
    "    y_train.extend(y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5628c20-947f-4716-b6cb-adfdeeb1d6f5",
   "metadata": {},
   "source": [
    "* Let's now check the length of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf45244c-ff08-42f9-821e-5d13990bd3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61813, 61813)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c967279-64ca-4097-b573-7ce86404bbdc",
   "metadata": {},
   "source": [
    "* Let's see random 10 samples from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57328fa0-3ea3-4e79-b963-0a590b2f185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([3], [11]), ([51, 5, 177, 45], [5, 177, 45, 31]), ([17, 16, 5], [16, 5, 4]), ([12, 1111, 6, 1112], [1111, 6, 1112, 1113]), ([4, 497, 1149, 7, 6, 401], [497, 1149, 7, 6, 401, 1985]), ([40, 1238, 2312, 5, 4, 2313, 2314, 3], [1238, 2312, 5, 4, 2313, 2314, 3, 529]), ([5, 205, 6], [205, 6, 183]), ([88, 63, 2306, 10, 6, 176, 24], [63, 2306, 10, 6, 176, 24, 117]), ([3228], [4]), ([156, 10, 18, 256, 16], [10, 18, 256, 16, 5])]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(zip(X_train, y_train)), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed920452-98f6-4086-ae59-aad79c3bf849",
   "metadata": {},
   "source": [
    "* Now let's get the last n (3 to 20) words containing 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8378d469-79a2-45b5-b495-81c5d619c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 2053.12it/s]\n"
     ]
    }
   ],
   "source": [
    "last_n_words = []\n",
    "for i in tqdm(range(3, 21)):\n",
    "    tokenized_sentences_500 = random.sample(list(sequences), 500)\n",
    "    for s in tokenized_sentences_500:\n",
    "        last_n_words.append(s[::-1][:i][::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99062f0c-4121-4cb4-b284-d182676a9416",
   "metadata": {},
   "source": [
    "* Let's print 10 samples of last n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "065ec430-91d1-419c-a6d2-d2f282fcf9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102, 1309, 5, 204, 4, 855, 3, 346, 7, 33, 18], [9, 15, 96, 8, 97, 2, 408, 6, 156, 13, 55, 7, 11, 147], [37, 45, 53, 40, 1, 76, 14, 6, 378, 1214, 2, 9, 3140, 3141, 44, 53, 79, 3142], [129, 37, 118], [1, 22, 31, 320, 15, 67], [72, 68, 31], [13, 1302, 98, 5, 17, 43, 133, 2857, 1, 133, 405, 1, 53, 43, 20, 2858, 7, 19, 613, 149], [98, 168, 17, 84, 1356, 158, 2, 3029, 10, 4, 351, 135, 3030], [2, 2991, 78, 65, 1, 3, 25, 9, 14, 10, 17], [1, 5, 855, 856, 2, 67, 276, 2616, 1, 289, 152, 2617]]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(last_n_words, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8defe847-ad9d-4fbc-a6fe-9c347c07ec6c",
   "metadata": {},
   "source": [
    "* Let's see length of last n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09882889-6809-40f7-90f1-209a3b41a70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e22e3-372e-49f6-969f-343d4ea42249",
   "metadata": {},
   "source": [
    "* Now as we have last n words let's create end of sentence training pairs where y_train contains end token at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "227e2135-6791-48a0-8761-eeeb6e5a5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eos = []\n",
    "y_train_eos = []\n",
    "for s in last_n_words:\n",
    "    if 1 < len(s):\n",
    "        X_train_eos.append(s)\n",
    "        y_train_eos.append(s[1:] + [END_TOKEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c025bf8-cf1a-4529-b4f0-57d9b543f260",
   "metadata": {},
   "source": [
    "* Let's now check the length of the end of sentence training pairs and let's view first 10 samples from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bed6195-95b5-4b6e-a2de-3915ca796272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8991, 8991)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_eos), len(y_train_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0cb64b1-3272-4912-a2e2-dc45d5387b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[53, 62, 3028],\n",
       "  [59, 23, 1504],\n",
       "  [87, 22, 1274],\n",
       "  [173, 11, 3112],\n",
       "  [10, 31, 92],\n",
       "  [1269, 34, 2464],\n",
       "  [99, 43, 169],\n",
       "  [25, 2121, 2122],\n",
       "  [19, 1057, 149],\n",
       "  [1, 167, 766]],\n",
       " [[62, 3028, 3229],\n",
       "  [23, 1504, 3229],\n",
       "  [22, 1274, 3229],\n",
       "  [11, 3112, 3229],\n",
       "  [31, 92, 3229],\n",
       "  [34, 2464, 3229],\n",
       "  [43, 169, 3229],\n",
       "  [2121, 2122, 3229],\n",
       "  [1057, 149, 3229],\n",
       "  [167, 766, 3229]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_eos[:10], y_train_eos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a38c5e-c346-419a-86cf-ab8516085e9c",
   "metadata": {},
   "source": [
    "* Let's now extend dataset with this one as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4ff01ae-2ccf-4923-b797-4fab307582e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.extend(X_train_eos)\n",
    "y_train.extend(y_train_eos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc965286-924a-4eae-800d-8fea51043ad2",
   "metadata": {},
   "source": [
    "* Let's check the length of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e23d090-bb11-4430-9b5a-67f5555d60b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70804, 70804)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6803a3",
   "metadata": {},
   "source": [
    "### Pickle\n",
    "* Now we're ready to pickle our training dataset, our tokenized sentences, and our dictionaries and reverse dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9604d251-8410-4de7-8af3-a01a74d6e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/X_train_shakespeare_ngrams.pkl', 'wb') as file: \n",
    "    pickle.dump(X_train, file)\n",
    "\n",
    "with open('./data/y_train_shakespeare_ngrams.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "\n",
    "with open('./data/tokenized_sentences_shakespeare.pkl', 'wb') as file:\n",
    "    pickle.dump(sequences, file)\n",
    "\n",
    "with open('./data/word_to_index_shakespeare.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer.word_index, file)\n",
    "\n",
    "with open('./data/index_to_word_shakespeare.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer.index_word, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0e4bf-659d-41a1-8e9c-7dc829f1226f",
   "metadata": {},
   "source": [
    "* Loading pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b32c5e2-ffac-4acb-9941-5f0076839678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X_train_shakespeare_ngrams.pkl', 'rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "\n",
    "with open('data/y_train_shakespeare_ngrams.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "\n",
    "with open('data/tokenized_sentences_shakespeare.pkl', 'rb') as file:\n",
    "    sequences = pickle.load(file)\n",
    "\n",
    "with open('data/word_to_index_shakespeare.pkl', 'rb') as file:\n",
    "    tokenizer.word_index = pickle.load(file)\n",
    "\n",
    "with open('data/index_to_word_shakespeare.pkl', 'rb') as file:\n",
    "    tokenizer.index_word = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2b919",
   "metadata": {},
   "source": [
    "### Padding the Dataset\n",
    "\n",
    "* Now let's pad the dataset I have set MAX_LENGTH to 20 as dataset contains maximum 20 ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72cd30fe-8ba4-4bb9-9b75-838086df0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
    "y_train_p = tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f5e8e-b4ec-418f-b86a-34145ac1b02c",
   "metadata": {},
   "source": [
    "* Let's check the length of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df4a1164-ed2c-4290-8345-8b23c21e2176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70804, 70804)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_p), len(y_train_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a0855-4eb2-4fc4-943f-83de3de5e9b3",
   "metadata": {},
   "source": [
    "* Let's view some data to see if padding is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ccf3c59-afb3-4a1c-a72a-3fbccd557beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 102,   69, 3093,  338,    4, 1350,    9,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_p[50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "425a8480-c423-41fe-84b4-f6f05bd6b509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  69, 3093,  338,    4, 1350,    9,  603,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_p[50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188ec41-5413-426b-b494-d149b8b8e92b",
   "metadata": {},
   "source": [
    "* Now let's create terraform dataset and pass data to it and set chache, shuffle, batch and autotune (for better performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a2682c4-9f04-4909-8fd4-b2c36d9372df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765360273.047738 1299065 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79086 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': X_train_p\n",
    "    },\n",
    "    {\n",
    "        'outputs': y_train_p\n",
    "    }\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d42c9db2-6915-4bd1-b7bf-28fe3ccdf593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=({'inputs': TensorSpec(shape=(None, 20), dtype=tf.int32, name=None)}, {'outputs': TensorSpec(shape=(None, 20), dtype=tf.int32, name=None)})>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81e522-81af-4d3d-b539-08cac36853c4",
   "metadata": {},
   "source": [
    "* Let's view the first batch from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2440c17e-6919-4b3a-89cf-87fbfadc68ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: {'inputs': <tf.Tensor: shape=(64, 20), dtype=int32, numpy=\n",
      "array([[  12,    0,    0, ...,    0,    0,    0],\n",
      "       [1028,   13,    0, ...,    0,    0,    0],\n",
      "       [ 276,    0,    0, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  53, 1341,    0, ...,    0,    0,    0],\n",
      "       [ 119,   40,    0, ...,    0,    0,    0],\n",
      "       [  32,    4,    0, ...,    0,    0,    0]],\n",
      "      shape=(64, 20), dtype=int32)>}\n",
      "\n",
      "output: {'outputs': <tf.Tensor: shape=(64, 20), dtype=int32, numpy=\n",
      "array([[ 196,    0,    0, ...,    0,    0,    0],\n",
      "       [  13, 1029,    0, ...,    0,    0,    0],\n",
      "       [  31,    0,    0, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [1341,  209,    0, ...,    0,    0,    0],\n",
      "       [  40,  718,    0, ...,    0,    0,    0],\n",
      "       [   4, 1575,    0, ...,    0,    0,    0]],\n",
      "      shape=(64, 20), dtype=int32)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:51:14.464071: W tensorflow/core/kernels/data/cache_dataset_ops.cc:917] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for (batch, (inp, tar)) in enumerate(dataset):\n",
    "    print(\"input:\", inp)\n",
    "    print()\n",
    "    print(\"output:\", tar)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e9f3f",
   "metadata": {},
   "source": [
    "* Let's write the scaled dot-product attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "* This function tells how much each word should pay attention to every other word in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb58e4e6-16d1-49af-9d2a-0e401311f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, attention_mask=None):\n",
    "    \"\"\"Function to calculate the attention weights\"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = tf.cast(matmul_qk, tf.float32) / tf.math.sqrt(dk)\n",
    "    # add the mask to the scaled tensor.\n",
    "    if attention_mask is not None:\n",
    "        #scaled_attention_logits += (mask * -1e9)  \n",
    "        scaled_attention_logits += (tf.cast(attention_mask, tf.float32) * -1e9)\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    #output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    output = tf.matmul(attention_weights, tf.cast(v, tf.float32))\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec90980-3270-4973-8d67-016806d6d1de",
   "metadata": {},
   "source": [
    "* Let's test this function and make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83b516df-f8b0-4c5c-a5a7-6bc0ee10e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 128]), TensorShape([4, 4]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = tf.random.uniform((4,128), dtype=tf.int64, minval=0, maxval=200)\n",
    "key = tf.random.uniform((4,128), dtype=tf.int64, minval=0, maxval=200)\n",
    "value = tf.random.uniform((4,128), dtype=tf.int64, minval=0, maxval=200)\n",
    "mask = tf.random.uniform((4, 4), dtype=tf.int64, minval=0, maxval=200)\n",
    "fn_out, fn_w = scaled_dot_product_attention(query, key, value, attention_mask=mask)\n",
    "fn_out.shape, fn_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab9611",
   "metadata": {},
   "source": [
    "* As you can see from above that the function is working as expected as each 4 positions atten to every four position in returned attention weights\n",
    "* Let's now write code for multi-head attention\n",
    "\n",
    "### Multi-head Attention\n",
    "\n",
    "* Multi-head attention multiple attensions in parallel each looking at different aspect of relation ship between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73b515f1-92d9-4471-8b81-6a93e7fd731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask through this layer\n",
    "        return mask\n",
    "    \n",
    "    def call(self, v, k, q, attention_mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, attention_mask=attention_mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f78df-c1ea-45e5-a90f-43664750b017",
   "metadata": {},
   "source": [
    "* Let's test this multi-head attention code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abd45877-4ca6-4251-8f77-f7ebe7923fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, attention_mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5f26f",
   "metadata": {},
   "source": [
    "* As you can see from above that we are getting correct output we are getting 8 heads where 60 positions is attending to every other 60 positions\n",
    "* Now let's write a function for creating mask for padding token so that model ignores them during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cee1fae2-09fa-482f-929f-f3a07745cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"Function to create mask for padding tokens\"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1579f10-5202-4ab3-92cf-f72e163c172d",
   "metadata": {},
   "source": [
    "* Let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c7a8dd2-7b15-4997-b0fc-61132e5c355b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef9bfa-3def-4b02-8a4a-4f44cdf20c01",
   "metadata": {},
   "source": [
    "* As seen from above that the create mask function is working as expected as padding(0) returns 1 and all other token returns 0\n",
    "* Now let's write a function for creating look ahead mask which will mask future words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "080e5390-1513-4ff0-baff-6c8c19410d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f51b7-97fd-4790-be91-1138bd6b1276",
   "metadata": {},
   "source": [
    "* Let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58994405-8327-471e-92eb-926f4975f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1.]\n",
      "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44224cbb-c069-4700-989c-b7b9a44d5221",
   "metadata": {},
   "source": [
    "* As you can see that function is working as expected masking future words\n",
    "* Now let's create a point wise feed forward network which will be applied independently to each word in sequence\n",
    "* This point wise feed forward network contains two dense layers\n",
    "* At start dimension is expanded to learn complex patterns and then compressed back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2c62fc5-12aa-43f1-8274-5e8e51f713b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d55a9e-0bc4-45a6-adcd-6e63fe0019f0",
   "metadata": {},
   "source": [
    "* Let's now get the model distribution strategy\n",
    "* As we are using single GPU only so here REPLICAS will be 1 which is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "027f5a7e-b2ff-47f1-8311-f01e7ea22f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d93bb",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "* Now let's write a function for performing positional encoding\n",
    "* This is required because model needs to know the positions of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ce8098f-0b12-4663-a6b2-81c7de0cc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1) \n",
    "    return tf.cast(pos_encoding[np.newaxis, :, :], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5f65-543d-48bc-b5a0-cb2f2ac2efce",
   "metadata": {},
   "source": [
    "* Let's now write a code for encoder layer\n",
    "\n",
    "### Encoder Layer\n",
    "\n",
    "* This layer processes input through self-attention and feed-forward network with residual (skip) connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3772e78-3786-4d85-9cd6-ca1c1567097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training=True, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, attention_mask=mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask through this layer\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a837eff-7e07-4ac5-8a59-08c12f7592ec",
   "metadata": {},
   "source": [
    "* Let's test the encoder layer and make sure that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa7add6c-2a91-4f62-b188-f52c315482ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), training=False, mask=None)\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547e2bd",
   "metadata": {},
   "source": [
    "* As you can see we are getting expected results\n",
    "* Now let's write code for encoder\n",
    "\n",
    "### Encoder\n",
    "\n",
    "* The Encoder consists of input Embedding, positional Encoding and number of encoder layers\n",
    "* The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca96cf86-f59e-4530-be15-09d6297f13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training=True, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask through this layer\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b42e8-7d7f-4163-836e-1b6f05691c12",
   "metadata": {},
   "source": [
    "* Let's test the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "255962e1-03e3-40fa-b054-f603f6624d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 20), dtype=tf.int64, minval=0, maxval=200)\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfbcd7",
   "metadata": {},
   "source": [
    "* As you can see we are getting correct output\n",
    "* Now let's write  code for decoder layer\n",
    "\n",
    "### Decoder Layer\n",
    "\n",
    "* This layer processes target sequence using self-attention (with look ahead mask), cross-attention to encoder output (with padding mask), and feed-forward network each with residual connections, layer normalization, and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2cedee80-3947-41bf-b88c-67ab47f7bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask through this layer\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4812489-3d98-43e7-85cb-85a6ebb550af",
   "metadata": {},
   "source": [
    "* Let's test the decoder layer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ee87e09-677c-4407-a144-3cf384c2b6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 20, 512)), sample_encoder_layer_output, \n",
    "    training=True, look_ahead_mask=None, padding_mask=None)\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bac76",
   "metadata": {},
   "source": [
    "* As you can see from above that we are getting expected results\n",
    "* Now let's write decoder code\n",
    "\n",
    "### Decoder \n",
    "\n",
    "* The Decoder converts target tokens into representations using self-attention, cross-attention to encoder, stacked across multiple decoder layers.\n",
    "* The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear (dense) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5ed4e1e-23c3-423c-baf3-fbba6ec8f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training,\n",
    "                                                 look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "        return x, attention_weights\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask through this layer\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ef69d-fabc-4dad-b999-5ddf6b9dae3b",
   "metadata": {},
   "source": [
    "* Let's test decoder code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8937d4c-dded-4fd2-9ce8-1af875044987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 20, 512]), TensorShape([64, 8, 20, 20]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 20), dtype=tf.int64, minval=0, maxval=200)\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0960c8d",
   "metadata": {},
   "source": [
    "* As you can see from above results that we are getting expected results\n",
    "* Now let's write code for transformer\n",
    "\n",
    "### Transformer\n",
    "\n",
    "* Transformer consists of the encoder, decoder and a final linear (dense) layer.\n",
    "* It combines encoder and decoder to transform input sequences to output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35039e41-7c10-4448-b4fd-6e8cc83cecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training=False, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
    "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Propagate the mask through this layer\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eae86-9504-4f8b-a8d1-cce79839c284",
   "metadata": {},
   "source": [
    "* Let's test the transformer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8b36609-8b8f-4444-905d-7dfd3a197329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 20, 8000])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "temp_input = tf.random.uniform((64, 20), dtype=tf.int32, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 20), dtype=tf.int32, minval=0, maxval=200)\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fc9af-1621-481e-b9e6-6273b6dfbed6",
   "metadata": {},
   "source": [
    "* As you can see that we are getting correct output from transformer\n",
    "* ow let's write a loss function to calculate loss ignoring padding positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4a07cdc-495c-4db9-967f-691c7d51260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a6512-bb14-4177-939a-ab40e09f8a27",
   "metadata": {},
   "source": [
    "* Let's now create custom learning rate scheduler with warmup and decay for transformer\n",
    "* In warmup learning rate increases linearly and in decay it decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8589c88d-4ac3-4b4a-a0f4-7de9b872d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScheduleFloat(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomScheduleFloat, self).__init__()\n",
    "        self.d_model = tf.constant(d_model,dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model,\"warmup_steps\":self.warmup_steps}\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.cast(tf.math.rsqrt(tf.cast(step, tf.float32)), tf.float32)\n",
    "        arg2 = tf.cast(tf.cast(step, tf.float32) * (tf.cast(self.warmup_steps, tf.float32)**-1.5), tf.float32)\n",
    "        size = tf.cast(tf.math.rsqrt(self.d_model), tf.float32)\n",
    "        return tf.math.multiply(size, tf.math.minimum(arg1, arg2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30193d6d-a05e-48c0-af74-11c51ab40477",
   "metadata": {},
   "source": [
    "* Let's visualize this learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "440de4a9-4e38-48d6-b6fe-2f368eb4f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_learning_rate = CustomScheduleFloat(d_model=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21a98b8f-a2ce-4411-8284-963f8c92abeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568cc95",
   "metadata": {},
   "source": [
    "* Now let's initialize and compile model with our predefined custom learning rate and Adam optimizer under the strategy scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3cf2e35-2cb7-4c6e-9cb2-2ca8358488f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomScheduleFloat(d_model=128)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98ffc9-cc42-44f5-afb5-8db12938f016",
   "metadata": {},
   "source": [
    "Now let's write custom accuracy function and clear any previous model and compile newly created model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c840442c-0518-4f8b-978d-e28212e7ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear backend\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# Creating model\n",
    "transformer = Transformer(num_layers=NUM_LAYERS, d_model=D_MODEL, num_heads=NUM_HEADS, dff=UNITS,\n",
    "                          input_vocab_size=VOCAB_SIZE, target_vocab_size=VOCAB_SIZE, \n",
    "                          pe_input=VOCAB_SIZE, \n",
    "                          pe_target=VOCAB_SIZE,\n",
    "                          rate=DROPOUT)\n",
    "\n",
    "# Compiling model\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c408252-2ddf-4e21-bd89-6df033e3b332",
   "metadata": {},
   "source": [
    "* Let's write a function to create all three masks required by transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ea1d1ac-6b5d-4154-a90d-c9f8d982b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tar)\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65acbd6-3ada-4fc8-8394-59fd05bc20bf",
   "metadata": {},
   "source": [
    "* Let's define training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2ed4fca-2da2-4a83-a443-c5c0882c478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fedb6-fc31-49d5-a4ea-bfa496f0ea16",
   "metadata": {},
   "source": [
    "* Let's now write training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7219d969-b88f-43da-91a4-6f427da680a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our corpus\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, 20), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, 20), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    # Creating masks\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    # Forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, training=True, \n",
    "                               enc_padding_mask=enc_padding_mask, \n",
    "                               look_ahead_mask=combined_mask,\n",
    "                               dec_padding_mask=dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    # Backward pass\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)   \n",
    "    # Updating weights\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    # Updating metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54564295-34e6-463c-b59b-b2fb2e743775",
   "metadata": {},
   "source": [
    "* Now let's write a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec6d10da-0440-48c6-b056-01e87bd13720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.1015 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.1029 Accuracy 0.0000\n",
      "Epoch 1 Batch 100 Loss 7.9937 Accuracy 0.0002\n",
      "Epoch 1 Batch 150 Loss 7.8260 Accuracy 0.0006\n",
      "Epoch 1 Batch 200 Loss 7.6610 Accuracy 0.0009\n",
      "Epoch 1 Batch 250 Loss 7.4940 Accuracy 0.0012\n",
      "Epoch 1 Batch 300 Loss 7.3438 Accuracy 0.0018\n",
      "Epoch 1 Batch 350 Loss 7.2087 Accuracy 0.0027\n",
      "Epoch 1 Batch 400 Loss 7.0823 Accuracy 0.0043\n",
      "Epoch 1 Batch 450 Loss 6.9572 Accuracy 0.0062\n",
      "Epoch 1 Batch 500 Loss 6.8287 Accuracy 0.0085\n",
      "Epoch 1 Batch 550 Loss 6.7010 Accuracy 0.0114\n",
      "Epoch 1 Batch 600 Loss 6.5663 Accuracy 0.0150\n",
      "Epoch 1 Batch 650 Loss 6.4186 Accuracy 0.0199\n",
      "Epoch 1 Batch 700 Loss 6.2636 Accuracy 0.0257\n",
      "Epoch 1 Batch 750 Loss 6.1096 Accuracy 0.0323\n",
      "Epoch 1 Batch 800 Loss 5.9560 Accuracy 0.0404\n",
      "Epoch 1 Batch 850 Loss 5.7927 Accuracy 0.0500\n",
      "Epoch 1 Batch 900 Loss 5.6213 Accuracy 0.0602\n",
      "Epoch 1 Batch 950 Loss 5.4466 Accuracy 0.0705\n",
      "Epoch 1 Batch 1000 Loss 5.2717 Accuracy 0.0811\n",
      "Epoch 1 Batch 1050 Loss 5.1007 Accuracy 0.0916\n",
      "Epoch 1 Batch 1100 Loss 4.9346 Accuracy 0.1019\n",
      "Epoch 1 Loss 4.9150 Accuracy 0.1029\n",
      "\n",
      "Epoch 2 Batch 0 Loss 5.2983 Accuracy 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:51:47.528025: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 50 Loss 5.5194 Accuracy 0.0031\n",
      "Epoch 2 Batch 100 Loss 5.1337 Accuracy 0.0059\n",
      "Epoch 2 Batch 150 Loss 4.8417 Accuracy 0.0089\n",
      "Epoch 2 Batch 200 Loss 4.5814 Accuracy 0.0120\n",
      "Epoch 2 Batch 250 Loss 4.3220 Accuracy 0.0163\n",
      "Epoch 2 Batch 300 Loss 4.0907 Accuracy 0.0207\n",
      "Epoch 2 Batch 350 Loss 3.8735 Accuracy 0.0261\n",
      "Epoch 2 Batch 400 Loss 3.6720 Accuracy 0.0321\n",
      "Epoch 2 Batch 450 Loss 3.4728 Accuracy 0.0391\n",
      "Epoch 2 Batch 500 Loss 3.2799 Accuracy 0.0474\n",
      "Epoch 2 Batch 550 Loss 3.0880 Accuracy 0.0570\n",
      "Epoch 2 Batch 600 Loss 2.9071 Accuracy 0.0683\n",
      "Epoch 2 Batch 650 Loss 2.7342 Accuracy 0.0816\n",
      "Epoch 2 Batch 700 Loss 2.5802 Accuracy 0.0955\n",
      "Epoch 2 Batch 750 Loss 2.4523 Accuracy 0.1079\n",
      "Epoch 2 Batch 800 Loss 2.3399 Accuracy 0.1221\n",
      "Epoch 2 Batch 850 Loss 2.2358 Accuracy 0.1365\n",
      "Epoch 2 Batch 900 Loss 2.1373 Accuracy 0.1495\n",
      "Epoch 2 Batch 950 Loss 2.0450 Accuracy 0.1613\n",
      "Epoch 2 Batch 1000 Loss 1.9594 Accuracy 0.1722\n",
      "Epoch 2 Batch 1050 Loss 1.8810 Accuracy 0.1821\n",
      "Epoch 2 Batch 1100 Loss 1.8081 Accuracy 0.1912\n",
      "Epoch 2 Loss 1.7998 Accuracy 0.1920\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.8204 Accuracy 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:52:03.977640: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 50 Loss 4.5567 Accuracy 0.0053\n",
      "Epoch 3 Batch 100 Loss 4.2338 Accuracy 0.0087\n",
      "Epoch 3 Batch 150 Loss 3.8958 Accuracy 0.0123\n",
      "Epoch 3 Batch 200 Loss 3.5895 Accuracy 0.0166\n",
      "Epoch 3 Batch 250 Loss 3.3177 Accuracy 0.0215\n",
      "Epoch 3 Batch 300 Loss 3.0651 Accuracy 0.0275\n",
      "Epoch 3 Batch 350 Loss 2.8381 Accuracy 0.0339\n",
      "Epoch 3 Batch 400 Loss 2.6345 Accuracy 0.0409\n",
      "Epoch 3 Batch 450 Loss 2.4475 Accuracy 0.0489\n",
      "Epoch 3 Batch 500 Loss 2.2757 Accuracy 0.0578\n",
      "Epoch 3 Batch 550 Loss 2.1212 Accuracy 0.0676\n",
      "Epoch 3 Batch 600 Loss 1.9764 Accuracy 0.0796\n",
      "Epoch 3 Batch 650 Loss 1.8455 Accuracy 0.0935\n",
      "Epoch 3 Batch 700 Loss 1.7345 Accuracy 0.1069\n",
      "Epoch 3 Batch 750 Loss 1.6418 Accuracy 0.1196\n",
      "Epoch 3 Batch 800 Loss 1.5640 Accuracy 0.1337\n",
      "Epoch 3 Batch 850 Loss 1.4904 Accuracy 0.1486\n",
      "Epoch 3 Batch 900 Loss 1.4219 Accuracy 0.1614\n",
      "Epoch 3 Batch 950 Loss 1.3595 Accuracy 0.1728\n",
      "Epoch 3 Batch 1000 Loss 1.3023 Accuracy 0.1834\n",
      "Epoch 3 Batch 1050 Loss 1.2500 Accuracy 0.1930\n",
      "Epoch 3 Batch 1100 Loss 1.2022 Accuracy 0.2018\n",
      "Epoch 3 Loss 1.1967 Accuracy 0.2026\n",
      "\n",
      "Epoch 4 Batch 0 Loss 5.2288 Accuracy 0.0033\n",
      "Epoch 4 Batch 50 Loss 4.7139 Accuracy 0.0056\n",
      "Epoch 4 Batch 100 Loss 4.2405 Accuracy 0.0090\n",
      "Epoch 4 Batch 150 Loss 3.8258 Accuracy 0.0132\n",
      "Epoch 4 Batch 200 Loss 3.5049 Accuracy 0.0175\n",
      "Epoch 4 Batch 250 Loss 3.2160 Accuracy 0.0228\n",
      "Epoch 4 Batch 300 Loss 2.9611 Accuracy 0.0282\n",
      "Epoch 4 Batch 350 Loss 2.7229 Accuracy 0.0351\n",
      "Epoch 4 Batch 400 Loss 2.5146 Accuracy 0.0422\n",
      "Epoch 4 Batch 450 Loss 2.3278 Accuracy 0.0503\n",
      "Epoch 4 Batch 500 Loss 2.1551 Accuracy 0.0596\n",
      "Epoch 4 Batch 550 Loss 2.0030 Accuracy 0.0697\n",
      "Epoch 4 Batch 600 Loss 1.8663 Accuracy 0.0811\n",
      "Epoch 4 Batch 650 Loss 1.7435 Accuracy 0.0943\n",
      "Epoch 4 Batch 700 Loss 1.6385 Accuracy 0.1081\n",
      "Epoch 4 Batch 750 Loss 1.5517 Accuracy 0.1213\n",
      "Epoch 4 Batch 800 Loss 1.4793 Accuracy 0.1353\n",
      "Epoch 4 Batch 850 Loss 1.4109 Accuracy 0.1496\n",
      "Epoch 4 Batch 900 Loss 1.3468 Accuracy 0.1626\n",
      "Epoch 4 Batch 950 Loss 1.2874 Accuracy 0.1743\n",
      "Epoch 4 Batch 1000 Loss 1.2332 Accuracy 0.1843\n",
      "Epoch 4 Batch 1050 Loss 1.1844 Accuracy 0.1935\n",
      "Epoch 4 Batch 1100 Loss 1.1393 Accuracy 0.2022\n",
      "Epoch 4 Loss 1.1344 Accuracy 0.2031\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.2930 Accuracy 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:52:36.885129: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 50 Loss 4.4736 Accuracy 0.0069\n",
      "Epoch 5 Batch 100 Loss 4.1485 Accuracy 0.0093\n",
      "Epoch 5 Batch 150 Loss 3.7572 Accuracy 0.0133\n",
      "Epoch 5 Batch 200 Loss 3.4220 Accuracy 0.0178\n",
      "Epoch 5 Batch 250 Loss 3.1217 Accuracy 0.0231\n",
      "Epoch 5 Batch 300 Loss 2.8487 Accuracy 0.0291\n",
      "Epoch 5 Batch 350 Loss 2.6106 Accuracy 0.0361\n",
      "Epoch 5 Batch 400 Loss 2.3915 Accuracy 0.0439\n",
      "Epoch 5 Batch 450 Loss 2.1958 Accuracy 0.0525\n",
      "Epoch 5 Batch 500 Loss 2.0269 Accuracy 0.0616\n",
      "Epoch 5 Batch 550 Loss 1.8761 Accuracy 0.0722\n",
      "Epoch 5 Batch 600 Loss 1.7424 Accuracy 0.0840\n",
      "Epoch 5 Batch 650 Loss 1.6228 Accuracy 0.0975\n",
      "Epoch 5 Batch 700 Loss 1.5219 Accuracy 0.1111\n",
      "Epoch 5 Batch 750 Loss 1.4413 Accuracy 0.1232\n",
      "Epoch 5 Batch 800 Loss 1.3721 Accuracy 0.1373\n",
      "Epoch 5 Batch 850 Loss 1.3065 Accuracy 0.1516\n",
      "Epoch 5 Batch 900 Loss 1.2449 Accuracy 0.1644\n",
      "Epoch 5 Batch 950 Loss 1.1878 Accuracy 0.1763\n",
      "Epoch 5 Batch 1000 Loss 1.1359 Accuracy 0.1866\n",
      "Epoch 5 Batch 1050 Loss 1.0889 Accuracy 0.1963\n",
      "Epoch 5 Batch 1100 Loss 1.0461 Accuracy 0.2055\n",
      "Epoch 5 Loss 1.0412 Accuracy 0.2064\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.8502 Accuracy 0.0099\n",
      "Epoch 6 Batch 50 Loss 3.9034 Accuracy 0.0087\n",
      "Epoch 6 Batch 100 Loss 3.5975 Accuracy 0.0118\n",
      "Epoch 6 Batch 150 Loss 3.2387 Accuracy 0.0161\n",
      "Epoch 6 Batch 200 Loss 2.9441 Accuracy 0.0207\n",
      "Epoch 6 Batch 250 Loss 2.6821 Accuracy 0.0264\n",
      "Epoch 6 Batch 300 Loss 2.4385 Accuracy 0.0325\n",
      "Epoch 6 Batch 350 Loss 2.2216 Accuracy 0.0396\n",
      "Epoch 6 Batch 400 Loss 2.0317 Accuracy 0.0470\n",
      "Epoch 6 Batch 450 Loss 1.8637 Accuracy 0.0555\n",
      "Epoch 6 Batch 500 Loss 1.7152 Accuracy 0.0649\n",
      "Epoch 6 Batch 550 Loss 1.5872 Accuracy 0.0751\n",
      "Epoch 6 Batch 600 Loss 1.4724 Accuracy 0.0868\n",
      "Epoch 6 Batch 650 Loss 1.3704 Accuracy 0.1004\n",
      "Epoch 6 Batch 700 Loss 1.2839 Accuracy 0.1140\n",
      "Epoch 6 Batch 750 Loss 1.2138 Accuracy 0.1266\n",
      "Epoch 6 Batch 800 Loss 1.1550 Accuracy 0.1408\n",
      "Epoch 6 Batch 850 Loss 1.0998 Accuracy 0.1551\n",
      "Epoch 6 Batch 900 Loss 1.0471 Accuracy 0.1683\n",
      "Epoch 6 Batch 950 Loss 0.9999 Accuracy 0.1799\n",
      "Epoch 6 Batch 1000 Loss 0.9561 Accuracy 0.1904\n",
      "Epoch 6 Batch 1050 Loss 0.9163 Accuracy 0.2002\n",
      "Epoch 6 Batch 1100 Loss 0.8796 Accuracy 0.2088\n",
      "Epoch 6 Loss 0.8753 Accuracy 0.2096\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.1369 Accuracy 0.0082\n",
      "Epoch 7 Batch 50 Loss 3.4702 Accuracy 0.0095\n",
      "Epoch 7 Batch 100 Loss 3.2516 Accuracy 0.0127\n",
      "Epoch 7 Batch 150 Loss 2.9040 Accuracy 0.0174\n",
      "Epoch 7 Batch 200 Loss 2.6500 Accuracy 0.0218\n",
      "Epoch 7 Batch 250 Loss 2.4045 Accuracy 0.0276\n",
      "Epoch 7 Batch 300 Loss 2.1813 Accuracy 0.0343\n",
      "Epoch 7 Batch 350 Loss 1.9834 Accuracy 0.0417\n",
      "Epoch 7 Batch 400 Loss 1.8085 Accuracy 0.0495\n",
      "Epoch 7 Batch 450 Loss 1.6573 Accuracy 0.0580\n",
      "Epoch 7 Batch 500 Loss 1.5241 Accuracy 0.0673\n",
      "Epoch 7 Batch 550 Loss 1.4072 Accuracy 0.0777\n",
      "Epoch 7 Batch 600 Loss 1.3039 Accuracy 0.0895\n",
      "Epoch 7 Batch 650 Loss 1.2133 Accuracy 0.1026\n",
      "Epoch 7 Batch 700 Loss 1.1364 Accuracy 0.1165\n",
      "Epoch 7 Batch 750 Loss 1.0731 Accuracy 0.1293\n",
      "Epoch 7 Batch 800 Loss 1.0196 Accuracy 0.1434\n",
      "Epoch 7 Batch 850 Loss 0.9700 Accuracy 0.1575\n",
      "Epoch 7 Batch 900 Loss 0.9237 Accuracy 0.1705\n",
      "Epoch 7 Batch 950 Loss 0.8814 Accuracy 0.1821\n",
      "Epoch 7 Batch 1000 Loss 0.8424 Accuracy 0.1924\n",
      "Epoch 7 Batch 1050 Loss 0.8074 Accuracy 0.2021\n",
      "Epoch 7 Batch 1100 Loss 0.7755 Accuracy 0.2109\n",
      "Epoch 7 Loss 0.7718 Accuracy 0.2118\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.4515 Accuracy 0.0082\n",
      "Epoch 8 Batch 50 Loss 3.1679 Accuracy 0.0109\n",
      "Epoch 8 Batch 100 Loss 2.8984 Accuracy 0.0148\n",
      "Epoch 8 Batch 150 Loss 2.6197 Accuracy 0.0193\n",
      "Epoch 8 Batch 200 Loss 2.3623 Accuracy 0.0240\n",
      "Epoch 8 Batch 250 Loss 2.1403 Accuracy 0.0297\n",
      "Epoch 8 Batch 300 Loss 1.9382 Accuracy 0.0361\n",
      "Epoch 8 Batch 350 Loss 1.7589 Accuracy 0.0433\n",
      "Epoch 8 Batch 400 Loss 1.5994 Accuracy 0.0512\n",
      "Epoch 8 Batch 450 Loss 1.4645 Accuracy 0.0597\n",
      "Epoch 8 Batch 500 Loss 1.3462 Accuracy 0.0688\n",
      "Epoch 8 Batch 550 Loss 1.2430 Accuracy 0.0791\n",
      "Epoch 8 Batch 600 Loss 1.1519 Accuracy 0.0907\n",
      "Epoch 8 Batch 650 Loss 1.0712 Accuracy 0.1040\n",
      "Epoch 8 Batch 700 Loss 1.0039 Accuracy 0.1173\n",
      "Epoch 8 Batch 750 Loss 0.9466 Accuracy 0.1302\n",
      "Epoch 8 Batch 800 Loss 0.8987 Accuracy 0.1445\n",
      "Epoch 8 Batch 850 Loss 0.8552 Accuracy 0.1592\n",
      "Epoch 8 Batch 900 Loss 0.8141 Accuracy 0.1718\n",
      "Epoch 8 Batch 950 Loss 0.7769 Accuracy 0.1833\n",
      "Epoch 8 Batch 1000 Loss 0.7430 Accuracy 0.1937\n",
      "Epoch 8 Batch 1050 Loss 0.7118 Accuracy 0.2037\n",
      "Epoch 8 Batch 1100 Loss 0.6832 Accuracy 0.2124\n",
      "Epoch 8 Loss 0.6799 Accuracy 0.2134\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.9653 Accuracy 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:53:42.686766: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 50 Loss 2.8861 Accuracy 0.0111\n",
      "Epoch 9 Batch 100 Loss 2.6570 Accuracy 0.0154\n",
      "Epoch 9 Batch 150 Loss 2.4245 Accuracy 0.0195\n",
      "Epoch 9 Batch 200 Loss 2.1892 Accuracy 0.0245\n",
      "Epoch 9 Batch 250 Loss 1.9737 Accuracy 0.0306\n",
      "Epoch 9 Batch 300 Loss 1.7848 Accuracy 0.0371\n",
      "Epoch 9 Batch 350 Loss 1.6166 Accuracy 0.0446\n",
      "Epoch 9 Batch 400 Loss 1.4745 Accuracy 0.0523\n",
      "Epoch 9 Batch 450 Loss 1.3482 Accuracy 0.0609\n",
      "Epoch 9 Batch 500 Loss 1.2392 Accuracy 0.0701\n",
      "Epoch 9 Batch 550 Loss 1.1434 Accuracy 0.0802\n",
      "Epoch 9 Batch 600 Loss 1.0587 Accuracy 0.0920\n",
      "Epoch 9 Batch 650 Loss 0.9842 Accuracy 0.1056\n",
      "Epoch 9 Batch 700 Loss 0.9216 Accuracy 0.1192\n",
      "Epoch 9 Batch 750 Loss 0.8693 Accuracy 0.1319\n",
      "Epoch 9 Batch 800 Loss 0.8246 Accuracy 0.1462\n",
      "Epoch 9 Batch 850 Loss 0.7845 Accuracy 0.1606\n",
      "Epoch 9 Batch 900 Loss 0.7468 Accuracy 0.1734\n",
      "Epoch 9 Batch 950 Loss 0.7126 Accuracy 0.1849\n",
      "Epoch 9 Batch 1000 Loss 0.6816 Accuracy 0.1955\n",
      "Epoch 9 Batch 1050 Loss 0.6532 Accuracy 0.2051\n",
      "Epoch 9 Batch 1100 Loss 0.6270 Accuracy 0.2136\n",
      "Epoch 9 Loss 0.6240 Accuracy 0.2145\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.7431 Accuracy 0.0173\n",
      "Epoch 10 Batch 50 Loss 2.6751 Accuracy 0.0127\n",
      "Epoch 10 Batch 100 Loss 2.4792 Accuracy 0.0162\n",
      "Epoch 10 Batch 150 Loss 2.2256 Accuracy 0.0211\n",
      "Epoch 10 Batch 200 Loss 2.0153 Accuracy 0.0260\n",
      "Epoch 10 Batch 250 Loss 1.8148 Accuracy 0.0320\n",
      "Epoch 10 Batch 300 Loss 1.6426 Accuracy 0.0381\n",
      "Epoch 10 Batch 350 Loss 1.4920 Accuracy 0.0455\n",
      "Epoch 10 Batch 400 Loss 1.3583 Accuracy 0.0531\n",
      "Epoch 10 Batch 450 Loss 1.2410 Accuracy 0.0617\n",
      "Epoch 10 Batch 500 Loss 1.1385 Accuracy 0.0710\n",
      "Epoch 10 Batch 550 Loss 1.0495 Accuracy 0.0814\n",
      "Epoch 10 Batch 600 Loss 0.9724 Accuracy 0.0931\n",
      "Epoch 10 Batch 650 Loss 0.9038 Accuracy 0.1066\n",
      "Epoch 10 Batch 700 Loss 0.8460 Accuracy 0.1201\n",
      "Epoch 10 Batch 750 Loss 0.7972 Accuracy 0.1325\n",
      "Epoch 10 Batch 800 Loss 0.7570 Accuracy 0.1468\n",
      "Epoch 10 Batch 850 Loss 0.7198 Accuracy 0.1618\n",
      "Epoch 10 Batch 900 Loss 0.6848 Accuracy 0.1747\n",
      "Epoch 10 Batch 950 Loss 0.6529 Accuracy 0.1861\n",
      "Epoch 10 Batch 1000 Loss 0.6239 Accuracy 0.1965\n",
      "Epoch 10 Batch 1050 Loss 0.5976 Accuracy 0.2061\n",
      "Epoch 10 Batch 1100 Loss 0.5734 Accuracy 0.2145\n",
      "Epoch 10 Loss 0.5705 Accuracy 0.2155\n",
      "\n",
      "Epoch 11 Batch 0 Loss 3.4190 Accuracy 0.0082\n",
      "Epoch 11 Batch 50 Loss 2.5329 Accuracy 0.0130\n",
      "Epoch 11 Batch 100 Loss 2.2844 Accuracy 0.0177\n",
      "Epoch 11 Batch 150 Loss 2.0693 Accuracy 0.0221\n",
      "Epoch 11 Batch 200 Loss 1.8635 Accuracy 0.0272\n",
      "Epoch 11 Batch 250 Loss 1.6863 Accuracy 0.0328\n",
      "Epoch 11 Batch 300 Loss 1.5275 Accuracy 0.0394\n",
      "Epoch 11 Batch 350 Loss 1.3859 Accuracy 0.0466\n",
      "Epoch 11 Batch 400 Loss 1.2569 Accuracy 0.0547\n",
      "Epoch 11 Batch 450 Loss 1.1485 Accuracy 0.0633\n",
      "Epoch 11 Batch 500 Loss 1.0527 Accuracy 0.0725\n",
      "Epoch 11 Batch 550 Loss 0.9702 Accuracy 0.0825\n",
      "Epoch 11 Batch 600 Loss 0.8982 Accuracy 0.0941\n",
      "Epoch 11 Batch 650 Loss 0.8351 Accuracy 0.1075\n",
      "Epoch 11 Batch 700 Loss 0.7818 Accuracy 0.1208\n",
      "Epoch 11 Batch 750 Loss 0.7373 Accuracy 0.1333\n",
      "Epoch 11 Batch 800 Loss 0.6988 Accuracy 0.1476\n",
      "Epoch 11 Batch 850 Loss 0.6640 Accuracy 0.1625\n",
      "Epoch 11 Batch 900 Loss 0.6320 Accuracy 0.1755\n",
      "Epoch 11 Batch 950 Loss 0.6026 Accuracy 0.1874\n",
      "Epoch 11 Batch 1000 Loss 0.5762 Accuracy 0.1980\n",
      "Epoch 11 Batch 1050 Loss 0.5520 Accuracy 0.2074\n",
      "Epoch 11 Batch 1100 Loss 0.5296 Accuracy 0.2154\n",
      "Epoch 11 Loss 0.5271 Accuracy 0.2162\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.3738 Accuracy 0.0181\n",
      "Epoch 12 Batch 50 Loss 2.4405 Accuracy 0.0135\n",
      "Epoch 12 Batch 100 Loss 2.1740 Accuracy 0.0178\n",
      "Epoch 12 Batch 150 Loss 1.9354 Accuracy 0.0228\n",
      "Epoch 12 Batch 200 Loss 1.7517 Accuracy 0.0279\n",
      "Epoch 12 Batch 250 Loss 1.5769 Accuracy 0.0341\n",
      "Epoch 12 Batch 300 Loss 1.4255 Accuracy 0.0407\n",
      "Epoch 12 Batch 350 Loss 1.2897 Accuracy 0.0479\n",
      "Epoch 12 Batch 400 Loss 1.1718 Accuracy 0.0557\n",
      "Epoch 12 Batch 450 Loss 1.0702 Accuracy 0.0641\n",
      "Epoch 12 Batch 500 Loss 0.9819 Accuracy 0.0732\n",
      "Epoch 12 Batch 550 Loss 0.9038 Accuracy 0.0833\n",
      "Epoch 12 Batch 600 Loss 0.8373 Accuracy 0.0946\n",
      "Epoch 12 Batch 650 Loss 0.7788 Accuracy 0.1077\n",
      "Epoch 12 Batch 700 Loss 0.7294 Accuracy 0.1208\n",
      "Epoch 12 Batch 750 Loss 0.6874 Accuracy 0.1332\n",
      "Epoch 12 Batch 800 Loss 0.6516 Accuracy 0.1482\n",
      "Epoch 12 Batch 850 Loss 0.6194 Accuracy 0.1633\n",
      "Epoch 12 Batch 900 Loss 0.5892 Accuracy 0.1762\n",
      "Epoch 12 Batch 950 Loss 0.5617 Accuracy 0.1876\n",
      "Epoch 12 Batch 1000 Loss 0.5368 Accuracy 0.1978\n",
      "Epoch 12 Batch 1050 Loss 0.5141 Accuracy 0.2075\n",
      "Epoch 12 Batch 1100 Loss 0.4936 Accuracy 0.2161\n",
      "Epoch 12 Loss 0.4913 Accuracy 0.2169\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.7882 Accuracy 0.0123\n",
      "Epoch 13 Batch 50 Loss 2.1570 Accuracy 0.0149\n",
      "Epoch 13 Batch 100 Loss 1.9895 Accuracy 0.0190\n",
      "Epoch 13 Batch 150 Loss 1.7759 Accuracy 0.0240\n",
      "Epoch 13 Batch 200 Loss 1.6284 Accuracy 0.0287\n",
      "Epoch 13 Batch 250 Loss 1.4727 Accuracy 0.0348\n",
      "Epoch 13 Batch 300 Loss 1.3320 Accuracy 0.0414\n",
      "Epoch 13 Batch 350 Loss 1.2087 Accuracy 0.0484\n",
      "Epoch 13 Batch 400 Loss 1.0982 Accuracy 0.0564\n",
      "Epoch 13 Batch 450 Loss 1.0030 Accuracy 0.0646\n",
      "Epoch 13 Batch 500 Loss 0.9184 Accuracy 0.0738\n",
      "Epoch 13 Batch 550 Loss 0.8471 Accuracy 0.0838\n",
      "Epoch 13 Batch 600 Loss 0.7840 Accuracy 0.0954\n",
      "Epoch 13 Batch 650 Loss 0.7294 Accuracy 0.1084\n",
      "Epoch 13 Batch 700 Loss 0.6825 Accuracy 0.1227\n",
      "Epoch 13 Batch 750 Loss 0.6442 Accuracy 0.1348\n",
      "Epoch 13 Batch 800 Loss 0.6113 Accuracy 0.1494\n",
      "Epoch 13 Batch 850 Loss 0.5806 Accuracy 0.1641\n",
      "Epoch 13 Batch 900 Loss 0.5520 Accuracy 0.1772\n",
      "Epoch 13 Batch 950 Loss 0.5267 Accuracy 0.1885\n",
      "Epoch 13 Batch 1000 Loss 0.5034 Accuracy 0.1988\n",
      "Epoch 13 Batch 1050 Loss 0.4820 Accuracy 0.2085\n",
      "Epoch 13 Batch 1100 Loss 0.4627 Accuracy 0.2166\n",
      "Epoch 13 Loss 0.4604 Accuracy 0.2175\n",
      "\n",
      "Epoch 14 Batch 0 Loss 2.1135 Accuracy 0.0123\n",
      "Epoch 14 Batch 50 Loss 1.9709 Accuracy 0.0154\n",
      "Epoch 14 Batch 100 Loss 1.8794 Accuracy 0.0197\n",
      "Epoch 14 Batch 150 Loss 1.6990 Accuracy 0.0245\n",
      "Epoch 14 Batch 200 Loss 1.5341 Accuracy 0.0296\n",
      "Epoch 14 Batch 250 Loss 1.3850 Accuracy 0.0357\n",
      "Epoch 14 Batch 300 Loss 1.2560 Accuracy 0.0421\n",
      "Epoch 14 Batch 350 Loss 1.1373 Accuracy 0.0491\n",
      "Epoch 14 Batch 400 Loss 1.0322 Accuracy 0.0567\n",
      "Epoch 14 Batch 450 Loss 0.9413 Accuracy 0.0653\n",
      "Epoch 14 Batch 500 Loss 0.8629 Accuracy 0.0746\n",
      "Epoch 14 Batch 550 Loss 0.7951 Accuracy 0.0847\n",
      "Epoch 14 Batch 600 Loss 0.7362 Accuracy 0.0960\n",
      "Epoch 14 Batch 650 Loss 0.6843 Accuracy 0.1091\n",
      "Epoch 14 Batch 700 Loss 0.6404 Accuracy 0.1227\n",
      "Epoch 14 Batch 750 Loss 0.6034 Accuracy 0.1352\n",
      "Epoch 14 Batch 800 Loss 0.5726 Accuracy 0.1497\n",
      "Epoch 14 Batch 850 Loss 0.5446 Accuracy 0.1642\n",
      "Epoch 14 Batch 900 Loss 0.5180 Accuracy 0.1767\n",
      "Epoch 14 Batch 950 Loss 0.4940 Accuracy 0.1884\n",
      "Epoch 14 Batch 1000 Loss 0.4723 Accuracy 0.1991\n",
      "Epoch 14 Batch 1050 Loss 0.4525 Accuracy 0.2085\n",
      "Epoch 14 Batch 1100 Loss 0.4342 Accuracy 0.2170\n",
      "Epoch 14 Loss 0.4321 Accuracy 0.2179\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.9296 Accuracy 0.0115\n",
      "Epoch 15 Batch 50 Loss 1.9166 Accuracy 0.0165\n",
      "Epoch 15 Batch 100 Loss 1.7551 Accuracy 0.0208\n",
      "Epoch 15 Batch 150 Loss 1.6084 Accuracy 0.0253\n",
      "Epoch 15 Batch 200 Loss 1.4625 Accuracy 0.0306\n",
      "Epoch 15 Batch 250 Loss 1.3331 Accuracy 0.0362\n",
      "Epoch 15 Batch 300 Loss 1.2068 Accuracy 0.0424\n",
      "Epoch 15 Batch 350 Loss 1.0916 Accuracy 0.0497\n",
      "Epoch 15 Batch 400 Loss 0.9895 Accuracy 0.0576\n",
      "Epoch 15 Batch 450 Loss 0.9019 Accuracy 0.0661\n",
      "Epoch 15 Batch 500 Loss 0.8264 Accuracy 0.0754\n",
      "Epoch 15 Batch 550 Loss 0.7617 Accuracy 0.0849\n",
      "Epoch 15 Batch 600 Loss 0.7055 Accuracy 0.0965\n",
      "Epoch 15 Batch 650 Loss 0.6553 Accuracy 0.1099\n",
      "Epoch 15 Batch 700 Loss 0.6130 Accuracy 0.1236\n",
      "Epoch 15 Batch 750 Loss 0.5778 Accuracy 0.1360\n",
      "Epoch 15 Batch 800 Loss 0.5476 Accuracy 0.1502\n",
      "Epoch 15 Batch 850 Loss 0.5201 Accuracy 0.1647\n",
      "Epoch 15 Batch 900 Loss 0.4948 Accuracy 0.1775\n",
      "Epoch 15 Batch 950 Loss 0.4717 Accuracy 0.1892\n",
      "Epoch 15 Batch 1000 Loss 0.4507 Accuracy 0.1994\n",
      "Epoch 15 Batch 1050 Loss 0.4314 Accuracy 0.2088\n",
      "Epoch 15 Batch 1100 Loss 0.4140 Accuracy 0.2175\n",
      "Epoch 15 Loss 0.4122 Accuracy 0.2184\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.3009 Accuracy 0.0189\n",
      "Epoch 16 Batch 50 Loss 1.7565 Accuracy 0.0162\n",
      "Epoch 16 Batch 100 Loss 1.6841 Accuracy 0.0205\n",
      "Epoch 16 Batch 150 Loss 1.5254 Accuracy 0.0256\n",
      "Epoch 16 Batch 200 Loss 1.3872 Accuracy 0.0306\n",
      "Epoch 16 Batch 250 Loss 1.2559 Accuracy 0.0366\n",
      "Epoch 16 Batch 300 Loss 1.1427 Accuracy 0.0428\n",
      "Epoch 16 Batch 350 Loss 1.0360 Accuracy 0.0500\n",
      "Epoch 16 Batch 400 Loss 0.9427 Accuracy 0.0576\n",
      "Epoch 16 Batch 450 Loss 0.8619 Accuracy 0.0660\n",
      "Epoch 16 Batch 500 Loss 0.7919 Accuracy 0.0750\n",
      "Epoch 16 Batch 550 Loss 0.7287 Accuracy 0.0852\n",
      "Epoch 16 Batch 600 Loss 0.6745 Accuracy 0.0966\n",
      "Epoch 16 Batch 650 Loss 0.6267 Accuracy 0.1101\n",
      "Epoch 16 Batch 700 Loss 0.5862 Accuracy 0.1240\n",
      "Epoch 16 Batch 750 Loss 0.5521 Accuracy 0.1364\n",
      "Epoch 16 Batch 800 Loss 0.5234 Accuracy 0.1508\n",
      "Epoch 16 Batch 850 Loss 0.4972 Accuracy 0.1648\n",
      "Epoch 16 Batch 900 Loss 0.4729 Accuracy 0.1777\n",
      "Epoch 16 Batch 950 Loss 0.4506 Accuracy 0.1888\n",
      "Epoch 16 Batch 1000 Loss 0.4304 Accuracy 0.1994\n",
      "Epoch 16 Batch 1050 Loss 0.4121 Accuracy 0.2091\n",
      "Epoch 16 Batch 1100 Loss 0.3955 Accuracy 0.2178\n",
      "Epoch 16 Loss 0.3936 Accuracy 0.2187\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.5061 Accuracy 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 04:55:54.313234: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 50 Loss 1.7856 Accuracy 0.0170\n",
      "Epoch 17 Batch 100 Loss 1.6139 Accuracy 0.0215\n",
      "Epoch 17 Batch 150 Loss 1.4792 Accuracy 0.0260\n",
      "Epoch 17 Batch 200 Loss 1.3464 Accuracy 0.0309\n",
      "Epoch 17 Batch 250 Loss 1.2211 Accuracy 0.0368\n",
      "Epoch 17 Batch 300 Loss 1.1050 Accuracy 0.0432\n",
      "Epoch 17 Batch 350 Loss 1.0036 Accuracy 0.0504\n",
      "Epoch 17 Batch 400 Loss 0.9105 Accuracy 0.0581\n",
      "Epoch 17 Batch 450 Loss 0.8298 Accuracy 0.0664\n",
      "Epoch 17 Batch 500 Loss 0.7604 Accuracy 0.0757\n",
      "Epoch 17 Batch 550 Loss 0.7018 Accuracy 0.0855\n",
      "Epoch 17 Batch 600 Loss 0.6492 Accuracy 0.0971\n",
      "Epoch 17 Batch 650 Loss 0.6033 Accuracy 0.1103\n",
      "Epoch 17 Batch 700 Loss 0.5646 Accuracy 0.1236\n",
      "Epoch 17 Batch 750 Loss 0.5320 Accuracy 0.1364\n",
      "Epoch 17 Batch 800 Loss 0.5048 Accuracy 0.1506\n",
      "Epoch 17 Batch 850 Loss 0.4791 Accuracy 0.1652\n",
      "Epoch 17 Batch 900 Loss 0.4557 Accuracy 0.1782\n",
      "Epoch 17 Batch 950 Loss 0.4344 Accuracy 0.1895\n",
      "Epoch 17 Batch 1000 Loss 0.4148 Accuracy 0.2002\n",
      "Epoch 17 Batch 1050 Loss 0.3970 Accuracy 0.2097\n",
      "Epoch 17 Batch 1100 Loss 0.3808 Accuracy 0.2181\n",
      "Epoch 17 Loss 0.3789 Accuracy 0.2190\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.7024 Accuracy 0.0123\n",
      "Epoch 18 Batch 50 Loss 1.7456 Accuracy 0.0168\n",
      "Epoch 18 Batch 100 Loss 1.5609 Accuracy 0.0217\n",
      "Epoch 18 Batch 150 Loss 1.4311 Accuracy 0.0263\n",
      "Epoch 18 Batch 200 Loss 1.2985 Accuracy 0.0313\n",
      "Epoch 18 Batch 250 Loss 1.1784 Accuracy 0.0372\n",
      "Epoch 18 Batch 300 Loss 1.0690 Accuracy 0.0435\n",
      "Epoch 18 Batch 350 Loss 0.9686 Accuracy 0.0508\n",
      "Epoch 18 Batch 400 Loss 0.8831 Accuracy 0.0585\n",
      "Epoch 18 Batch 450 Loss 0.8057 Accuracy 0.0670\n",
      "Epoch 18 Batch 500 Loss 0.7373 Accuracy 0.0762\n",
      "Epoch 18 Batch 550 Loss 0.6792 Accuracy 0.0862\n",
      "Epoch 18 Batch 600 Loss 0.6280 Accuracy 0.0978\n",
      "Epoch 18 Batch 650 Loss 0.5831 Accuracy 0.1111\n",
      "Epoch 18 Batch 700 Loss 0.5459 Accuracy 0.1243\n",
      "Epoch 18 Batch 750 Loss 0.5146 Accuracy 0.1364\n",
      "Epoch 18 Batch 800 Loss 0.4877 Accuracy 0.1507\n",
      "Epoch 18 Batch 850 Loss 0.4634 Accuracy 0.1654\n",
      "Epoch 18 Batch 900 Loss 0.4410 Accuracy 0.1786\n",
      "Epoch 18 Batch 950 Loss 0.4202 Accuracy 0.1902\n",
      "Epoch 18 Batch 1000 Loss 0.4012 Accuracy 0.2008\n",
      "Epoch 18 Batch 1050 Loss 0.3839 Accuracy 0.2098\n",
      "Epoch 18 Batch 1100 Loss 0.3683 Accuracy 0.2183\n",
      "Epoch 18 Loss 0.3665 Accuracy 0.2193\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.8208 Accuracy 0.0140\n",
      "Epoch 19 Batch 50 Loss 1.5889 Accuracy 0.0179\n",
      "Epoch 19 Batch 100 Loss 1.5106 Accuracy 0.0222\n",
      "Epoch 19 Batch 150 Loss 1.3911 Accuracy 0.0269\n",
      "Epoch 19 Batch 200 Loss 1.2745 Accuracy 0.0316\n",
      "Epoch 19 Batch 250 Loss 1.1520 Accuracy 0.0378\n",
      "Epoch 19 Batch 300 Loss 1.0439 Accuracy 0.0441\n",
      "Epoch 19 Batch 350 Loss 0.9434 Accuracy 0.0514\n",
      "Epoch 19 Batch 400 Loss 0.8578 Accuracy 0.0588\n",
      "Epoch 19 Batch 450 Loss 0.7831 Accuracy 0.0670\n",
      "Epoch 19 Batch 500 Loss 0.7180 Accuracy 0.0760\n",
      "Epoch 19 Batch 550 Loss 0.6620 Accuracy 0.0860\n",
      "Epoch 19 Batch 600 Loss 0.6126 Accuracy 0.0975\n",
      "Epoch 19 Batch 650 Loss 0.5698 Accuracy 0.1102\n",
      "Epoch 19 Batch 700 Loss 0.5332 Accuracy 0.1238\n",
      "Epoch 19 Batch 750 Loss 0.5024 Accuracy 0.1363\n",
      "Epoch 19 Batch 800 Loss 0.4767 Accuracy 0.1505\n",
      "Epoch 19 Batch 850 Loss 0.4526 Accuracy 0.1652\n",
      "Epoch 19 Batch 900 Loss 0.4302 Accuracy 0.1783\n",
      "Epoch 19 Batch 950 Loss 0.4101 Accuracy 0.1897\n",
      "Epoch 19 Batch 1000 Loss 0.3917 Accuracy 0.2003\n",
      "Epoch 19 Batch 1050 Loss 0.3747 Accuracy 0.2101\n",
      "Epoch 19 Batch 1100 Loss 0.3593 Accuracy 0.2187\n",
      "Epoch 19 Loss 0.3576 Accuracy 0.2195\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.7685 Accuracy 0.0090\n",
      "Epoch 20 Batch 50 Loss 1.5158 Accuracy 0.0176\n",
      "Epoch 20 Batch 100 Loss 1.4245 Accuracy 0.0223\n",
      "Epoch 20 Batch 150 Loss 1.3071 Accuracy 0.0276\n",
      "Epoch 20 Batch 200 Loss 1.1981 Accuracy 0.0323\n",
      "Epoch 20 Batch 250 Loss 1.0962 Accuracy 0.0380\n",
      "Epoch 20 Batch 300 Loss 0.9955 Accuracy 0.0444\n",
      "Epoch 20 Batch 350 Loss 0.9057 Accuracy 0.0516\n",
      "Epoch 20 Batch 400 Loss 0.8238 Accuracy 0.0592\n",
      "Epoch 20 Batch 450 Loss 0.7512 Accuracy 0.0677\n",
      "Epoch 20 Batch 500 Loss 0.6895 Accuracy 0.0768\n",
      "Epoch 20 Batch 550 Loss 0.6360 Accuracy 0.0865\n",
      "Epoch 20 Batch 600 Loss 0.5882 Accuracy 0.0979\n",
      "Epoch 20 Batch 650 Loss 0.5467 Accuracy 0.1111\n",
      "Epoch 20 Batch 700 Loss 0.5112 Accuracy 0.1244\n",
      "Epoch 20 Batch 750 Loss 0.4816 Accuracy 0.1370\n",
      "Epoch 20 Batch 800 Loss 0.4564 Accuracy 0.1513\n",
      "Epoch 20 Batch 850 Loss 0.4335 Accuracy 0.1655\n",
      "Epoch 20 Batch 900 Loss 0.4124 Accuracy 0.1786\n",
      "Epoch 20 Batch 950 Loss 0.3930 Accuracy 0.1902\n",
      "Epoch 20 Batch 1000 Loss 0.3751 Accuracy 0.2008\n",
      "Epoch 20 Batch 1050 Loss 0.3590 Accuracy 0.2102\n",
      "Epoch 20 Batch 1100 Loss 0.3441 Accuracy 0.2187\n",
      "Epoch 20 Loss 0.3424 Accuracy 0.2197\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.9816 Accuracy 0.0164\n",
      "Epoch 21 Batch 50 Loss 1.4928 Accuracy 0.0187\n",
      "Epoch 21 Batch 100 Loss 1.4369 Accuracy 0.0225\n",
      "Epoch 21 Batch 150 Loss 1.3172 Accuracy 0.0272\n",
      "Epoch 21 Batch 200 Loss 1.1964 Accuracy 0.0322\n",
      "Epoch 21 Batch 250 Loss 1.0767 Accuracy 0.0382\n",
      "Epoch 21 Batch 300 Loss 0.9871 Accuracy 0.0440\n",
      "Epoch 21 Batch 350 Loss 0.8916 Accuracy 0.0514\n",
      "Epoch 21 Batch 400 Loss 0.8122 Accuracy 0.0590\n",
      "Epoch 21 Batch 450 Loss 0.7396 Accuracy 0.0677\n",
      "Epoch 21 Batch 500 Loss 0.6778 Accuracy 0.0770\n",
      "Epoch 21 Batch 550 Loss 0.6236 Accuracy 0.0874\n",
      "Epoch 21 Batch 600 Loss 0.5768 Accuracy 0.0985\n",
      "Epoch 21 Batch 650 Loss 0.5361 Accuracy 0.1117\n",
      "Epoch 21 Batch 700 Loss 0.5019 Accuracy 0.1247\n",
      "Epoch 21 Batch 750 Loss 0.4728 Accuracy 0.1373\n",
      "Epoch 21 Batch 800 Loss 0.4484 Accuracy 0.1516\n",
      "Epoch 21 Batch 850 Loss 0.4257 Accuracy 0.1660\n",
      "Epoch 21 Batch 900 Loss 0.4048 Accuracy 0.1790\n",
      "Epoch 21 Batch 950 Loss 0.3857 Accuracy 0.1908\n",
      "Epoch 21 Batch 1000 Loss 0.3682 Accuracy 0.2011\n",
      "Epoch 21 Batch 1050 Loss 0.3524 Accuracy 0.2107\n",
      "Epoch 21 Batch 1100 Loss 0.3379 Accuracy 0.2190\n",
      "Epoch 21 Loss 0.3362 Accuracy 0.2199\n",
      "\n",
      "Epoch 22 Batch 0 Loss 1.0393 Accuracy 0.0206\n",
      "Epoch 22 Batch 50 Loss 1.4517 Accuracy 0.0189\n",
      "Epoch 22 Batch 100 Loss 1.4072 Accuracy 0.0227\n",
      "Epoch 22 Batch 150 Loss 1.2822 Accuracy 0.0278\n",
      "Epoch 22 Batch 200 Loss 1.1587 Accuracy 0.0328\n",
      "Epoch 22 Batch 250 Loss 1.0542 Accuracy 0.0388\n",
      "Epoch 22 Batch 300 Loss 0.9623 Accuracy 0.0447\n",
      "Epoch 22 Batch 350 Loss 0.8742 Accuracy 0.0516\n",
      "Epoch 22 Batch 400 Loss 0.7938 Accuracy 0.0591\n",
      "Epoch 22 Batch 450 Loss 0.7261 Accuracy 0.0675\n",
      "Epoch 22 Batch 500 Loss 0.6659 Accuracy 0.0765\n",
      "Epoch 22 Batch 550 Loss 0.6132 Accuracy 0.0867\n",
      "Epoch 22 Batch 600 Loss 0.5673 Accuracy 0.0979\n",
      "Epoch 22 Batch 650 Loss 0.5269 Accuracy 0.1112\n",
      "Epoch 22 Batch 700 Loss 0.4928 Accuracy 0.1248\n",
      "Epoch 22 Batch 750 Loss 0.4642 Accuracy 0.1375\n",
      "Epoch 22 Batch 800 Loss 0.4396 Accuracy 0.1518\n",
      "Epoch 22 Batch 850 Loss 0.4170 Accuracy 0.1663\n",
      "Epoch 22 Batch 900 Loss 0.3965 Accuracy 0.1791\n",
      "Epoch 22 Batch 950 Loss 0.3779 Accuracy 0.1904\n",
      "Epoch 22 Batch 1000 Loss 0.3611 Accuracy 0.2009\n",
      "Epoch 22 Batch 1050 Loss 0.3455 Accuracy 0.2103\n",
      "Epoch 22 Batch 1100 Loss 0.3314 Accuracy 0.2191\n",
      "Epoch 22 Loss 0.3298 Accuracy 0.2200\n",
      "\n",
      "Epoch 23 Batch 0 Loss 1.4521 Accuracy 0.0181\n",
      "Epoch 23 Batch 50 Loss 1.5100 Accuracy 0.0184\n",
      "Epoch 23 Batch 100 Loss 1.4448 Accuracy 0.0226\n",
      "Epoch 23 Batch 150 Loss 1.3008 Accuracy 0.0277\n",
      "Epoch 23 Batch 200 Loss 1.1702 Accuracy 0.0328\n",
      "Epoch 23 Batch 250 Loss 1.0607 Accuracy 0.0387\n",
      "Epoch 23 Batch 300 Loss 0.9609 Accuracy 0.0449\n",
      "Epoch 23 Batch 350 Loss 0.8735 Accuracy 0.0519\n",
      "Epoch 23 Batch 400 Loss 0.7948 Accuracy 0.0595\n",
      "Epoch 23 Batch 450 Loss 0.7250 Accuracy 0.0677\n",
      "Epoch 23 Batch 500 Loss 0.6626 Accuracy 0.0770\n",
      "Epoch 23 Batch 550 Loss 0.6109 Accuracy 0.0868\n",
      "Epoch 23 Batch 600 Loss 0.5654 Accuracy 0.0984\n",
      "Epoch 23 Batch 650 Loss 0.5254 Accuracy 0.1116\n",
      "Epoch 23 Batch 700 Loss 0.4917 Accuracy 0.1247\n",
      "Epoch 23 Batch 750 Loss 0.4631 Accuracy 0.1371\n",
      "Epoch 23 Batch 800 Loss 0.4397 Accuracy 0.1517\n",
      "Epoch 23 Batch 850 Loss 0.4171 Accuracy 0.1662\n",
      "Epoch 23 Batch 900 Loss 0.3962 Accuracy 0.1792\n",
      "Epoch 23 Batch 950 Loss 0.3771 Accuracy 0.1907\n",
      "Epoch 23 Batch 1000 Loss 0.3600 Accuracy 0.2010\n",
      "Epoch 23 Batch 1050 Loss 0.3446 Accuracy 0.2107\n",
      "Epoch 23 Batch 1100 Loss 0.3305 Accuracy 0.2192\n",
      "Epoch 23 Loss 0.3288 Accuracy 0.2201\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.6905 Accuracy 0.0140\n",
      "Epoch 24 Batch 50 Loss 1.3628 Accuracy 0.0192\n",
      "Epoch 24 Batch 100 Loss 1.3208 Accuracy 0.0236\n",
      "Epoch 24 Batch 150 Loss 1.2208 Accuracy 0.0280\n",
      "Epoch 24 Batch 200 Loss 1.1217 Accuracy 0.0328\n",
      "Epoch 24 Batch 250 Loss 1.0207 Accuracy 0.0388\n",
      "Epoch 24 Batch 300 Loss 0.9306 Accuracy 0.0455\n",
      "Epoch 24 Batch 350 Loss 0.8450 Accuracy 0.0525\n",
      "Epoch 24 Batch 400 Loss 0.7682 Accuracy 0.0601\n",
      "Epoch 24 Batch 450 Loss 0.6986 Accuracy 0.0682\n",
      "Epoch 24 Batch 500 Loss 0.6407 Accuracy 0.0771\n",
      "Epoch 24 Batch 550 Loss 0.5897 Accuracy 0.0870\n",
      "Epoch 24 Batch 600 Loss 0.5462 Accuracy 0.0980\n",
      "Epoch 24 Batch 650 Loss 0.5073 Accuracy 0.1118\n",
      "Epoch 24 Batch 700 Loss 0.4748 Accuracy 0.1251\n",
      "Epoch 24 Batch 750 Loss 0.4470 Accuracy 0.1376\n",
      "Epoch 24 Batch 800 Loss 0.4239 Accuracy 0.1523\n",
      "Epoch 24 Batch 850 Loss 0.4026 Accuracy 0.1665\n",
      "Epoch 24 Batch 900 Loss 0.3829 Accuracy 0.1793\n",
      "Epoch 24 Batch 950 Loss 0.3644 Accuracy 0.1911\n",
      "Epoch 24 Batch 1000 Loss 0.3481 Accuracy 0.2014\n",
      "Epoch 24 Batch 1050 Loss 0.3330 Accuracy 0.2107\n",
      "Epoch 24 Batch 1100 Loss 0.3192 Accuracy 0.2193\n",
      "Epoch 24 Loss 0.3176 Accuracy 0.2202\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.9118 Accuracy 0.0156\n",
      "Epoch 25 Batch 50 Loss 1.4423 Accuracy 0.0180\n",
      "Epoch 25 Batch 100 Loss 1.3195 Accuracy 0.0233\n",
      "Epoch 25 Batch 150 Loss 1.1962 Accuracy 0.0284\n",
      "Epoch 25 Batch 200 Loss 1.0896 Accuracy 0.0333\n",
      "Epoch 25 Batch 250 Loss 0.9846 Accuracy 0.0394\n",
      "Epoch 25 Batch 300 Loss 0.8930 Accuracy 0.0460\n",
      "Epoch 25 Batch 350 Loss 0.8141 Accuracy 0.0529\n",
      "Epoch 25 Batch 400 Loss 0.7399 Accuracy 0.0605\n",
      "Epoch 25 Batch 450 Loss 0.6753 Accuracy 0.0688\n",
      "Epoch 25 Batch 500 Loss 0.6179 Accuracy 0.0780\n",
      "Epoch 25 Batch 550 Loss 0.5689 Accuracy 0.0881\n",
      "Epoch 25 Batch 600 Loss 0.5265 Accuracy 0.0990\n",
      "Epoch 25 Batch 650 Loss 0.4894 Accuracy 0.1121\n",
      "Epoch 25 Batch 700 Loss 0.4578 Accuracy 0.1259\n",
      "Epoch 25 Batch 750 Loss 0.4312 Accuracy 0.1385\n",
      "Epoch 25 Batch 800 Loss 0.4082 Accuracy 0.1531\n",
      "Epoch 25 Batch 850 Loss 0.3878 Accuracy 0.1676\n",
      "Epoch 25 Batch 900 Loss 0.3684 Accuracy 0.1802\n",
      "Epoch 25 Batch 950 Loss 0.3510 Accuracy 0.1914\n",
      "Epoch 25 Batch 1000 Loss 0.3353 Accuracy 0.2014\n",
      "Epoch 25 Batch 1050 Loss 0.3208 Accuracy 0.2109\n",
      "Epoch 25 Batch 1100 Loss 0.3076 Accuracy 0.2195\n",
      "Epoch 25 Loss 0.3061 Accuracy 0.2205\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.2747 Accuracy 0.0164\n",
      "Epoch 26 Batch 50 Loss 1.3981 Accuracy 0.0192\n",
      "Epoch 26 Batch 100 Loss 1.3087 Accuracy 0.0236\n",
      "Epoch 26 Batch 150 Loss 1.1977 Accuracy 0.0285\n",
      "Epoch 26 Batch 200 Loss 1.0903 Accuracy 0.0333\n",
      "Epoch 26 Batch 250 Loss 0.9977 Accuracy 0.0389\n",
      "Epoch 26 Batch 300 Loss 0.9096 Accuracy 0.0454\n",
      "Epoch 26 Batch 350 Loss 0.8247 Accuracy 0.0525\n",
      "Epoch 26 Batch 400 Loss 0.7510 Accuracy 0.0600\n",
      "Epoch 26 Batch 450 Loss 0.6855 Accuracy 0.0682\n",
      "Epoch 26 Batch 500 Loss 0.6286 Accuracy 0.0773\n",
      "Epoch 26 Batch 550 Loss 0.5797 Accuracy 0.0874\n",
      "Epoch 26 Batch 600 Loss 0.5367 Accuracy 0.0987\n",
      "Epoch 26 Batch 650 Loss 0.4986 Accuracy 0.1122\n",
      "Epoch 26 Batch 700 Loss 0.4664 Accuracy 0.1255\n",
      "Epoch 26 Batch 750 Loss 0.4389 Accuracy 0.1382\n",
      "Epoch 26 Batch 800 Loss 0.4157 Accuracy 0.1526\n",
      "Epoch 26 Batch 850 Loss 0.3947 Accuracy 0.1670\n",
      "Epoch 26 Batch 900 Loss 0.3748 Accuracy 0.1795\n",
      "Epoch 26 Batch 950 Loss 0.3566 Accuracy 0.1912\n",
      "Epoch 26 Batch 1000 Loss 0.3402 Accuracy 0.2016\n",
      "Epoch 26 Batch 1050 Loss 0.3254 Accuracy 0.2111\n",
      "Epoch 26 Batch 1100 Loss 0.3120 Accuracy 0.2195\n",
      "Epoch 26 Loss 0.3105 Accuracy 0.2205\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.6852 Accuracy 0.0197\n",
      "Epoch 27 Batch 50 Loss 1.3536 Accuracy 0.0194\n",
      "Epoch 27 Batch 100 Loss 1.2519 Accuracy 0.0241\n",
      "Epoch 27 Batch 150 Loss 1.1565 Accuracy 0.0290\n",
      "Epoch 27 Batch 200 Loss 1.0677 Accuracy 0.0338\n",
      "Epoch 27 Batch 250 Loss 0.9701 Accuracy 0.0397\n",
      "Epoch 27 Batch 300 Loss 0.8797 Accuracy 0.0462\n",
      "Epoch 27 Batch 350 Loss 0.7996 Accuracy 0.0532\n",
      "Epoch 27 Batch 400 Loss 0.7257 Accuracy 0.0609\n",
      "Epoch 27 Batch 450 Loss 0.6638 Accuracy 0.0691\n",
      "Epoch 27 Batch 500 Loss 0.6095 Accuracy 0.0777\n",
      "Epoch 27 Batch 550 Loss 0.5607 Accuracy 0.0877\n",
      "Epoch 27 Batch 600 Loss 0.5191 Accuracy 0.0992\n",
      "Epoch 27 Batch 650 Loss 0.4818 Accuracy 0.1125\n",
      "Epoch 27 Batch 700 Loss 0.4508 Accuracy 0.1259\n",
      "Epoch 27 Batch 750 Loss 0.4245 Accuracy 0.1383\n",
      "Epoch 27 Batch 800 Loss 0.4022 Accuracy 0.1527\n",
      "Epoch 27 Batch 850 Loss 0.3820 Accuracy 0.1670\n",
      "Epoch 27 Batch 900 Loss 0.3633 Accuracy 0.1803\n",
      "Epoch 27 Batch 950 Loss 0.3459 Accuracy 0.1919\n",
      "Epoch 27 Batch 1000 Loss 0.3299 Accuracy 0.2019\n",
      "Epoch 27 Batch 1050 Loss 0.3155 Accuracy 0.2113\n",
      "Epoch 27 Batch 1100 Loss 0.3025 Accuracy 0.2198\n",
      "Epoch 27 Loss 0.3010 Accuracy 0.2206\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.3278 Accuracy 0.0181\n",
      "Epoch 28 Batch 50 Loss 1.3014 Accuracy 0.0193\n",
      "Epoch 28 Batch 100 Loss 1.2231 Accuracy 0.0241\n",
      "Epoch 28 Batch 150 Loss 1.1227 Accuracy 0.0291\n",
      "Epoch 28 Batch 200 Loss 1.0403 Accuracy 0.0337\n",
      "Epoch 28 Batch 250 Loss 0.9505 Accuracy 0.0396\n",
      "Epoch 28 Batch 300 Loss 0.8642 Accuracy 0.0456\n",
      "Epoch 28 Batch 350 Loss 0.7836 Accuracy 0.0528\n",
      "Epoch 28 Batch 400 Loss 0.7170 Accuracy 0.0606\n",
      "Epoch 28 Batch 450 Loss 0.6557 Accuracy 0.0688\n",
      "Epoch 28 Batch 500 Loss 0.6011 Accuracy 0.0777\n",
      "Epoch 28 Batch 550 Loss 0.5530 Accuracy 0.0880\n",
      "Epoch 28 Batch 600 Loss 0.5115 Accuracy 0.0993\n",
      "Epoch 28 Batch 650 Loss 0.4752 Accuracy 0.1125\n",
      "Epoch 28 Batch 700 Loss 0.4447 Accuracy 0.1258\n",
      "Epoch 28 Batch 750 Loss 0.4190 Accuracy 0.1382\n",
      "Epoch 28 Batch 800 Loss 0.3970 Accuracy 0.1526\n",
      "Epoch 28 Batch 850 Loss 0.3765 Accuracy 0.1670\n",
      "Epoch 28 Batch 900 Loss 0.3577 Accuracy 0.1803\n",
      "Epoch 28 Batch 950 Loss 0.3404 Accuracy 0.1920\n",
      "Epoch 28 Batch 1000 Loss 0.3252 Accuracy 0.2025\n",
      "Epoch 28 Batch 1050 Loss 0.3111 Accuracy 0.2115\n",
      "Epoch 28 Batch 1100 Loss 0.2983 Accuracy 0.2197\n",
      "Epoch 28 Loss 0.2968 Accuracy 0.2206\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.6293 Accuracy 0.0148\n",
      "Epoch 29 Batch 50 Loss 1.2795 Accuracy 0.0195\n",
      "Epoch 29 Batch 100 Loss 1.2815 Accuracy 0.0236\n",
      "Epoch 29 Batch 150 Loss 1.1661 Accuracy 0.0286\n",
      "Epoch 29 Batch 200 Loss 1.0445 Accuracy 0.0340\n",
      "Epoch 29 Batch 250 Loss 0.9569 Accuracy 0.0398\n",
      "Epoch 29 Batch 300 Loss 0.8707 Accuracy 0.0461\n",
      "Epoch 29 Batch 350 Loss 0.7915 Accuracy 0.0529\n",
      "Epoch 29 Batch 400 Loss 0.7210 Accuracy 0.0604\n",
      "Epoch 29 Batch 450 Loss 0.6588 Accuracy 0.0687\n",
      "Epoch 29 Batch 500 Loss 0.6050 Accuracy 0.0777\n",
      "Epoch 29 Batch 550 Loss 0.5573 Accuracy 0.0877\n",
      "Epoch 29 Batch 600 Loss 0.5152 Accuracy 0.0989\n",
      "Epoch 29 Batch 650 Loss 0.4784 Accuracy 0.1120\n",
      "Epoch 29 Batch 700 Loss 0.4471 Accuracy 0.1252\n",
      "Epoch 29 Batch 750 Loss 0.4207 Accuracy 0.1377\n",
      "Epoch 29 Batch 800 Loss 0.3986 Accuracy 0.1519\n",
      "Epoch 29 Batch 850 Loss 0.3785 Accuracy 0.1666\n",
      "Epoch 29 Batch 900 Loss 0.3598 Accuracy 0.1799\n",
      "Epoch 29 Batch 950 Loss 0.3426 Accuracy 0.1917\n",
      "Epoch 29 Batch 1000 Loss 0.3270 Accuracy 0.2019\n",
      "Epoch 29 Batch 1050 Loss 0.3126 Accuracy 0.2113\n",
      "Epoch 29 Batch 1100 Loss 0.2997 Accuracy 0.2197\n",
      "Epoch 29 Loss 0.2982 Accuracy 0.2208\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.7504 Accuracy 0.0173\n",
      "Epoch 30 Batch 50 Loss 1.2639 Accuracy 0.0198\n",
      "Epoch 30 Batch 100 Loss 1.2008 Accuracy 0.0247\n",
      "Epoch 30 Batch 150 Loss 1.1091 Accuracy 0.0293\n",
      "Epoch 30 Batch 200 Loss 1.0142 Accuracy 0.0343\n",
      "Epoch 30 Batch 250 Loss 0.9234 Accuracy 0.0401\n",
      "Epoch 30 Batch 300 Loss 0.8403 Accuracy 0.0464\n",
      "Epoch 30 Batch 350 Loss 0.7642 Accuracy 0.0535\n",
      "Epoch 30 Batch 400 Loss 0.6952 Accuracy 0.0610\n",
      "Epoch 30 Batch 450 Loss 0.6344 Accuracy 0.0691\n",
      "Epoch 30 Batch 500 Loss 0.5813 Accuracy 0.0778\n",
      "Epoch 30 Batch 550 Loss 0.5347 Accuracy 0.0879\n",
      "Epoch 30 Batch 600 Loss 0.4952 Accuracy 0.0989\n",
      "Epoch 30 Batch 650 Loss 0.4601 Accuracy 0.1123\n",
      "Epoch 30 Batch 700 Loss 0.4301 Accuracy 0.1254\n",
      "Epoch 30 Batch 750 Loss 0.4055 Accuracy 0.1375\n",
      "Epoch 30 Batch 800 Loss 0.3834 Accuracy 0.1518\n",
      "Epoch 30 Batch 850 Loss 0.3636 Accuracy 0.1667\n",
      "Epoch 30 Batch 900 Loss 0.3453 Accuracy 0.1795\n",
      "Epoch 30 Batch 950 Loss 0.3288 Accuracy 0.1913\n",
      "Epoch 30 Batch 1000 Loss 0.3139 Accuracy 0.2015\n",
      "Epoch 30 Batch 1050 Loss 0.3000 Accuracy 0.2114\n",
      "Epoch 30 Batch 1100 Loss 0.2875 Accuracy 0.2201\n",
      "Epoch 30 Loss 0.2861 Accuracy 0.2209\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.6644 Accuracy 0.0156\n",
      "Epoch 31 Batch 50 Loss 1.2531 Accuracy 0.0208\n",
      "Epoch 31 Batch 100 Loss 1.1988 Accuracy 0.0245\n",
      "Epoch 31 Batch 150 Loss 1.1100 Accuracy 0.0294\n",
      "Epoch 31 Batch 200 Loss 1.0217 Accuracy 0.0343\n",
      "Epoch 31 Batch 250 Loss 0.9297 Accuracy 0.0401\n",
      "Epoch 31 Batch 300 Loss 0.8482 Accuracy 0.0465\n",
      "Epoch 31 Batch 350 Loss 0.7716 Accuracy 0.0533\n",
      "Epoch 31 Batch 400 Loss 0.7039 Accuracy 0.0606\n",
      "Epoch 31 Batch 450 Loss 0.6422 Accuracy 0.0691\n",
      "Epoch 31 Batch 500 Loss 0.5884 Accuracy 0.0783\n",
      "Epoch 31 Batch 550 Loss 0.5418 Accuracy 0.0880\n",
      "Epoch 31 Batch 600 Loss 0.5011 Accuracy 0.0993\n",
      "Epoch 31 Batch 650 Loss 0.4654 Accuracy 0.1126\n",
      "Epoch 31 Batch 700 Loss 0.4348 Accuracy 0.1260\n",
      "Epoch 31 Batch 750 Loss 0.4091 Accuracy 0.1383\n",
      "Epoch 31 Batch 800 Loss 0.3875 Accuracy 0.1527\n",
      "Epoch 31 Batch 850 Loss 0.3677 Accuracy 0.1670\n",
      "Epoch 31 Batch 900 Loss 0.3492 Accuracy 0.1803\n",
      "Epoch 31 Batch 950 Loss 0.3325 Accuracy 0.1920\n",
      "Epoch 31 Batch 1000 Loss 0.3174 Accuracy 0.2026\n",
      "Epoch 31 Batch 1050 Loss 0.3036 Accuracy 0.2115\n",
      "Epoch 31 Batch 1100 Loss 0.2909 Accuracy 0.2200\n",
      "Epoch 31 Loss 0.2895 Accuracy 0.2209\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.5734 Accuracy 0.0164\n",
      "Epoch 32 Batch 50 Loss 1.2854 Accuracy 0.0197\n",
      "Epoch 32 Batch 100 Loss 1.2202 Accuracy 0.0242\n",
      "Epoch 32 Batch 150 Loss 1.1153 Accuracy 0.0292\n",
      "Epoch 32 Batch 200 Loss 1.0286 Accuracy 0.0340\n",
      "Epoch 32 Batch 250 Loss 0.9349 Accuracy 0.0398\n",
      "Epoch 32 Batch 300 Loss 0.8559 Accuracy 0.0460\n",
      "Epoch 32 Batch 350 Loss 0.7700 Accuracy 0.0533\n",
      "Epoch 32 Batch 400 Loss 0.7012 Accuracy 0.0607\n",
      "Epoch 32 Batch 450 Loss 0.6393 Accuracy 0.0692\n",
      "Epoch 32 Batch 500 Loss 0.5864 Accuracy 0.0781\n",
      "Epoch 32 Batch 550 Loss 0.5389 Accuracy 0.0882\n",
      "Epoch 32 Batch 600 Loss 0.4985 Accuracy 0.0995\n",
      "Epoch 32 Batch 650 Loss 0.4632 Accuracy 0.1128\n",
      "Epoch 32 Batch 700 Loss 0.4330 Accuracy 0.1261\n",
      "Epoch 32 Batch 750 Loss 0.4082 Accuracy 0.1382\n",
      "Epoch 32 Batch 800 Loss 0.3866 Accuracy 0.1528\n",
      "Epoch 32 Batch 850 Loss 0.3673 Accuracy 0.1669\n",
      "Epoch 32 Batch 900 Loss 0.3488 Accuracy 0.1801\n",
      "Epoch 32 Batch 950 Loss 0.3320 Accuracy 0.1917\n",
      "Epoch 32 Batch 1000 Loss 0.3167 Accuracy 0.2020\n",
      "Epoch 32 Batch 1050 Loss 0.3028 Accuracy 0.2114\n",
      "Epoch 32 Batch 1100 Loss 0.2903 Accuracy 0.2201\n",
      "Epoch 32 Loss 0.2888 Accuracy 0.2209\n",
      "\n",
      "Epoch 33 Batch 0 Loss 1.2659 Accuracy 0.0230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 05:00:17.574598: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 50 Loss 1.2699 Accuracy 0.0204\n",
      "Epoch 33 Batch 100 Loss 1.1587 Accuracy 0.0248\n",
      "Epoch 33 Batch 150 Loss 1.0688 Accuracy 0.0295\n",
      "Epoch 33 Batch 200 Loss 0.9830 Accuracy 0.0342\n",
      "Epoch 33 Batch 250 Loss 0.8981 Accuracy 0.0396\n",
      "Epoch 33 Batch 300 Loss 0.8199 Accuracy 0.0461\n",
      "Epoch 33 Batch 350 Loss 0.7453 Accuracy 0.0531\n",
      "Epoch 33 Batch 400 Loss 0.6787 Accuracy 0.0605\n",
      "Epoch 33 Batch 450 Loss 0.6208 Accuracy 0.0689\n",
      "Epoch 33 Batch 500 Loss 0.5692 Accuracy 0.0777\n",
      "Epoch 33 Batch 550 Loss 0.5241 Accuracy 0.0880\n",
      "Epoch 33 Batch 600 Loss 0.4841 Accuracy 0.0993\n",
      "Epoch 33 Batch 650 Loss 0.4494 Accuracy 0.1126\n",
      "Epoch 33 Batch 700 Loss 0.4204 Accuracy 0.1260\n",
      "Epoch 33 Batch 750 Loss 0.3964 Accuracy 0.1383\n",
      "Epoch 33 Batch 800 Loss 0.3752 Accuracy 0.1525\n",
      "Epoch 33 Batch 850 Loss 0.3560 Accuracy 0.1668\n",
      "Epoch 33 Batch 900 Loss 0.3382 Accuracy 0.1799\n",
      "Epoch 33 Batch 950 Loss 0.3221 Accuracy 0.1915\n",
      "Epoch 33 Batch 1000 Loss 0.3073 Accuracy 0.2019\n",
      "Epoch 33 Batch 1050 Loss 0.2940 Accuracy 0.2112\n",
      "Epoch 33 Batch 1100 Loss 0.2819 Accuracy 0.2201\n",
      "Epoch 33 Loss 0.2804 Accuracy 0.2210\n",
      "\n",
      "Epoch 34 Batch 0 Loss 1.1880 Accuracy 0.0222\n",
      "Epoch 34 Batch 50 Loss 1.2226 Accuracy 0.0198\n",
      "Epoch 34 Batch 100 Loss 1.1259 Accuracy 0.0242\n",
      "Epoch 34 Batch 150 Loss 1.0343 Accuracy 0.0292\n",
      "Epoch 34 Batch 200 Loss 0.9543 Accuracy 0.0342\n",
      "Epoch 34 Batch 250 Loss 0.8722 Accuracy 0.0403\n",
      "Epoch 34 Batch 300 Loss 0.7997 Accuracy 0.0465\n",
      "Epoch 34 Batch 350 Loss 0.7311 Accuracy 0.0534\n",
      "Epoch 34 Batch 400 Loss 0.6651 Accuracy 0.0612\n",
      "Epoch 34 Batch 450 Loss 0.6053 Accuracy 0.0696\n",
      "Epoch 34 Batch 500 Loss 0.5543 Accuracy 0.0787\n",
      "Epoch 34 Batch 550 Loss 0.5110 Accuracy 0.0888\n",
      "Epoch 34 Batch 600 Loss 0.4732 Accuracy 0.0999\n",
      "Epoch 34 Batch 650 Loss 0.4394 Accuracy 0.1128\n",
      "Epoch 34 Batch 700 Loss 0.4109 Accuracy 0.1262\n",
      "Epoch 34 Batch 750 Loss 0.3871 Accuracy 0.1387\n",
      "Epoch 34 Batch 800 Loss 0.3667 Accuracy 0.1531\n",
      "Epoch 34 Batch 850 Loss 0.3480 Accuracy 0.1672\n",
      "Epoch 34 Batch 900 Loss 0.3306 Accuracy 0.1802\n",
      "Epoch 34 Batch 950 Loss 0.3147 Accuracy 0.1914\n",
      "Epoch 34 Batch 1000 Loss 0.3002 Accuracy 0.2020\n",
      "Epoch 34 Batch 1050 Loss 0.2871 Accuracy 0.2114\n",
      "Epoch 34 Batch 1100 Loss 0.2751 Accuracy 0.2202\n",
      "Epoch 34 Loss 0.2737 Accuracy 0.2211\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.7203 Accuracy 0.0238\n",
      "Epoch 35 Batch 50 Loss 1.1775 Accuracy 0.0203\n",
      "Epoch 35 Batch 100 Loss 1.1729 Accuracy 0.0246\n",
      "Epoch 35 Batch 150 Loss 1.0669 Accuracy 0.0295\n",
      "Epoch 35 Batch 200 Loss 0.9771 Accuracy 0.0346\n",
      "Epoch 35 Batch 250 Loss 0.8853 Accuracy 0.0406\n",
      "Epoch 35 Batch 300 Loss 0.8066 Accuracy 0.0466\n",
      "Epoch 35 Batch 350 Loss 0.7317 Accuracy 0.0537\n",
      "Epoch 35 Batch 400 Loss 0.6655 Accuracy 0.0614\n",
      "Epoch 35 Batch 450 Loss 0.6063 Accuracy 0.0696\n",
      "Epoch 35 Batch 500 Loss 0.5559 Accuracy 0.0788\n",
      "Epoch 35 Batch 550 Loss 0.5121 Accuracy 0.0884\n",
      "Epoch 35 Batch 600 Loss 0.4736 Accuracy 0.0999\n",
      "Epoch 35 Batch 650 Loss 0.4397 Accuracy 0.1128\n",
      "Epoch 35 Batch 700 Loss 0.4112 Accuracy 0.1267\n",
      "Epoch 35 Batch 750 Loss 0.3871 Accuracy 0.1393\n",
      "Epoch 35 Batch 800 Loss 0.3665 Accuracy 0.1537\n",
      "Epoch 35 Batch 850 Loss 0.3474 Accuracy 0.1682\n",
      "Epoch 35 Batch 900 Loss 0.3302 Accuracy 0.1808\n",
      "Epoch 35 Batch 950 Loss 0.3145 Accuracy 0.1923\n",
      "Epoch 35 Batch 1000 Loss 0.3000 Accuracy 0.2024\n",
      "Epoch 35 Batch 1050 Loss 0.2868 Accuracy 0.2120\n",
      "Epoch 35 Batch 1100 Loss 0.2748 Accuracy 0.2203\n",
      "Epoch 35 Loss 0.2734 Accuracy 0.2212\n",
      "\n",
      "Epoch 36 Batch 0 Loss 1.8903 Accuracy 0.0132\n",
      "Epoch 36 Batch 50 Loss 1.2537 Accuracy 0.0198\n",
      "Epoch 36 Batch 100 Loss 1.1728 Accuracy 0.0249\n",
      "Epoch 36 Batch 150 Loss 1.0702 Accuracy 0.0298\n",
      "Epoch 36 Batch 200 Loss 0.9842 Accuracy 0.0344\n",
      "Epoch 36 Batch 250 Loss 0.8920 Accuracy 0.0404\n",
      "Epoch 36 Batch 300 Loss 0.8070 Accuracy 0.0466\n",
      "Epoch 36 Batch 350 Loss 0.7303 Accuracy 0.0536\n",
      "Epoch 36 Batch 400 Loss 0.6639 Accuracy 0.0614\n",
      "Epoch 36 Batch 450 Loss 0.6072 Accuracy 0.0695\n",
      "Epoch 36 Batch 500 Loss 0.5566 Accuracy 0.0785\n",
      "Epoch 36 Batch 550 Loss 0.5120 Accuracy 0.0885\n",
      "Epoch 36 Batch 600 Loss 0.4738 Accuracy 0.0995\n",
      "Epoch 36 Batch 650 Loss 0.4397 Accuracy 0.1126\n",
      "Epoch 36 Batch 700 Loss 0.4111 Accuracy 0.1259\n",
      "Epoch 36 Batch 750 Loss 0.3867 Accuracy 0.1384\n",
      "Epoch 36 Batch 800 Loss 0.3662 Accuracy 0.1523\n",
      "Epoch 36 Batch 850 Loss 0.3477 Accuracy 0.1668\n",
      "Epoch 36 Batch 900 Loss 0.3300 Accuracy 0.1798\n",
      "Epoch 36 Batch 950 Loss 0.3139 Accuracy 0.1916\n",
      "Epoch 36 Batch 1000 Loss 0.2994 Accuracy 0.2019\n",
      "Epoch 36 Batch 1050 Loss 0.2862 Accuracy 0.2116\n",
      "Epoch 36 Batch 1100 Loss 0.2742 Accuracy 0.2203\n",
      "Epoch 36 Loss 0.2728 Accuracy 0.2213\n",
      "\n",
      "Epoch 37 Batch 0 Loss 1.5649 Accuracy 0.0206\n",
      "Epoch 37 Batch 50 Loss 1.2039 Accuracy 0.0197\n",
      "Epoch 37 Batch 100 Loss 1.1524 Accuracy 0.0249\n",
      "Epoch 37 Batch 150 Loss 1.0518 Accuracy 0.0298\n",
      "Epoch 37 Batch 200 Loss 0.9678 Accuracy 0.0347\n",
      "Epoch 37 Batch 250 Loss 0.8811 Accuracy 0.0405\n",
      "Epoch 37 Batch 300 Loss 0.8014 Accuracy 0.0466\n",
      "Epoch 37 Batch 350 Loss 0.7275 Accuracy 0.0533\n",
      "Epoch 37 Batch 400 Loss 0.6588 Accuracy 0.0613\n",
      "Epoch 37 Batch 450 Loss 0.5991 Accuracy 0.0695\n",
      "Epoch 37 Batch 500 Loss 0.5507 Accuracy 0.0780\n",
      "Epoch 37 Batch 550 Loss 0.5068 Accuracy 0.0882\n",
      "Epoch 37 Batch 600 Loss 0.4684 Accuracy 0.0997\n",
      "Epoch 37 Batch 650 Loss 0.4350 Accuracy 0.1129\n",
      "Epoch 37 Batch 700 Loss 0.4068 Accuracy 0.1263\n",
      "Epoch 37 Batch 750 Loss 0.3832 Accuracy 0.1387\n",
      "Epoch 37 Batch 800 Loss 0.3626 Accuracy 0.1532\n",
      "Epoch 37 Batch 850 Loss 0.3436 Accuracy 0.1681\n",
      "Epoch 37 Batch 900 Loss 0.3263 Accuracy 0.1812\n",
      "Epoch 37 Batch 950 Loss 0.3105 Accuracy 0.1925\n",
      "Epoch 37 Batch 1000 Loss 0.2962 Accuracy 0.2026\n",
      "Epoch 37 Batch 1050 Loss 0.2834 Accuracy 0.2119\n",
      "Epoch 37 Batch 1100 Loss 0.2716 Accuracy 0.2204\n",
      "Epoch 37 Loss 0.2702 Accuracy 0.2213\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.5965 Accuracy 0.0181\n",
      "Epoch 38 Batch 50 Loss 1.1269 Accuracy 0.0206\n",
      "Epoch 38 Batch 100 Loss 1.0903 Accuracy 0.0250\n",
      "Epoch 38 Batch 150 Loss 1.0208 Accuracy 0.0296\n",
      "Epoch 38 Batch 200 Loss 0.9412 Accuracy 0.0344\n",
      "Epoch 38 Batch 250 Loss 0.8642 Accuracy 0.0402\n",
      "Epoch 38 Batch 300 Loss 0.7927 Accuracy 0.0463\n",
      "Epoch 38 Batch 350 Loss 0.7183 Accuracy 0.0534\n",
      "Epoch 38 Batch 400 Loss 0.6512 Accuracy 0.0611\n",
      "Epoch 38 Batch 450 Loss 0.5936 Accuracy 0.0696\n",
      "Epoch 38 Batch 500 Loss 0.5442 Accuracy 0.0785\n",
      "Epoch 38 Batch 550 Loss 0.5014 Accuracy 0.0882\n",
      "Epoch 38 Batch 600 Loss 0.4636 Accuracy 0.0993\n",
      "Epoch 38 Batch 650 Loss 0.4307 Accuracy 0.1126\n",
      "Epoch 38 Batch 700 Loss 0.4026 Accuracy 0.1260\n",
      "Epoch 38 Batch 750 Loss 0.3793 Accuracy 0.1386\n",
      "Epoch 38 Batch 800 Loss 0.3588 Accuracy 0.1532\n",
      "Epoch 38 Batch 850 Loss 0.3403 Accuracy 0.1673\n",
      "Epoch 38 Batch 900 Loss 0.3233 Accuracy 0.1804\n",
      "Epoch 38 Batch 950 Loss 0.3077 Accuracy 0.1918\n",
      "Epoch 38 Batch 1000 Loss 0.2935 Accuracy 0.2024\n",
      "Epoch 38 Batch 1050 Loss 0.2805 Accuracy 0.2120\n",
      "Epoch 38 Batch 1100 Loss 0.2688 Accuracy 0.2204\n",
      "Epoch 38 Loss 0.2674 Accuracy 0.2213\n",
      "\n",
      "Epoch 39 Batch 0 Loss 1.1055 Accuracy 0.0197\n",
      "Epoch 39 Batch 50 Loss 1.1359 Accuracy 0.0204\n",
      "Epoch 39 Batch 100 Loss 1.0835 Accuracy 0.0249\n",
      "Epoch 39 Batch 150 Loss 1.0000 Accuracy 0.0298\n",
      "Epoch 39 Batch 200 Loss 0.9078 Accuracy 0.0348\n",
      "Epoch 39 Batch 250 Loss 0.8223 Accuracy 0.0411\n",
      "Epoch 39 Batch 300 Loss 0.7524 Accuracy 0.0475\n",
      "Epoch 39 Batch 350 Loss 0.6854 Accuracy 0.0542\n",
      "Epoch 39 Batch 400 Loss 0.6256 Accuracy 0.0618\n",
      "Epoch 39 Batch 450 Loss 0.5718 Accuracy 0.0698\n",
      "Epoch 39 Batch 500 Loss 0.5241 Accuracy 0.0788\n",
      "Epoch 39 Batch 550 Loss 0.4829 Accuracy 0.0888\n",
      "Epoch 39 Batch 600 Loss 0.4468 Accuracy 0.1001\n",
      "Epoch 39 Batch 650 Loss 0.4153 Accuracy 0.1133\n",
      "Epoch 39 Batch 700 Loss 0.3884 Accuracy 0.1266\n",
      "Epoch 39 Batch 750 Loss 0.3660 Accuracy 0.1387\n",
      "Epoch 39 Batch 800 Loss 0.3470 Accuracy 0.1530\n",
      "Epoch 39 Batch 850 Loss 0.3289 Accuracy 0.1677\n",
      "Epoch 39 Batch 900 Loss 0.3124 Accuracy 0.1805\n",
      "Epoch 39 Batch 950 Loss 0.2974 Accuracy 0.1921\n",
      "Epoch 39 Batch 1000 Loss 0.2840 Accuracy 0.2024\n",
      "Epoch 39 Batch 1050 Loss 0.2715 Accuracy 0.2120\n",
      "Epoch 39 Batch 1100 Loss 0.2602 Accuracy 0.2204\n",
      "Epoch 39 Loss 0.2589 Accuracy 0.2214\n",
      "\n",
      "Epoch 40 Batch 0 Loss 1.2348 Accuracy 0.0156\n",
      "Epoch 40 Batch 50 Loss 1.1432 Accuracy 0.0201\n",
      "Epoch 40 Batch 100 Loss 1.0778 Accuracy 0.0248\n",
      "Epoch 40 Batch 150 Loss 0.9706 Accuracy 0.0299\n",
      "Epoch 40 Batch 200 Loss 0.8955 Accuracy 0.0349\n",
      "Epoch 40 Batch 250 Loss 0.8207 Accuracy 0.0407\n",
      "Epoch 40 Batch 300 Loss 0.7441 Accuracy 0.0470\n",
      "Epoch 40 Batch 350 Loss 0.6759 Accuracy 0.0541\n",
      "Epoch 40 Batch 400 Loss 0.6141 Accuracy 0.0620\n",
      "Epoch 40 Batch 450 Loss 0.5597 Accuracy 0.0702\n",
      "Epoch 40 Batch 500 Loss 0.5132 Accuracy 0.0791\n",
      "Epoch 40 Batch 550 Loss 0.4729 Accuracy 0.0890\n",
      "Epoch 40 Batch 600 Loss 0.4374 Accuracy 0.1001\n",
      "Epoch 40 Batch 650 Loss 0.4064 Accuracy 0.1132\n",
      "Epoch 40 Batch 700 Loss 0.3802 Accuracy 0.1268\n",
      "Epoch 40 Batch 750 Loss 0.3582 Accuracy 0.1390\n",
      "Epoch 40 Batch 800 Loss 0.3395 Accuracy 0.1535\n",
      "Epoch 40 Batch 850 Loss 0.3221 Accuracy 0.1681\n",
      "Epoch 40 Batch 900 Loss 0.3058 Accuracy 0.1809\n",
      "Epoch 40 Batch 950 Loss 0.2912 Accuracy 0.1923\n",
      "Epoch 40 Batch 1000 Loss 0.2781 Accuracy 0.2026\n",
      "Epoch 40 Batch 1050 Loss 0.2660 Accuracy 0.2120\n",
      "Epoch 40 Batch 1100 Loss 0.2548 Accuracy 0.2206\n",
      "Epoch 40 Loss 0.2535 Accuracy 0.2214\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.7877 Accuracy 0.0181\n",
      "Epoch 41 Batch 50 Loss 1.1460 Accuracy 0.0203\n",
      "Epoch 41 Batch 100 Loss 1.0624 Accuracy 0.0253\n",
      "Epoch 41 Batch 150 Loss 0.9825 Accuracy 0.0300\n",
      "Epoch 41 Batch 200 Loss 0.9138 Accuracy 0.0348\n",
      "Epoch 41 Batch 250 Loss 0.8339 Accuracy 0.0406\n",
      "Epoch 41 Batch 300 Loss 0.7581 Accuracy 0.0472\n",
      "Epoch 41 Batch 350 Loss 0.6857 Accuracy 0.0542\n",
      "Epoch 41 Batch 400 Loss 0.6230 Accuracy 0.0619\n",
      "Epoch 41 Batch 450 Loss 0.5708 Accuracy 0.0699\n",
      "Epoch 41 Batch 500 Loss 0.5244 Accuracy 0.0788\n",
      "Epoch 41 Batch 550 Loss 0.4830 Accuracy 0.0888\n",
      "Epoch 41 Batch 600 Loss 0.4472 Accuracy 0.1000\n",
      "Epoch 41 Batch 650 Loss 0.4156 Accuracy 0.1129\n",
      "Epoch 41 Batch 700 Loss 0.3888 Accuracy 0.1260\n",
      "Epoch 41 Batch 750 Loss 0.3664 Accuracy 0.1383\n",
      "Epoch 41 Batch 800 Loss 0.3470 Accuracy 0.1529\n",
      "Epoch 41 Batch 850 Loss 0.3288 Accuracy 0.1673\n",
      "Epoch 41 Batch 900 Loss 0.3124 Accuracy 0.1806\n",
      "Epoch 41 Batch 950 Loss 0.2972 Accuracy 0.1919\n",
      "Epoch 41 Batch 1000 Loss 0.2834 Accuracy 0.2029\n",
      "Epoch 41 Batch 1050 Loss 0.2710 Accuracy 0.2117\n",
      "Epoch 41 Batch 1100 Loss 0.2597 Accuracy 0.2206\n",
      "Epoch 41 Loss 0.2584 Accuracy 0.2214\n",
      "\n",
      "Epoch 42 Batch 0 Loss 1.7400 Accuracy 0.0115\n",
      "Epoch 42 Batch 50 Loss 1.1749 Accuracy 0.0197\n",
      "Epoch 42 Batch 100 Loss 1.0803 Accuracy 0.0248\n",
      "Epoch 42 Batch 150 Loss 0.9697 Accuracy 0.0303\n",
      "Epoch 42 Batch 200 Loss 0.9017 Accuracy 0.0352\n",
      "Epoch 42 Batch 250 Loss 0.8153 Accuracy 0.0413\n",
      "Epoch 42 Batch 300 Loss 0.7424 Accuracy 0.0473\n",
      "Epoch 42 Batch 350 Loss 0.6788 Accuracy 0.0538\n",
      "Epoch 42 Batch 400 Loss 0.6211 Accuracy 0.0614\n",
      "Epoch 42 Batch 450 Loss 0.5668 Accuracy 0.0698\n",
      "Epoch 42 Batch 500 Loss 0.5188 Accuracy 0.0791\n",
      "Epoch 42 Batch 550 Loss 0.4774 Accuracy 0.0890\n",
      "Epoch 42 Batch 600 Loss 0.4412 Accuracy 0.1001\n",
      "Epoch 42 Batch 650 Loss 0.4102 Accuracy 0.1131\n",
      "Epoch 42 Batch 700 Loss 0.3834 Accuracy 0.1263\n",
      "Epoch 42 Batch 750 Loss 0.3606 Accuracy 0.1389\n",
      "Epoch 42 Batch 800 Loss 0.3418 Accuracy 0.1531\n",
      "Epoch 42 Batch 850 Loss 0.3242 Accuracy 0.1679\n",
      "Epoch 42 Batch 900 Loss 0.3076 Accuracy 0.1806\n",
      "Epoch 42 Batch 950 Loss 0.2927 Accuracy 0.1920\n",
      "Epoch 42 Batch 1000 Loss 0.2794 Accuracy 0.2027\n",
      "Epoch 42 Batch 1050 Loss 0.2673 Accuracy 0.2121\n",
      "Epoch 42 Batch 1100 Loss 0.2561 Accuracy 0.2205\n",
      "Epoch 42 Loss 0.2549 Accuracy 0.2215\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.8747 Accuracy 0.0197\n",
      "Epoch 43 Batch 50 Loss 1.1911 Accuracy 0.0203\n",
      "Epoch 43 Batch 100 Loss 1.0829 Accuracy 0.0256\n",
      "Epoch 43 Batch 150 Loss 0.9838 Accuracy 0.0307\n",
      "Epoch 43 Batch 200 Loss 0.9139 Accuracy 0.0355\n",
      "Epoch 43 Batch 250 Loss 0.8349 Accuracy 0.0411\n",
      "Epoch 43 Batch 300 Loss 0.7598 Accuracy 0.0476\n",
      "Epoch 43 Batch 350 Loss 0.6912 Accuracy 0.0543\n",
      "Epoch 43 Batch 400 Loss 0.6269 Accuracy 0.0622\n",
      "Epoch 43 Batch 450 Loss 0.5717 Accuracy 0.0704\n",
      "Epoch 43 Batch 500 Loss 0.5224 Accuracy 0.0792\n",
      "Epoch 43 Batch 550 Loss 0.4808 Accuracy 0.0892\n",
      "Epoch 43 Batch 600 Loss 0.4443 Accuracy 0.1005\n",
      "Epoch 43 Batch 650 Loss 0.4125 Accuracy 0.1136\n",
      "Epoch 43 Batch 700 Loss 0.3855 Accuracy 0.1267\n",
      "Epoch 43 Batch 750 Loss 0.3630 Accuracy 0.1394\n",
      "Epoch 43 Batch 800 Loss 0.3437 Accuracy 0.1539\n",
      "Epoch 43 Batch 850 Loss 0.3259 Accuracy 0.1685\n",
      "Epoch 43 Batch 900 Loss 0.3096 Accuracy 0.1814\n",
      "Epoch 43 Batch 950 Loss 0.2946 Accuracy 0.1925\n",
      "Epoch 43 Batch 1000 Loss 0.2810 Accuracy 0.2031\n",
      "Epoch 43 Batch 1050 Loss 0.2686 Accuracy 0.2124\n",
      "Epoch 43 Batch 1100 Loss 0.2574 Accuracy 0.2207\n",
      "Epoch 43 Loss 0.2560 Accuracy 0.2215\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.8504 Accuracy 0.0164\n",
      "Epoch 44 Batch 50 Loss 1.0333 Accuracy 0.0206\n",
      "Epoch 44 Batch 100 Loss 1.0087 Accuracy 0.0254\n",
      "Epoch 44 Batch 150 Loss 0.9548 Accuracy 0.0302\n",
      "Epoch 44 Batch 200 Loss 0.8944 Accuracy 0.0349\n",
      "Epoch 44 Batch 250 Loss 0.8141 Accuracy 0.0410\n",
      "Epoch 44 Batch 300 Loss 0.7462 Accuracy 0.0471\n",
      "Epoch 44 Batch 350 Loss 0.6796 Accuracy 0.0540\n",
      "Epoch 44 Batch 400 Loss 0.6199 Accuracy 0.0617\n",
      "Epoch 44 Batch 450 Loss 0.5654 Accuracy 0.0704\n",
      "Epoch 44 Batch 500 Loss 0.5188 Accuracy 0.0792\n",
      "Epoch 44 Batch 550 Loss 0.4773 Accuracy 0.0888\n",
      "Epoch 44 Batch 600 Loss 0.4413 Accuracy 0.1001\n",
      "Epoch 44 Batch 650 Loss 0.4096 Accuracy 0.1132\n",
      "Epoch 44 Batch 700 Loss 0.3827 Accuracy 0.1266\n",
      "Epoch 44 Batch 750 Loss 0.3608 Accuracy 0.1391\n",
      "Epoch 44 Batch 800 Loss 0.3416 Accuracy 0.1529\n",
      "Epoch 44 Batch 850 Loss 0.3236 Accuracy 0.1675\n",
      "Epoch 44 Batch 900 Loss 0.3072 Accuracy 0.1802\n",
      "Epoch 44 Batch 950 Loss 0.2925 Accuracy 0.1917\n",
      "Epoch 44 Batch 1000 Loss 0.2792 Accuracy 0.2022\n",
      "Epoch 44 Batch 1050 Loss 0.2668 Accuracy 0.2119\n",
      "Epoch 44 Batch 1100 Loss 0.2557 Accuracy 0.2207\n",
      "Epoch 44 Loss 0.2544 Accuracy 0.2215\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.7267 Accuracy 0.0206\n",
      "Epoch 45 Batch 50 Loss 1.0825 Accuracy 0.0212\n",
      "Epoch 45 Batch 100 Loss 1.0152 Accuracy 0.0255\n",
      "Epoch 45 Batch 150 Loss 0.9267 Accuracy 0.0304\n",
      "Epoch 45 Batch 200 Loss 0.8622 Accuracy 0.0352\n",
      "Epoch 45 Batch 250 Loss 0.7918 Accuracy 0.0412\n",
      "Epoch 45 Batch 300 Loss 0.7168 Accuracy 0.0474\n",
      "Epoch 45 Batch 350 Loss 0.6542 Accuracy 0.0547\n",
      "Epoch 45 Batch 400 Loss 0.5958 Accuracy 0.0626\n",
      "Epoch 45 Batch 450 Loss 0.5471 Accuracy 0.0705\n",
      "Epoch 45 Batch 500 Loss 0.5023 Accuracy 0.0796\n",
      "Epoch 45 Batch 550 Loss 0.4631 Accuracy 0.0893\n",
      "Epoch 45 Batch 600 Loss 0.4279 Accuracy 0.1001\n",
      "Epoch 45 Batch 650 Loss 0.3974 Accuracy 0.1131\n",
      "Epoch 45 Batch 700 Loss 0.3716 Accuracy 0.1264\n",
      "Epoch 45 Batch 750 Loss 0.3503 Accuracy 0.1390\n",
      "Epoch 45 Batch 800 Loss 0.3315 Accuracy 0.1534\n",
      "Epoch 45 Batch 850 Loss 0.3146 Accuracy 0.1679\n",
      "Epoch 45 Batch 900 Loss 0.2987 Accuracy 0.1811\n",
      "Epoch 45 Batch 950 Loss 0.2844 Accuracy 0.1923\n",
      "Epoch 45 Batch 1000 Loss 0.2713 Accuracy 0.2027\n",
      "Epoch 45 Batch 1050 Loss 0.2593 Accuracy 0.2124\n",
      "Epoch 45 Batch 1100 Loss 0.2485 Accuracy 0.2206\n",
      "Epoch 45 Loss 0.2472 Accuracy 0.2215\n",
      "\n",
      "Epoch 46 Batch 0 Loss 1.0276 Accuracy 0.0247\n",
      "Epoch 46 Batch 50 Loss 1.1916 Accuracy 0.0201\n",
      "Epoch 46 Batch 100 Loss 1.0975 Accuracy 0.0256\n",
      "Epoch 46 Batch 150 Loss 0.9900 Accuracy 0.0300\n",
      "Epoch 46 Batch 200 Loss 0.9051 Accuracy 0.0350\n",
      "Epoch 46 Batch 250 Loss 0.8147 Accuracy 0.0413\n",
      "Epoch 46 Batch 300 Loss 0.7371 Accuracy 0.0478\n",
      "Epoch 46 Batch 350 Loss 0.6707 Accuracy 0.0547\n",
      "Epoch 46 Batch 400 Loss 0.6111 Accuracy 0.0621\n",
      "Epoch 46 Batch 450 Loss 0.5579 Accuracy 0.0702\n",
      "Epoch 46 Batch 500 Loss 0.5111 Accuracy 0.0792\n",
      "Epoch 46 Batch 550 Loss 0.4704 Accuracy 0.0893\n",
      "Epoch 46 Batch 600 Loss 0.4354 Accuracy 0.1005\n",
      "Epoch 46 Batch 650 Loss 0.4045 Accuracy 0.1136\n",
      "Epoch 46 Batch 700 Loss 0.3781 Accuracy 0.1268\n",
      "Epoch 46 Batch 750 Loss 0.3564 Accuracy 0.1394\n",
      "Epoch 46 Batch 800 Loss 0.3375 Accuracy 0.1536\n",
      "Epoch 46 Batch 850 Loss 0.3200 Accuracy 0.1680\n",
      "Epoch 46 Batch 900 Loss 0.3037 Accuracy 0.1808\n",
      "Epoch 46 Batch 950 Loss 0.2891 Accuracy 0.1924\n",
      "Epoch 46 Batch 1000 Loss 0.2757 Accuracy 0.2027\n",
      "Epoch 46 Batch 1050 Loss 0.2634 Accuracy 0.2124\n",
      "Epoch 46 Batch 1100 Loss 0.2522 Accuracy 0.2208\n",
      "Epoch 46 Loss 0.2509 Accuracy 0.2217\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.3836 Accuracy 0.0206\n",
      "Epoch 47 Batch 50 Loss 1.1454 Accuracy 0.0209\n",
      "Epoch 47 Batch 100 Loss 1.0349 Accuracy 0.0260\n",
      "Epoch 47 Batch 150 Loss 0.9690 Accuracy 0.0301\n",
      "Epoch 47 Batch 200 Loss 0.8838 Accuracy 0.0348\n",
      "Epoch 47 Batch 250 Loss 0.8016 Accuracy 0.0410\n",
      "Epoch 47 Batch 300 Loss 0.7300 Accuracy 0.0476\n",
      "Epoch 47 Batch 350 Loss 0.6636 Accuracy 0.0548\n",
      "Epoch 47 Batch 400 Loss 0.6030 Accuracy 0.0623\n",
      "Epoch 47 Batch 450 Loss 0.5502 Accuracy 0.0704\n",
      "Epoch 47 Batch 500 Loss 0.5049 Accuracy 0.0791\n",
      "Epoch 47 Batch 550 Loss 0.4658 Accuracy 0.0888\n",
      "Epoch 47 Batch 600 Loss 0.4305 Accuracy 0.1002\n",
      "Epoch 47 Batch 650 Loss 0.4001 Accuracy 0.1136\n",
      "Epoch 47 Batch 700 Loss 0.3737 Accuracy 0.1271\n",
      "Epoch 47 Batch 750 Loss 0.3520 Accuracy 0.1393\n",
      "Epoch 47 Batch 800 Loss 0.3332 Accuracy 0.1535\n",
      "Epoch 47 Batch 850 Loss 0.3158 Accuracy 0.1680\n",
      "Epoch 47 Batch 900 Loss 0.2998 Accuracy 0.1810\n",
      "Epoch 47 Batch 950 Loss 0.2855 Accuracy 0.1922\n",
      "Epoch 47 Batch 1000 Loss 0.2722 Accuracy 0.2028\n",
      "Epoch 47 Batch 1050 Loss 0.2600 Accuracy 0.2124\n",
      "Epoch 47 Batch 1100 Loss 0.2490 Accuracy 0.2209\n",
      "Epoch 47 Loss 0.2477 Accuracy 0.2216\n",
      "\n",
      "Epoch 48 Batch 0 Loss 1.4872 Accuracy 0.0164\n",
      "Epoch 48 Batch 50 Loss 1.0616 Accuracy 0.0218\n",
      "Epoch 48 Batch 100 Loss 0.9922 Accuracy 0.0261\n",
      "Epoch 48 Batch 150 Loss 0.9402 Accuracy 0.0304\n",
      "Epoch 48 Batch 200 Loss 0.8768 Accuracy 0.0352\n",
      "Epoch 48 Batch 250 Loss 0.8024 Accuracy 0.0408\n",
      "Epoch 48 Batch 300 Loss 0.7285 Accuracy 0.0472\n",
      "Epoch 48 Batch 350 Loss 0.6606 Accuracy 0.0545\n",
      "Epoch 48 Batch 400 Loss 0.6003 Accuracy 0.0619\n",
      "Epoch 48 Batch 450 Loss 0.5468 Accuracy 0.0702\n",
      "Epoch 48 Batch 500 Loss 0.5000 Accuracy 0.0796\n",
      "Epoch 48 Batch 550 Loss 0.4612 Accuracy 0.0894\n",
      "Epoch 48 Batch 600 Loss 0.4263 Accuracy 0.1004\n",
      "Epoch 48 Batch 650 Loss 0.3960 Accuracy 0.1135\n",
      "Epoch 48 Batch 700 Loss 0.3701 Accuracy 0.1268\n",
      "Epoch 48 Batch 750 Loss 0.3485 Accuracy 0.1397\n",
      "Epoch 48 Batch 800 Loss 0.3298 Accuracy 0.1543\n",
      "Epoch 48 Batch 850 Loss 0.3128 Accuracy 0.1690\n",
      "Epoch 48 Batch 900 Loss 0.2969 Accuracy 0.1811\n",
      "Epoch 48 Batch 950 Loss 0.2825 Accuracy 0.1927\n",
      "Epoch 48 Batch 1000 Loss 0.2693 Accuracy 0.2028\n",
      "Epoch 48 Batch 1050 Loss 0.2573 Accuracy 0.2121\n",
      "Epoch 48 Batch 1100 Loss 0.2464 Accuracy 0.2208\n",
      "Epoch 48 Loss 0.2451 Accuracy 0.2217\n",
      "\n",
      "Epoch 49 Batch 0 Loss 1.1435 Accuracy 0.0263\n",
      "Epoch 49 Batch 50 Loss 1.1968 Accuracy 0.0205\n",
      "Epoch 49 Batch 100 Loss 1.1386 Accuracy 0.0248\n",
      "Epoch 49 Batch 150 Loss 1.0133 Accuracy 0.0299\n",
      "Epoch 49 Batch 200 Loss 0.9096 Accuracy 0.0349\n",
      "Epoch 49 Batch 250 Loss 0.8207 Accuracy 0.0412\n",
      "Epoch 49 Batch 300 Loss 0.7472 Accuracy 0.0474\n",
      "Epoch 49 Batch 350 Loss 0.6802 Accuracy 0.0544\n",
      "Epoch 49 Batch 400 Loss 0.6202 Accuracy 0.0615\n",
      "Epoch 49 Batch 450 Loss 0.5656 Accuracy 0.0695\n",
      "Epoch 49 Batch 500 Loss 0.5174 Accuracy 0.0786\n",
      "Epoch 49 Batch 550 Loss 0.4753 Accuracy 0.0885\n",
      "Epoch 49 Batch 600 Loss 0.4387 Accuracy 0.0998\n",
      "Epoch 49 Batch 650 Loss 0.4072 Accuracy 0.1130\n",
      "Epoch 49 Batch 700 Loss 0.3806 Accuracy 0.1268\n",
      "Epoch 49 Batch 750 Loss 0.3579 Accuracy 0.1392\n",
      "Epoch 49 Batch 800 Loss 0.3388 Accuracy 0.1533\n",
      "Epoch 49 Batch 850 Loss 0.3208 Accuracy 0.1682\n",
      "Epoch 49 Batch 900 Loss 0.3046 Accuracy 0.1811\n",
      "Epoch 49 Batch 950 Loss 0.2897 Accuracy 0.1925\n",
      "Epoch 49 Batch 1000 Loss 0.2762 Accuracy 0.2030\n",
      "Epoch 49 Batch 1050 Loss 0.2640 Accuracy 0.2121\n",
      "Epoch 49 Batch 1100 Loss 0.2529 Accuracy 0.2209\n",
      "Epoch 49 Loss 0.2516 Accuracy 0.2217\n",
      "\n",
      "Epoch 50 Batch 0 Loss 1.4090 Accuracy 0.0156\n",
      "Epoch 50 Batch 50 Loss 0.9892 Accuracy 0.0213\n",
      "Epoch 50 Batch 100 Loss 0.9977 Accuracy 0.0258\n",
      "Epoch 50 Batch 150 Loss 0.9217 Accuracy 0.0310\n",
      "Epoch 50 Batch 200 Loss 0.8610 Accuracy 0.0355\n",
      "Epoch 50 Batch 250 Loss 0.7796 Accuracy 0.0416\n",
      "Epoch 50 Batch 300 Loss 0.7108 Accuracy 0.0478\n",
      "Epoch 50 Batch 350 Loss 0.6442 Accuracy 0.0550\n",
      "Epoch 50 Batch 400 Loss 0.5858 Accuracy 0.0625\n",
      "Epoch 50 Batch 450 Loss 0.5341 Accuracy 0.0706\n",
      "Epoch 50 Batch 500 Loss 0.4896 Accuracy 0.0796\n",
      "Epoch 50 Batch 550 Loss 0.4513 Accuracy 0.0892\n",
      "Epoch 50 Batch 600 Loss 0.4176 Accuracy 0.1004\n",
      "Epoch 50 Batch 650 Loss 0.3882 Accuracy 0.1129\n",
      "Epoch 50 Batch 700 Loss 0.3628 Accuracy 0.1267\n",
      "Epoch 50 Batch 750 Loss 0.3413 Accuracy 0.1392\n",
      "Epoch 50 Batch 800 Loss 0.3231 Accuracy 0.1539\n",
      "Epoch 50 Batch 850 Loss 0.3065 Accuracy 0.1683\n",
      "Epoch 50 Batch 900 Loss 0.2910 Accuracy 0.1811\n",
      "Epoch 50 Batch 950 Loss 0.2770 Accuracy 0.1923\n",
      "Epoch 50 Batch 1000 Loss 0.2641 Accuracy 0.2027\n",
      "Epoch 50 Batch 1050 Loss 0.2524 Accuracy 0.2123\n",
      "Epoch 50 Batch 1100 Loss 0.2419 Accuracy 0.2207\n",
      "Saved weights for epoch 50 at weights/transformer_weights_epoch_50.weights.h5\n",
      "Epoch 50 Loss 0.2407 Accuracy 0.2217\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.5697 Accuracy 0.0214\n",
      "Epoch 51 Batch 50 Loss 1.1550 Accuracy 0.0202\n",
      "Epoch 51 Batch 100 Loss 1.0582 Accuracy 0.0251\n",
      "Epoch 51 Batch 150 Loss 0.9645 Accuracy 0.0301\n",
      "Epoch 51 Batch 200 Loss 0.8991 Accuracy 0.0348\n",
      "Epoch 51 Batch 250 Loss 0.8115 Accuracy 0.0409\n",
      "Epoch 51 Batch 300 Loss 0.7392 Accuracy 0.0471\n",
      "Epoch 51 Batch 350 Loss 0.6703 Accuracy 0.0544\n",
      "Epoch 51 Batch 400 Loss 0.6094 Accuracy 0.0621\n",
      "Epoch 51 Batch 450 Loss 0.5562 Accuracy 0.0703\n",
      "Epoch 51 Batch 500 Loss 0.5091 Accuracy 0.0795\n",
      "Epoch 51 Batch 550 Loss 0.4691 Accuracy 0.0893\n",
      "Epoch 51 Batch 600 Loss 0.4330 Accuracy 0.1008\n",
      "Epoch 51 Batch 650 Loss 0.4024 Accuracy 0.1136\n",
      "Epoch 51 Batch 700 Loss 0.3758 Accuracy 0.1268\n",
      "Epoch 51 Batch 750 Loss 0.3537 Accuracy 0.1392\n",
      "Epoch 51 Batch 800 Loss 0.3347 Accuracy 0.1539\n",
      "Epoch 51 Batch 850 Loss 0.3172 Accuracy 0.1686\n",
      "Epoch 51 Batch 900 Loss 0.3009 Accuracy 0.1812\n",
      "Epoch 51 Batch 950 Loss 0.2862 Accuracy 0.1923\n",
      "Epoch 51 Batch 1000 Loss 0.2729 Accuracy 0.2032\n",
      "Epoch 51 Batch 1050 Loss 0.2610 Accuracy 0.2126\n",
      "Epoch 51 Batch 1100 Loss 0.2500 Accuracy 0.2207\n",
      "Epoch 51 Loss 0.2487 Accuracy 0.2217\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.6202 Accuracy 0.0164\n",
      "Epoch 52 Batch 50 Loss 1.1041 Accuracy 0.0204\n",
      "Epoch 52 Batch 100 Loss 1.0118 Accuracy 0.0258\n",
      "Epoch 52 Batch 150 Loss 0.9380 Accuracy 0.0306\n",
      "Epoch 52 Batch 200 Loss 0.8636 Accuracy 0.0353\n",
      "Epoch 52 Batch 250 Loss 0.7854 Accuracy 0.0414\n",
      "Epoch 52 Batch 300 Loss 0.7168 Accuracy 0.0473\n",
      "Epoch 52 Batch 350 Loss 0.6517 Accuracy 0.0543\n",
      "Epoch 52 Batch 400 Loss 0.5939 Accuracy 0.0617\n",
      "Epoch 52 Batch 450 Loss 0.5419 Accuracy 0.0702\n",
      "Epoch 52 Batch 500 Loss 0.4961 Accuracy 0.0796\n",
      "Epoch 52 Batch 550 Loss 0.4569 Accuracy 0.0895\n",
      "Epoch 52 Batch 600 Loss 0.4230 Accuracy 0.1005\n",
      "Epoch 52 Batch 650 Loss 0.3927 Accuracy 0.1135\n",
      "Epoch 52 Batch 700 Loss 0.3671 Accuracy 0.1266\n",
      "Epoch 52 Batch 750 Loss 0.3454 Accuracy 0.1395\n",
      "Epoch 52 Batch 800 Loss 0.3272 Accuracy 0.1543\n",
      "Epoch 52 Batch 850 Loss 0.3101 Accuracy 0.1685\n",
      "Epoch 52 Batch 900 Loss 0.2945 Accuracy 0.1814\n",
      "Epoch 52 Batch 950 Loss 0.2801 Accuracy 0.1926\n",
      "Epoch 52 Batch 1000 Loss 0.2673 Accuracy 0.2028\n",
      "Epoch 52 Batch 1050 Loss 0.2555 Accuracy 0.2122\n",
      "Epoch 52 Batch 1100 Loss 0.2447 Accuracy 0.2208\n",
      "Epoch 52 Loss 0.2434 Accuracy 0.2217\n",
      "\n",
      "Epoch 53 Batch 0 Loss 1.6311 Accuracy 0.0148\n",
      "Epoch 53 Batch 50 Loss 1.0962 Accuracy 0.0206\n",
      "Epoch 53 Batch 100 Loss 1.0696 Accuracy 0.0253\n",
      "Epoch 53 Batch 150 Loss 0.9657 Accuracy 0.0304\n",
      "Epoch 53 Batch 200 Loss 0.8776 Accuracy 0.0353\n",
      "Epoch 53 Batch 250 Loss 0.7927 Accuracy 0.0411\n",
      "Epoch 53 Batch 300 Loss 0.7239 Accuracy 0.0473\n",
      "Epoch 53 Batch 350 Loss 0.6583 Accuracy 0.0544\n",
      "Epoch 53 Batch 400 Loss 0.5951 Accuracy 0.0623\n",
      "Epoch 53 Batch 450 Loss 0.5436 Accuracy 0.0706\n",
      "Epoch 53 Batch 500 Loss 0.4987 Accuracy 0.0794\n",
      "Epoch 53 Batch 550 Loss 0.4596 Accuracy 0.0892\n",
      "Epoch 53 Batch 600 Loss 0.4246 Accuracy 0.1006\n",
      "Epoch 53 Batch 650 Loss 0.3944 Accuracy 0.1137\n",
      "Epoch 53 Batch 700 Loss 0.3683 Accuracy 0.1269\n",
      "Epoch 53 Batch 750 Loss 0.3465 Accuracy 0.1395\n",
      "Epoch 53 Batch 800 Loss 0.3273 Accuracy 0.1539\n",
      "Epoch 53 Batch 850 Loss 0.3101 Accuracy 0.1686\n",
      "Epoch 53 Batch 900 Loss 0.2944 Accuracy 0.1815\n",
      "Epoch 53 Batch 950 Loss 0.2800 Accuracy 0.1932\n",
      "Epoch 53 Batch 1000 Loss 0.2671 Accuracy 0.2030\n",
      "Epoch 53 Batch 1050 Loss 0.2554 Accuracy 0.2125\n",
      "Epoch 53 Batch 1100 Loss 0.2447 Accuracy 0.2208\n",
      "Epoch 53 Loss 0.2434 Accuracy 0.2218\n",
      "\n",
      "Epoch 54 Batch 0 Loss 1.3688 Accuracy 0.0173\n",
      "Epoch 54 Batch 50 Loss 1.1183 Accuracy 0.0204\n",
      "Epoch 54 Batch 100 Loss 1.0507 Accuracy 0.0252\n",
      "Epoch 54 Batch 150 Loss 0.9540 Accuracy 0.0304\n",
      "Epoch 54 Batch 200 Loss 0.8672 Accuracy 0.0354\n",
      "Epoch 54 Batch 250 Loss 0.7955 Accuracy 0.0413\n",
      "Epoch 54 Batch 300 Loss 0.7202 Accuracy 0.0477\n",
      "Epoch 54 Batch 350 Loss 0.6569 Accuracy 0.0545\n",
      "Epoch 54 Batch 400 Loss 0.5978 Accuracy 0.0621\n",
      "Epoch 54 Batch 450 Loss 0.5453 Accuracy 0.0701\n",
      "Epoch 54 Batch 500 Loss 0.4990 Accuracy 0.0792\n",
      "Epoch 54 Batch 550 Loss 0.4599 Accuracy 0.0890\n",
      "Epoch 54 Batch 600 Loss 0.4251 Accuracy 0.1000\n",
      "Epoch 54 Batch 650 Loss 0.3946 Accuracy 0.1134\n",
      "Epoch 54 Batch 700 Loss 0.3687 Accuracy 0.1267\n",
      "Epoch 54 Batch 750 Loss 0.3472 Accuracy 0.1392\n",
      "Epoch 54 Batch 800 Loss 0.3284 Accuracy 0.1540\n",
      "Epoch 54 Batch 850 Loss 0.3112 Accuracy 0.1685\n",
      "Epoch 54 Batch 900 Loss 0.2954 Accuracy 0.1812\n",
      "Epoch 54 Batch 950 Loss 0.2813 Accuracy 0.1925\n",
      "Epoch 54 Batch 1000 Loss 0.2682 Accuracy 0.2028\n",
      "Epoch 54 Batch 1050 Loss 0.2562 Accuracy 0.2124\n",
      "Epoch 54 Batch 1100 Loss 0.2454 Accuracy 0.2209\n",
      "Epoch 54 Loss 0.2442 Accuracy 0.2217\n",
      "\n",
      "Epoch 55 Batch 0 Loss 1.2935 Accuracy 0.0148\n",
      "Epoch 55 Batch 50 Loss 1.1056 Accuracy 0.0209\n",
      "Epoch 55 Batch 100 Loss 1.0326 Accuracy 0.0257\n",
      "Epoch 55 Batch 150 Loss 0.9534 Accuracy 0.0305\n",
      "Epoch 55 Batch 200 Loss 0.8738 Accuracy 0.0353\n",
      "Epoch 55 Batch 250 Loss 0.7983 Accuracy 0.0413\n",
      "Epoch 55 Batch 300 Loss 0.7273 Accuracy 0.0471\n",
      "Epoch 55 Batch 350 Loss 0.6577 Accuracy 0.0540\n",
      "Epoch 55 Batch 400 Loss 0.5986 Accuracy 0.0616\n",
      "Epoch 55 Batch 450 Loss 0.5442 Accuracy 0.0699\n",
      "Epoch 55 Batch 500 Loss 0.4985 Accuracy 0.0789\n",
      "Epoch 55 Batch 550 Loss 0.4589 Accuracy 0.0890\n",
      "Epoch 55 Batch 600 Loss 0.4240 Accuracy 0.1002\n",
      "Epoch 55 Batch 650 Loss 0.3935 Accuracy 0.1131\n",
      "Epoch 55 Batch 700 Loss 0.3675 Accuracy 0.1265\n",
      "Epoch 55 Batch 750 Loss 0.3459 Accuracy 0.1388\n",
      "Epoch 55 Batch 800 Loss 0.3268 Accuracy 0.1532\n",
      "Epoch 55 Batch 850 Loss 0.3098 Accuracy 0.1672\n",
      "Epoch 55 Batch 900 Loss 0.2941 Accuracy 0.1804\n",
      "Epoch 55 Batch 950 Loss 0.2798 Accuracy 0.1921\n",
      "Epoch 55 Batch 1000 Loss 0.2668 Accuracy 0.2028\n",
      "Epoch 55 Batch 1050 Loss 0.2549 Accuracy 0.2121\n",
      "Epoch 55 Batch 1100 Loss 0.2438 Accuracy 0.2208\n",
      "Epoch 55 Loss 0.2426 Accuracy 0.2218\n",
      "\n",
      "Epoch 56 Batch 0 Loss 1.1620 Accuracy 0.0222\n",
      "Epoch 56 Batch 50 Loss 1.0238 Accuracy 0.0217\n",
      "Epoch 56 Batch 100 Loss 0.9906 Accuracy 0.0261\n",
      "Epoch 56 Batch 150 Loss 0.9127 Accuracy 0.0310\n",
      "Epoch 56 Batch 200 Loss 0.8431 Accuracy 0.0358\n",
      "Epoch 56 Batch 250 Loss 0.7749 Accuracy 0.0414\n",
      "Epoch 56 Batch 300 Loss 0.7121 Accuracy 0.0474\n",
      "Epoch 56 Batch 350 Loss 0.6502 Accuracy 0.0541\n",
      "Epoch 56 Batch 400 Loss 0.5904 Accuracy 0.0618\n",
      "Epoch 56 Batch 450 Loss 0.5382 Accuracy 0.0700\n",
      "Epoch 56 Batch 500 Loss 0.4924 Accuracy 0.0792\n",
      "Epoch 56 Batch 550 Loss 0.4531 Accuracy 0.0891\n",
      "Epoch 56 Batch 600 Loss 0.4186 Accuracy 0.1004\n",
      "Epoch 56 Batch 650 Loss 0.3889 Accuracy 0.1137\n",
      "Epoch 56 Batch 700 Loss 0.3632 Accuracy 0.1273\n",
      "Epoch 56 Batch 750 Loss 0.3421 Accuracy 0.1400\n",
      "Epoch 56 Batch 800 Loss 0.3235 Accuracy 0.1543\n",
      "Epoch 56 Batch 850 Loss 0.3065 Accuracy 0.1688\n",
      "Epoch 56 Batch 900 Loss 0.2908 Accuracy 0.1815\n",
      "Epoch 56 Batch 950 Loss 0.2769 Accuracy 0.1925\n",
      "Epoch 56 Batch 1000 Loss 0.2639 Accuracy 0.2031\n",
      "Epoch 56 Batch 1050 Loss 0.2522 Accuracy 0.2125\n",
      "Epoch 56 Batch 1100 Loss 0.2415 Accuracy 0.2210\n",
      "Epoch 56 Loss 0.2403 Accuracy 0.2218\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.5202 Accuracy 0.0189\n",
      "Epoch 57 Batch 50 Loss 1.0586 Accuracy 0.0210\n",
      "Epoch 57 Batch 100 Loss 0.9910 Accuracy 0.0262\n",
      "Epoch 57 Batch 150 Loss 0.9121 Accuracy 0.0306\n",
      "Epoch 57 Batch 200 Loss 0.8358 Accuracy 0.0358\n",
      "Epoch 57 Batch 250 Loss 0.7675 Accuracy 0.0413\n",
      "Epoch 57 Batch 300 Loss 0.6982 Accuracy 0.0476\n",
      "Epoch 57 Batch 350 Loss 0.6370 Accuracy 0.0546\n",
      "Epoch 57 Batch 400 Loss 0.5788 Accuracy 0.0622\n",
      "Epoch 57 Batch 450 Loss 0.5275 Accuracy 0.0705\n",
      "Epoch 57 Batch 500 Loss 0.4830 Accuracy 0.0794\n",
      "Epoch 57 Batch 550 Loss 0.4448 Accuracy 0.0893\n",
      "Epoch 57 Batch 600 Loss 0.4112 Accuracy 0.1005\n",
      "Epoch 57 Batch 650 Loss 0.3821 Accuracy 0.1134\n",
      "Epoch 57 Batch 700 Loss 0.3571 Accuracy 0.1264\n",
      "Epoch 57 Batch 750 Loss 0.3364 Accuracy 0.1387\n",
      "Epoch 57 Batch 800 Loss 0.3181 Accuracy 0.1535\n",
      "Epoch 57 Batch 850 Loss 0.3013 Accuracy 0.1681\n",
      "Epoch 57 Batch 900 Loss 0.2859 Accuracy 0.1811\n",
      "Epoch 57 Batch 950 Loss 0.2720 Accuracy 0.1923\n",
      "Epoch 57 Batch 1000 Loss 0.2593 Accuracy 0.2028\n",
      "Epoch 57 Batch 1050 Loss 0.2478 Accuracy 0.2123\n",
      "Epoch 57 Batch 1100 Loss 0.2372 Accuracy 0.2210\n",
      "Epoch 57 Loss 0.2360 Accuracy 0.2218\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.7186 Accuracy 0.0189\n",
      "Epoch 58 Batch 50 Loss 1.0667 Accuracy 0.0202\n",
      "Epoch 58 Batch 100 Loss 0.9747 Accuracy 0.0254\n",
      "Epoch 58 Batch 150 Loss 0.8768 Accuracy 0.0307\n",
      "Epoch 58 Batch 200 Loss 0.8152 Accuracy 0.0356\n",
      "Epoch 58 Batch 250 Loss 0.7426 Accuracy 0.0415\n",
      "Epoch 58 Batch 300 Loss 0.6827 Accuracy 0.0476\n",
      "Epoch 58 Batch 350 Loss 0.6256 Accuracy 0.0544\n",
      "Epoch 58 Batch 400 Loss 0.5697 Accuracy 0.0621\n",
      "Epoch 58 Batch 450 Loss 0.5200 Accuracy 0.0705\n",
      "Epoch 58 Batch 500 Loss 0.4765 Accuracy 0.0794\n",
      "Epoch 58 Batch 550 Loss 0.4391 Accuracy 0.0894\n",
      "Epoch 58 Batch 600 Loss 0.4055 Accuracy 0.1007\n",
      "Epoch 58 Batch 650 Loss 0.3767 Accuracy 0.1139\n",
      "Epoch 58 Batch 700 Loss 0.3523 Accuracy 0.1270\n",
      "Epoch 58 Batch 750 Loss 0.3315 Accuracy 0.1396\n",
      "Epoch 58 Batch 800 Loss 0.3135 Accuracy 0.1544\n",
      "Epoch 58 Batch 850 Loss 0.2971 Accuracy 0.1685\n",
      "Epoch 58 Batch 900 Loss 0.2825 Accuracy 0.1814\n",
      "Epoch 58 Batch 950 Loss 0.2687 Accuracy 0.1925\n",
      "Epoch 58 Batch 1000 Loss 0.2562 Accuracy 0.2034\n",
      "Epoch 58 Batch 1050 Loss 0.2451 Accuracy 0.2123\n",
      "Epoch 58 Batch 1100 Loss 0.2348 Accuracy 0.2209\n",
      "Epoch 58 Loss 0.2337 Accuracy 0.2218\n",
      "\n",
      "Epoch 59 Batch 0 Loss 1.2375 Accuracy 0.0173\n",
      "Epoch 59 Batch 50 Loss 1.0494 Accuracy 0.0211\n",
      "Epoch 59 Batch 100 Loss 1.0025 Accuracy 0.0258\n",
      "Epoch 59 Batch 150 Loss 0.9187 Accuracy 0.0308\n",
      "Epoch 59 Batch 200 Loss 0.8449 Accuracy 0.0358\n",
      "Epoch 59 Batch 250 Loss 0.7717 Accuracy 0.0415\n",
      "Epoch 59 Batch 300 Loss 0.7000 Accuracy 0.0479\n",
      "Epoch 59 Batch 350 Loss 0.6357 Accuracy 0.0549\n",
      "Epoch 59 Batch 400 Loss 0.5793 Accuracy 0.0625\n",
      "Epoch 59 Batch 450 Loss 0.5297 Accuracy 0.0707\n",
      "Epoch 59 Batch 500 Loss 0.4843 Accuracy 0.0795\n",
      "Epoch 59 Batch 550 Loss 0.4458 Accuracy 0.0892\n",
      "Epoch 59 Batch 600 Loss 0.4119 Accuracy 0.1007\n",
      "Epoch 59 Batch 650 Loss 0.3827 Accuracy 0.1139\n",
      "Epoch 59 Batch 700 Loss 0.3577 Accuracy 0.1272\n",
      "Epoch 59 Batch 750 Loss 0.3365 Accuracy 0.1398\n",
      "Epoch 59 Batch 800 Loss 0.3184 Accuracy 0.1539\n",
      "Epoch 59 Batch 850 Loss 0.3019 Accuracy 0.1682\n",
      "Epoch 59 Batch 900 Loss 0.2864 Accuracy 0.1815\n",
      "Epoch 59 Batch 950 Loss 0.2724 Accuracy 0.1928\n",
      "Epoch 59 Batch 1000 Loss 0.2596 Accuracy 0.2032\n",
      "Epoch 59 Batch 1050 Loss 0.2481 Accuracy 0.2126\n",
      "Epoch 59 Batch 1100 Loss 0.2377 Accuracy 0.2209\n",
      "Epoch 59 Loss 0.2364 Accuracy 0.2218\n",
      "\n",
      "Epoch 60 Batch 0 Loss 1.0933 Accuracy 0.0173\n",
      "Epoch 60 Batch 50 Loss 0.9879 Accuracy 0.0209\n",
      "Epoch 60 Batch 100 Loss 0.9902 Accuracy 0.0262\n",
      "Epoch 60 Batch 150 Loss 0.9061 Accuracy 0.0314\n",
      "Epoch 60 Batch 200 Loss 0.8455 Accuracy 0.0357\n",
      "Epoch 60 Batch 250 Loss 0.7636 Accuracy 0.0414\n",
      "Epoch 60 Batch 300 Loss 0.7016 Accuracy 0.0474\n",
      "Epoch 60 Batch 350 Loss 0.6355 Accuracy 0.0547\n",
      "Epoch 60 Batch 400 Loss 0.5782 Accuracy 0.0622\n",
      "Epoch 60 Batch 450 Loss 0.5281 Accuracy 0.0704\n",
      "Epoch 60 Batch 500 Loss 0.4829 Accuracy 0.0792\n",
      "Epoch 60 Batch 550 Loss 0.4441 Accuracy 0.0890\n",
      "Epoch 60 Batch 600 Loss 0.4099 Accuracy 0.1000\n",
      "Epoch 60 Batch 650 Loss 0.3806 Accuracy 0.1128\n",
      "Epoch 60 Batch 700 Loss 0.3557 Accuracy 0.1263\n",
      "Epoch 60 Batch 750 Loss 0.3346 Accuracy 0.1393\n",
      "Epoch 60 Batch 800 Loss 0.3168 Accuracy 0.1537\n",
      "Epoch 60 Batch 850 Loss 0.3001 Accuracy 0.1680\n",
      "Epoch 60 Batch 900 Loss 0.2847 Accuracy 0.1812\n",
      "Epoch 60 Batch 950 Loss 0.2707 Accuracy 0.1929\n",
      "Epoch 60 Batch 1000 Loss 0.2581 Accuracy 0.2034\n",
      "Epoch 60 Batch 1050 Loss 0.2466 Accuracy 0.2125\n",
      "Epoch 60 Batch 1100 Loss 0.2362 Accuracy 0.2211\n",
      "Epoch 60 Loss 0.2350 Accuracy 0.2219\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.8292 Accuracy 0.0206\n",
      "Epoch 61 Batch 50 Loss 1.0021 Accuracy 0.0210\n",
      "Epoch 61 Batch 100 Loss 0.9227 Accuracy 0.0260\n",
      "Epoch 61 Batch 150 Loss 0.8784 Accuracy 0.0307\n",
      "Epoch 61 Batch 200 Loss 0.8094 Accuracy 0.0358\n",
      "Epoch 61 Batch 250 Loss 0.7383 Accuracy 0.0419\n",
      "Epoch 61 Batch 300 Loss 0.6761 Accuracy 0.0480\n",
      "Epoch 61 Batch 350 Loss 0.6165 Accuracy 0.0550\n",
      "Epoch 61 Batch 400 Loss 0.5600 Accuracy 0.0626\n",
      "Epoch 61 Batch 450 Loss 0.5124 Accuracy 0.0709\n",
      "Epoch 61 Batch 500 Loss 0.4702 Accuracy 0.0794\n",
      "Epoch 61 Batch 550 Loss 0.4322 Accuracy 0.0893\n",
      "Epoch 61 Batch 600 Loss 0.3995 Accuracy 0.1006\n",
      "Epoch 61 Batch 650 Loss 0.3710 Accuracy 0.1138\n",
      "Epoch 61 Batch 700 Loss 0.3467 Accuracy 0.1274\n",
      "Epoch 61 Batch 750 Loss 0.3262 Accuracy 0.1398\n",
      "Epoch 61 Batch 800 Loss 0.3085 Accuracy 0.1542\n",
      "Epoch 61 Batch 850 Loss 0.2924 Accuracy 0.1685\n",
      "Epoch 61 Batch 900 Loss 0.2775 Accuracy 0.1811\n",
      "Epoch 61 Batch 950 Loss 0.2640 Accuracy 0.1930\n",
      "Epoch 61 Batch 1000 Loss 0.2517 Accuracy 0.2032\n",
      "Epoch 61 Batch 1050 Loss 0.2406 Accuracy 0.2122\n",
      "Epoch 61 Batch 1100 Loss 0.2307 Accuracy 0.2210\n",
      "Epoch 61 Loss 0.2295 Accuracy 0.2219\n",
      "\n",
      "Epoch 62 Batch 0 Loss 1.3692 Accuracy 0.0156\n",
      "Epoch 62 Batch 50 Loss 1.0865 Accuracy 0.0207\n",
      "Epoch 62 Batch 100 Loss 0.9867 Accuracy 0.0258\n",
      "Epoch 62 Batch 150 Loss 0.9115 Accuracy 0.0308\n",
      "Epoch 62 Batch 200 Loss 0.8345 Accuracy 0.0356\n",
      "Epoch 62 Batch 250 Loss 0.7592 Accuracy 0.0416\n",
      "Epoch 62 Batch 300 Loss 0.6928 Accuracy 0.0476\n",
      "Epoch 62 Batch 350 Loss 0.6349 Accuracy 0.0542\n",
      "Epoch 62 Batch 400 Loss 0.5766 Accuracy 0.0620\n",
      "Epoch 62 Batch 450 Loss 0.5262 Accuracy 0.0697\n",
      "Epoch 62 Batch 500 Loss 0.4828 Accuracy 0.0786\n",
      "Epoch 62 Batch 550 Loss 0.4437 Accuracy 0.0888\n",
      "Epoch 62 Batch 600 Loss 0.4099 Accuracy 0.1002\n",
      "Epoch 62 Batch 650 Loss 0.3807 Accuracy 0.1129\n",
      "Epoch 62 Batch 700 Loss 0.3559 Accuracy 0.1263\n",
      "Epoch 62 Batch 750 Loss 0.3348 Accuracy 0.1388\n",
      "Epoch 62 Batch 800 Loss 0.3166 Accuracy 0.1536\n",
      "Epoch 62 Batch 850 Loss 0.3000 Accuracy 0.1682\n",
      "Epoch 62 Batch 900 Loss 0.2845 Accuracy 0.1809\n",
      "Epoch 62 Batch 950 Loss 0.2705 Accuracy 0.1923\n",
      "Epoch 62 Batch 1000 Loss 0.2579 Accuracy 0.2030\n",
      "Epoch 62 Batch 1050 Loss 0.2465 Accuracy 0.2120\n",
      "Epoch 62 Batch 1100 Loss 0.2361 Accuracy 0.2210\n",
      "Epoch 62 Loss 0.2348 Accuracy 0.2219\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.8905 Accuracy 0.0173\n",
      "Epoch 63 Batch 50 Loss 1.0477 Accuracy 0.0211\n",
      "Epoch 63 Batch 100 Loss 0.9664 Accuracy 0.0261\n",
      "Epoch 63 Batch 150 Loss 0.8810 Accuracy 0.0309\n",
      "Epoch 63 Batch 200 Loss 0.8109 Accuracy 0.0357\n",
      "Epoch 63 Batch 250 Loss 0.7409 Accuracy 0.0415\n",
      "Epoch 63 Batch 300 Loss 0.6758 Accuracy 0.0479\n",
      "Epoch 63 Batch 350 Loss 0.6170 Accuracy 0.0548\n",
      "Epoch 63 Batch 400 Loss 0.5635 Accuracy 0.0623\n",
      "Epoch 63 Batch 450 Loss 0.5145 Accuracy 0.0706\n",
      "Epoch 63 Batch 500 Loss 0.4721 Accuracy 0.0794\n",
      "Epoch 63 Batch 550 Loss 0.4344 Accuracy 0.0894\n",
      "Epoch 63 Batch 600 Loss 0.4014 Accuracy 0.1006\n",
      "Epoch 63 Batch 650 Loss 0.3726 Accuracy 0.1138\n",
      "Epoch 63 Batch 700 Loss 0.3480 Accuracy 0.1272\n",
      "Epoch 63 Batch 750 Loss 0.3278 Accuracy 0.1399\n",
      "Epoch 63 Batch 800 Loss 0.3099 Accuracy 0.1541\n",
      "Epoch 63 Batch 850 Loss 0.2937 Accuracy 0.1689\n",
      "Epoch 63 Batch 900 Loss 0.2788 Accuracy 0.1820\n",
      "Epoch 63 Batch 950 Loss 0.2652 Accuracy 0.1933\n",
      "Epoch 63 Batch 1000 Loss 0.2530 Accuracy 0.2035\n",
      "Epoch 63 Batch 1050 Loss 0.2419 Accuracy 0.2126\n",
      "Epoch 63 Batch 1100 Loss 0.2315 Accuracy 0.2211\n",
      "Epoch 63 Loss 0.2304 Accuracy 0.2219\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.6854 Accuracy 0.0181\n",
      "Epoch 64 Batch 50 Loss 1.0168 Accuracy 0.0208\n",
      "Epoch 64 Batch 100 Loss 0.9623 Accuracy 0.0256\n",
      "Epoch 64 Batch 150 Loss 0.8770 Accuracy 0.0308\n",
      "Epoch 64 Batch 200 Loss 0.8122 Accuracy 0.0355\n",
      "Epoch 64 Batch 250 Loss 0.7451 Accuracy 0.0413\n",
      "Epoch 64 Batch 300 Loss 0.6752 Accuracy 0.0479\n",
      "Epoch 64 Batch 350 Loss 0.6152 Accuracy 0.0550\n",
      "Epoch 64 Batch 400 Loss 0.5600 Accuracy 0.0623\n",
      "Epoch 64 Batch 450 Loss 0.5119 Accuracy 0.0707\n",
      "Epoch 64 Batch 500 Loss 0.4690 Accuracy 0.0796\n",
      "Epoch 64 Batch 550 Loss 0.4313 Accuracy 0.0896\n",
      "Epoch 64 Batch 600 Loss 0.3987 Accuracy 0.1005\n",
      "Epoch 64 Batch 650 Loss 0.3700 Accuracy 0.1135\n",
      "Epoch 64 Batch 700 Loss 0.3457 Accuracy 0.1271\n",
      "Epoch 64 Batch 750 Loss 0.3251 Accuracy 0.1398\n",
      "Epoch 64 Batch 800 Loss 0.3077 Accuracy 0.1540\n",
      "Epoch 64 Batch 850 Loss 0.2914 Accuracy 0.1681\n",
      "Epoch 64 Batch 900 Loss 0.2764 Accuracy 0.1810\n",
      "Epoch 64 Batch 950 Loss 0.2631 Accuracy 0.1927\n",
      "Epoch 64 Batch 1000 Loss 0.2509 Accuracy 0.2031\n",
      "Epoch 64 Batch 1050 Loss 0.2398 Accuracy 0.2125\n",
      "Epoch 64 Batch 1100 Loss 0.2297 Accuracy 0.2211\n",
      "Epoch 64 Loss 0.2286 Accuracy 0.2220\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.3306 Accuracy 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 05:09:02.025412: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 50 Loss 1.0034 Accuracy 0.0209\n",
      "Epoch 65 Batch 100 Loss 0.9425 Accuracy 0.0257\n",
      "Epoch 65 Batch 150 Loss 0.8787 Accuracy 0.0308\n",
      "Epoch 65 Batch 200 Loss 0.8165 Accuracy 0.0358\n",
      "Epoch 65 Batch 250 Loss 0.7485 Accuracy 0.0414\n",
      "Epoch 65 Batch 300 Loss 0.6792 Accuracy 0.0478\n",
      "Epoch 65 Batch 350 Loss 0.6191 Accuracy 0.0544\n",
      "Epoch 65 Batch 400 Loss 0.5642 Accuracy 0.0621\n",
      "Epoch 65 Batch 450 Loss 0.5140 Accuracy 0.0705\n",
      "Epoch 65 Batch 500 Loss 0.4699 Accuracy 0.0797\n",
      "Epoch 65 Batch 550 Loss 0.4321 Accuracy 0.0896\n",
      "Epoch 65 Batch 600 Loss 0.3994 Accuracy 0.1008\n",
      "Epoch 65 Batch 650 Loss 0.3709 Accuracy 0.1136\n",
      "Epoch 65 Batch 700 Loss 0.3465 Accuracy 0.1271\n",
      "Epoch 65 Batch 750 Loss 0.3263 Accuracy 0.1397\n",
      "Epoch 65 Batch 800 Loss 0.3085 Accuracy 0.1545\n",
      "Epoch 65 Batch 850 Loss 0.2924 Accuracy 0.1690\n",
      "Epoch 65 Batch 900 Loss 0.2776 Accuracy 0.1815\n",
      "Epoch 65 Batch 950 Loss 0.2640 Accuracy 0.1930\n",
      "Epoch 65 Batch 1000 Loss 0.2517 Accuracy 0.2034\n",
      "Epoch 65 Batch 1050 Loss 0.2406 Accuracy 0.2127\n",
      "Epoch 65 Batch 1100 Loss 0.2304 Accuracy 0.2211\n",
      "Epoch 65 Loss 0.2293 Accuracy 0.2220\n",
      "\n",
      "Epoch 66 Batch 0 Loss 1.0290 Accuracy 0.0132\n",
      "Epoch 66 Batch 50 Loss 1.0391 Accuracy 0.0205\n",
      "Epoch 66 Batch 100 Loss 0.9640 Accuracy 0.0255\n",
      "Epoch 66 Batch 150 Loss 0.8925 Accuracy 0.0303\n",
      "Epoch 66 Batch 200 Loss 0.8148 Accuracy 0.0355\n",
      "Epoch 66 Batch 250 Loss 0.7471 Accuracy 0.0413\n",
      "Epoch 66 Batch 300 Loss 0.6769 Accuracy 0.0477\n",
      "Epoch 66 Batch 350 Loss 0.6191 Accuracy 0.0545\n",
      "Epoch 66 Batch 400 Loss 0.5651 Accuracy 0.0619\n",
      "Epoch 66 Batch 450 Loss 0.5158 Accuracy 0.0704\n",
      "Epoch 66 Batch 500 Loss 0.4728 Accuracy 0.0792\n",
      "Epoch 66 Batch 550 Loss 0.4353 Accuracy 0.0889\n",
      "Epoch 66 Batch 600 Loss 0.4025 Accuracy 0.1002\n",
      "Epoch 66 Batch 650 Loss 0.3739 Accuracy 0.1133\n",
      "Epoch 66 Batch 700 Loss 0.3491 Accuracy 0.1268\n",
      "Epoch 66 Batch 750 Loss 0.3288 Accuracy 0.1391\n",
      "Epoch 66 Batch 800 Loss 0.3107 Accuracy 0.1538\n",
      "Epoch 66 Batch 850 Loss 0.2945 Accuracy 0.1683\n",
      "Epoch 66 Batch 900 Loss 0.2795 Accuracy 0.1809\n",
      "Epoch 66 Batch 950 Loss 0.2657 Accuracy 0.1925\n",
      "Epoch 66 Batch 1000 Loss 0.2532 Accuracy 0.2032\n",
      "Epoch 66 Batch 1050 Loss 0.2420 Accuracy 0.2127\n",
      "Epoch 66 Batch 1100 Loss 0.2318 Accuracy 0.2210\n",
      "Epoch 66 Loss 0.2306 Accuracy 0.2219\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.8752 Accuracy 0.0189\n",
      "Epoch 67 Batch 50 Loss 1.0382 Accuracy 0.0204\n",
      "Epoch 67 Batch 100 Loss 0.9824 Accuracy 0.0253\n",
      "Epoch 67 Batch 150 Loss 0.8888 Accuracy 0.0306\n",
      "Epoch 67 Batch 200 Loss 0.8085 Accuracy 0.0357\n",
      "Epoch 67 Batch 250 Loss 0.7374 Accuracy 0.0416\n",
      "Epoch 67 Batch 300 Loss 0.6750 Accuracy 0.0477\n",
      "Epoch 67 Batch 350 Loss 0.6138 Accuracy 0.0549\n",
      "Epoch 67 Batch 400 Loss 0.5581 Accuracy 0.0624\n",
      "Epoch 67 Batch 450 Loss 0.5100 Accuracy 0.0704\n",
      "Epoch 67 Batch 500 Loss 0.4669 Accuracy 0.0795\n",
      "Epoch 67 Batch 550 Loss 0.4303 Accuracy 0.0893\n",
      "Epoch 67 Batch 600 Loss 0.3980 Accuracy 0.1003\n",
      "Epoch 67 Batch 650 Loss 0.3694 Accuracy 0.1132\n",
      "Epoch 67 Batch 700 Loss 0.3449 Accuracy 0.1263\n",
      "Epoch 67 Batch 750 Loss 0.3243 Accuracy 0.1388\n",
      "Epoch 67 Batch 800 Loss 0.3064 Accuracy 0.1529\n",
      "Epoch 67 Batch 850 Loss 0.2902 Accuracy 0.1678\n",
      "Epoch 67 Batch 900 Loss 0.2754 Accuracy 0.1814\n",
      "Epoch 67 Batch 950 Loss 0.2620 Accuracy 0.1931\n",
      "Epoch 67 Batch 1000 Loss 0.2499 Accuracy 0.2033\n",
      "Epoch 67 Batch 1050 Loss 0.2387 Accuracy 0.2127\n",
      "Epoch 67 Batch 1100 Loss 0.2286 Accuracy 0.2211\n",
      "Epoch 67 Loss 0.2274 Accuracy 0.2220\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.7724 Accuracy 0.0206\n",
      "Epoch 68 Batch 50 Loss 0.9345 Accuracy 0.0215\n",
      "Epoch 68 Batch 100 Loss 0.9162 Accuracy 0.0261\n",
      "Epoch 68 Batch 150 Loss 0.8433 Accuracy 0.0310\n",
      "Epoch 68 Batch 200 Loss 0.7878 Accuracy 0.0359\n",
      "Epoch 68 Batch 250 Loss 0.7269 Accuracy 0.0419\n",
      "Epoch 68 Batch 300 Loss 0.6665 Accuracy 0.0482\n",
      "Epoch 68 Batch 350 Loss 0.6044 Accuracy 0.0551\n",
      "Epoch 68 Batch 400 Loss 0.5490 Accuracy 0.0627\n",
      "Epoch 68 Batch 450 Loss 0.5014 Accuracy 0.0708\n",
      "Epoch 68 Batch 500 Loss 0.4585 Accuracy 0.0802\n",
      "Epoch 68 Batch 550 Loss 0.4215 Accuracy 0.0901\n",
      "Epoch 68 Batch 600 Loss 0.3893 Accuracy 0.1013\n",
      "Epoch 68 Batch 650 Loss 0.3617 Accuracy 0.1143\n",
      "Epoch 68 Batch 700 Loss 0.3379 Accuracy 0.1274\n",
      "Epoch 68 Batch 750 Loss 0.3180 Accuracy 0.1399\n",
      "Epoch 68 Batch 800 Loss 0.3009 Accuracy 0.1542\n",
      "Epoch 68 Batch 850 Loss 0.2852 Accuracy 0.1688\n",
      "Epoch 68 Batch 900 Loss 0.2705 Accuracy 0.1816\n",
      "Epoch 68 Batch 950 Loss 0.2574 Accuracy 0.1928\n",
      "Epoch 68 Batch 1000 Loss 0.2454 Accuracy 0.2030\n",
      "Epoch 68 Batch 1050 Loss 0.2346 Accuracy 0.2123\n",
      "Epoch 68 Batch 1100 Loss 0.2247 Accuracy 0.2210\n",
      "Epoch 68 Loss 0.2236 Accuracy 0.2220\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.7824 Accuracy 0.0238\n",
      "Epoch 69 Batch 50 Loss 0.9196 Accuracy 0.0222\n",
      "Epoch 69 Batch 100 Loss 0.8913 Accuracy 0.0269\n",
      "Epoch 69 Batch 150 Loss 0.8329 Accuracy 0.0316\n",
      "Epoch 69 Batch 200 Loss 0.7793 Accuracy 0.0364\n",
      "Epoch 69 Batch 250 Loss 0.7092 Accuracy 0.0422\n",
      "Epoch 69 Batch 300 Loss 0.6505 Accuracy 0.0479\n",
      "Epoch 69 Batch 350 Loss 0.5961 Accuracy 0.0552\n",
      "Epoch 69 Batch 400 Loss 0.5445 Accuracy 0.0626\n",
      "Epoch 69 Batch 450 Loss 0.4968 Accuracy 0.0710\n",
      "Epoch 69 Batch 500 Loss 0.4554 Accuracy 0.0795\n",
      "Epoch 69 Batch 550 Loss 0.4188 Accuracy 0.0896\n",
      "Epoch 69 Batch 600 Loss 0.3878 Accuracy 0.1005\n",
      "Epoch 69 Batch 650 Loss 0.3599 Accuracy 0.1139\n",
      "Epoch 69 Batch 700 Loss 0.3359 Accuracy 0.1275\n",
      "Epoch 69 Batch 750 Loss 0.3160 Accuracy 0.1398\n",
      "Epoch 69 Batch 800 Loss 0.2990 Accuracy 0.1547\n",
      "Epoch 69 Batch 850 Loss 0.2832 Accuracy 0.1690\n",
      "Epoch 69 Batch 900 Loss 0.2687 Accuracy 0.1819\n",
      "Epoch 69 Batch 950 Loss 0.2556 Accuracy 0.1932\n",
      "Epoch 69 Batch 1000 Loss 0.2439 Accuracy 0.2036\n",
      "Epoch 69 Batch 1050 Loss 0.2329 Accuracy 0.2128\n",
      "Epoch 69 Batch 1100 Loss 0.2229 Accuracy 0.2213\n",
      "Epoch 69 Loss 0.2218 Accuracy 0.2220\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.6937 Accuracy 0.0197\n",
      "Epoch 70 Batch 50 Loss 1.1106 Accuracy 0.0210\n",
      "Epoch 70 Batch 100 Loss 1.0060 Accuracy 0.0263\n",
      "Epoch 70 Batch 150 Loss 0.9042 Accuracy 0.0312\n",
      "Epoch 70 Batch 200 Loss 0.8251 Accuracy 0.0360\n",
      "Epoch 70 Batch 250 Loss 0.7450 Accuracy 0.0420\n",
      "Epoch 70 Batch 300 Loss 0.6776 Accuracy 0.0482\n",
      "Epoch 70 Batch 350 Loss 0.6147 Accuracy 0.0553\n",
      "Epoch 70 Batch 400 Loss 0.5577 Accuracy 0.0628\n",
      "Epoch 70 Batch 450 Loss 0.5102 Accuracy 0.0709\n",
      "Epoch 70 Batch 500 Loss 0.4677 Accuracy 0.0799\n",
      "Epoch 70 Batch 550 Loss 0.4306 Accuracy 0.0897\n",
      "Epoch 70 Batch 600 Loss 0.3978 Accuracy 0.1007\n",
      "Epoch 70 Batch 650 Loss 0.3692 Accuracy 0.1140\n",
      "Epoch 70 Batch 700 Loss 0.3451 Accuracy 0.1270\n",
      "Epoch 70 Batch 750 Loss 0.3245 Accuracy 0.1391\n",
      "Epoch 70 Batch 800 Loss 0.3067 Accuracy 0.1536\n",
      "Epoch 70 Batch 850 Loss 0.2906 Accuracy 0.1683\n",
      "Epoch 70 Batch 900 Loss 0.2756 Accuracy 0.1811\n",
      "Epoch 70 Batch 950 Loss 0.2622 Accuracy 0.1929\n",
      "Epoch 70 Batch 1000 Loss 0.2501 Accuracy 0.2033\n",
      "Epoch 70 Batch 1050 Loss 0.2392 Accuracy 0.2126\n",
      "Epoch 70 Batch 1100 Loss 0.2292 Accuracy 0.2211\n",
      "Epoch 70 Loss 0.2280 Accuracy 0.2220\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.8605 Accuracy 0.0164\n",
      "Epoch 71 Batch 50 Loss 1.0824 Accuracy 0.0204\n",
      "Epoch 71 Batch 100 Loss 0.9953 Accuracy 0.0257\n",
      "Epoch 71 Batch 150 Loss 0.9106 Accuracy 0.0307\n",
      "Epoch 71 Batch 200 Loss 0.8248 Accuracy 0.0359\n",
      "Epoch 71 Batch 250 Loss 0.7522 Accuracy 0.0418\n",
      "Epoch 71 Batch 300 Loss 0.6872 Accuracy 0.0479\n",
      "Epoch 71 Batch 350 Loss 0.6242 Accuracy 0.0546\n",
      "Epoch 71 Batch 400 Loss 0.5677 Accuracy 0.0623\n",
      "Epoch 71 Batch 450 Loss 0.5164 Accuracy 0.0705\n",
      "Epoch 71 Batch 500 Loss 0.4739 Accuracy 0.0792\n",
      "Epoch 71 Batch 550 Loss 0.4360 Accuracy 0.0891\n",
      "Epoch 71 Batch 600 Loss 0.4027 Accuracy 0.1003\n",
      "Epoch 71 Batch 650 Loss 0.3741 Accuracy 0.1134\n",
      "Epoch 71 Batch 700 Loss 0.3495 Accuracy 0.1261\n",
      "Epoch 71 Batch 750 Loss 0.3290 Accuracy 0.1391\n",
      "Epoch 71 Batch 800 Loss 0.3111 Accuracy 0.1535\n",
      "Epoch 71 Batch 850 Loss 0.2943 Accuracy 0.1680\n",
      "Epoch 71 Batch 900 Loss 0.2792 Accuracy 0.1808\n",
      "Epoch 71 Batch 950 Loss 0.2655 Accuracy 0.1925\n",
      "Epoch 71 Batch 1000 Loss 0.2532 Accuracy 0.2032\n",
      "Epoch 71 Batch 1050 Loss 0.2419 Accuracy 0.2126\n",
      "Epoch 71 Batch 1100 Loss 0.2315 Accuracy 0.2211\n",
      "Epoch 71 Loss 0.2304 Accuracy 0.2220\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.8562 Accuracy 0.0197\n",
      "Epoch 72 Batch 50 Loss 0.9845 Accuracy 0.0215\n",
      "Epoch 72 Batch 100 Loss 0.9397 Accuracy 0.0256\n",
      "Epoch 72 Batch 150 Loss 0.8779 Accuracy 0.0310\n",
      "Epoch 72 Batch 200 Loss 0.8032 Accuracy 0.0358\n",
      "Epoch 72 Batch 250 Loss 0.7343 Accuracy 0.0417\n",
      "Epoch 72 Batch 300 Loss 0.6694 Accuracy 0.0480\n",
      "Epoch 72 Batch 350 Loss 0.6134 Accuracy 0.0546\n",
      "Epoch 72 Batch 400 Loss 0.5565 Accuracy 0.0621\n",
      "Epoch 72 Batch 450 Loss 0.5082 Accuracy 0.0704\n",
      "Epoch 72 Batch 500 Loss 0.4654 Accuracy 0.0796\n",
      "Epoch 72 Batch 550 Loss 0.4280 Accuracy 0.0896\n",
      "Epoch 72 Batch 600 Loss 0.3957 Accuracy 0.1006\n",
      "Epoch 72 Batch 650 Loss 0.3673 Accuracy 0.1136\n",
      "Epoch 72 Batch 700 Loss 0.3432 Accuracy 0.1271\n",
      "Epoch 72 Batch 750 Loss 0.3230 Accuracy 0.1398\n",
      "Epoch 72 Batch 800 Loss 0.3052 Accuracy 0.1540\n",
      "Epoch 72 Batch 850 Loss 0.2891 Accuracy 0.1686\n",
      "Epoch 72 Batch 900 Loss 0.2744 Accuracy 0.1812\n",
      "Epoch 72 Batch 950 Loss 0.2609 Accuracy 0.1930\n",
      "Epoch 72 Batch 1000 Loss 0.2487 Accuracy 0.2035\n",
      "Epoch 72 Batch 1050 Loss 0.2376 Accuracy 0.2127\n",
      "Epoch 72 Batch 1100 Loss 0.2275 Accuracy 0.2211\n",
      "Epoch 72 Loss 0.2264 Accuracy 0.2220\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.3695 Accuracy 0.0173\n",
      "Epoch 73 Batch 50 Loss 0.9312 Accuracy 0.0217\n",
      "Epoch 73 Batch 100 Loss 0.8964 Accuracy 0.0263\n",
      "Epoch 73 Batch 150 Loss 0.8432 Accuracy 0.0312\n",
      "Epoch 73 Batch 200 Loss 0.7742 Accuracy 0.0363\n",
      "Epoch 73 Batch 250 Loss 0.7116 Accuracy 0.0419\n",
      "Epoch 73 Batch 300 Loss 0.6442 Accuracy 0.0482\n",
      "Epoch 73 Batch 350 Loss 0.5887 Accuracy 0.0551\n",
      "Epoch 73 Batch 400 Loss 0.5350 Accuracy 0.0625\n",
      "Epoch 73 Batch 450 Loss 0.4885 Accuracy 0.0707\n",
      "Epoch 73 Batch 500 Loss 0.4469 Accuracy 0.0797\n",
      "Epoch 73 Batch 550 Loss 0.4109 Accuracy 0.0898\n",
      "Epoch 73 Batch 600 Loss 0.3799 Accuracy 0.1009\n",
      "Epoch 73 Batch 650 Loss 0.3527 Accuracy 0.1143\n",
      "Epoch 73 Batch 700 Loss 0.3298 Accuracy 0.1273\n",
      "Epoch 73 Batch 750 Loss 0.3102 Accuracy 0.1395\n",
      "Epoch 73 Batch 800 Loss 0.2932 Accuracy 0.1540\n",
      "Epoch 73 Batch 850 Loss 0.2775 Accuracy 0.1684\n",
      "Epoch 73 Batch 900 Loss 0.2634 Accuracy 0.1815\n",
      "Epoch 73 Batch 950 Loss 0.2507 Accuracy 0.1929\n",
      "Epoch 73 Batch 1000 Loss 0.2392 Accuracy 0.2030\n",
      "Epoch 73 Batch 1050 Loss 0.2286 Accuracy 0.2123\n",
      "Epoch 73 Batch 1100 Loss 0.2189 Accuracy 0.2212\n",
      "Epoch 73 Loss 0.2178 Accuracy 0.2221\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.7293 Accuracy 0.0197\n",
      "Epoch 74 Batch 50 Loss 1.0701 Accuracy 0.0208\n",
      "Epoch 74 Batch 100 Loss 1.0197 Accuracy 0.0254\n",
      "Epoch 74 Batch 150 Loss 0.8985 Accuracy 0.0308\n",
      "Epoch 74 Batch 200 Loss 0.8097 Accuracy 0.0361\n",
      "Epoch 74 Batch 250 Loss 0.7309 Accuracy 0.0423\n",
      "Epoch 74 Batch 300 Loss 0.6692 Accuracy 0.0483\n",
      "Epoch 74 Batch 350 Loss 0.6044 Accuracy 0.0555\n",
      "Epoch 74 Batch 400 Loss 0.5496 Accuracy 0.0630\n",
      "Epoch 74 Batch 450 Loss 0.5011 Accuracy 0.0710\n",
      "Epoch 74 Batch 500 Loss 0.4580 Accuracy 0.0798\n",
      "Epoch 74 Batch 550 Loss 0.4218 Accuracy 0.0897\n",
      "Epoch 74 Batch 600 Loss 0.3900 Accuracy 0.1006\n",
      "Epoch 74 Batch 650 Loss 0.3621 Accuracy 0.1139\n",
      "Epoch 74 Batch 700 Loss 0.3381 Accuracy 0.1270\n",
      "Epoch 74 Batch 750 Loss 0.3181 Accuracy 0.1396\n",
      "Epoch 74 Batch 800 Loss 0.3008 Accuracy 0.1541\n",
      "Epoch 74 Batch 850 Loss 0.2849 Accuracy 0.1684\n",
      "Epoch 74 Batch 900 Loss 0.2703 Accuracy 0.1815\n",
      "Epoch 74 Batch 950 Loss 0.2569 Accuracy 0.1932\n",
      "Epoch 74 Batch 1000 Loss 0.2451 Accuracy 0.2034\n",
      "Epoch 74 Batch 1050 Loss 0.2341 Accuracy 0.2126\n",
      "Epoch 74 Batch 1100 Loss 0.2243 Accuracy 0.2213\n",
      "Epoch 74 Loss 0.2231 Accuracy 0.2221\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.4038 Accuracy 0.0206\n",
      "Epoch 75 Batch 50 Loss 0.9715 Accuracy 0.0220\n",
      "Epoch 75 Batch 100 Loss 0.9132 Accuracy 0.0265\n",
      "Epoch 75 Batch 150 Loss 0.8372 Accuracy 0.0315\n",
      "Epoch 75 Batch 200 Loss 0.7759 Accuracy 0.0365\n",
      "Epoch 75 Batch 250 Loss 0.7048 Accuracy 0.0424\n",
      "Epoch 75 Batch 300 Loss 0.6415 Accuracy 0.0483\n",
      "Epoch 75 Batch 350 Loss 0.5831 Accuracy 0.0552\n",
      "Epoch 75 Batch 400 Loss 0.5304 Accuracy 0.0628\n",
      "Epoch 75 Batch 450 Loss 0.4843 Accuracy 0.0708\n",
      "Epoch 75 Batch 500 Loss 0.4429 Accuracy 0.0799\n",
      "Epoch 75 Batch 550 Loss 0.4072 Accuracy 0.0899\n",
      "Epoch 75 Batch 600 Loss 0.3769 Accuracy 0.1009\n",
      "Epoch 75 Batch 650 Loss 0.3499 Accuracy 0.1143\n",
      "Epoch 75 Batch 700 Loss 0.3267 Accuracy 0.1276\n",
      "Epoch 75 Batch 750 Loss 0.3075 Accuracy 0.1399\n",
      "Epoch 75 Batch 800 Loss 0.2907 Accuracy 0.1545\n",
      "Epoch 75 Batch 850 Loss 0.2756 Accuracy 0.1688\n",
      "Epoch 75 Batch 900 Loss 0.2617 Accuracy 0.1815\n",
      "Epoch 75 Batch 950 Loss 0.2490 Accuracy 0.1927\n",
      "Epoch 75 Batch 1000 Loss 0.2374 Accuracy 0.2030\n",
      "Epoch 75 Batch 1050 Loss 0.2269 Accuracy 0.2125\n",
      "Epoch 75 Batch 1100 Loss 0.2174 Accuracy 0.2213\n",
      "Epoch 75 Loss 0.2163 Accuracy 0.2221\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.9916 Accuracy 0.0164\n",
      "Epoch 76 Batch 50 Loss 0.9904 Accuracy 0.0210\n",
      "Epoch 76 Batch 100 Loss 0.9475 Accuracy 0.0260\n",
      "Epoch 76 Batch 150 Loss 0.8674 Accuracy 0.0307\n",
      "Epoch 76 Batch 200 Loss 0.8049 Accuracy 0.0358\n",
      "Epoch 76 Batch 250 Loss 0.7287 Accuracy 0.0416\n",
      "Epoch 76 Batch 300 Loss 0.6642 Accuracy 0.0481\n",
      "Epoch 76 Batch 350 Loss 0.6051 Accuracy 0.0551\n",
      "Epoch 76 Batch 400 Loss 0.5479 Accuracy 0.0628\n",
      "Epoch 76 Batch 450 Loss 0.5018 Accuracy 0.0708\n",
      "Epoch 76 Batch 500 Loss 0.4598 Accuracy 0.0796\n",
      "Epoch 76 Batch 550 Loss 0.4232 Accuracy 0.0895\n",
      "Epoch 76 Batch 600 Loss 0.3911 Accuracy 0.1005\n",
      "Epoch 76 Batch 650 Loss 0.3630 Accuracy 0.1135\n",
      "Epoch 76 Batch 700 Loss 0.3394 Accuracy 0.1266\n",
      "Epoch 76 Batch 750 Loss 0.3192 Accuracy 0.1390\n",
      "Epoch 76 Batch 800 Loss 0.3016 Accuracy 0.1538\n",
      "Epoch 76 Batch 850 Loss 0.2855 Accuracy 0.1682\n",
      "Epoch 76 Batch 900 Loss 0.2707 Accuracy 0.1812\n",
      "Epoch 76 Batch 950 Loss 0.2573 Accuracy 0.1932\n",
      "Epoch 76 Batch 1000 Loss 0.2454 Accuracy 0.2034\n",
      "Epoch 76 Batch 1050 Loss 0.2343 Accuracy 0.2128\n",
      "Epoch 76 Batch 1100 Loss 0.2245 Accuracy 0.2212\n",
      "Epoch 76 Loss 0.2234 Accuracy 0.2221\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.9678 Accuracy 0.0164\n",
      "Epoch 77 Batch 50 Loss 1.0080 Accuracy 0.0209\n",
      "Epoch 77 Batch 100 Loss 0.9340 Accuracy 0.0265\n",
      "Epoch 77 Batch 150 Loss 0.8531 Accuracy 0.0314\n",
      "Epoch 77 Batch 200 Loss 0.7855 Accuracy 0.0362\n",
      "Epoch 77 Batch 250 Loss 0.7136 Accuracy 0.0425\n",
      "Epoch 77 Batch 300 Loss 0.6523 Accuracy 0.0483\n",
      "Epoch 77 Batch 350 Loss 0.5922 Accuracy 0.0553\n",
      "Epoch 77 Batch 400 Loss 0.5404 Accuracy 0.0628\n",
      "Epoch 77 Batch 450 Loss 0.4914 Accuracy 0.0710\n",
      "Epoch 77 Batch 500 Loss 0.4501 Accuracy 0.0800\n",
      "Epoch 77 Batch 550 Loss 0.4140 Accuracy 0.0899\n",
      "Epoch 77 Batch 600 Loss 0.3833 Accuracy 0.1008\n",
      "Epoch 77 Batch 650 Loss 0.3555 Accuracy 0.1140\n",
      "Epoch 77 Batch 700 Loss 0.3323 Accuracy 0.1272\n",
      "Epoch 77 Batch 750 Loss 0.3126 Accuracy 0.1394\n",
      "Epoch 77 Batch 800 Loss 0.2953 Accuracy 0.1542\n",
      "Epoch 77 Batch 850 Loss 0.2795 Accuracy 0.1688\n",
      "Epoch 77 Batch 900 Loss 0.2650 Accuracy 0.1815\n",
      "Epoch 77 Batch 950 Loss 0.2521 Accuracy 0.1930\n",
      "Epoch 77 Batch 1000 Loss 0.2403 Accuracy 0.2034\n",
      "Epoch 77 Batch 1050 Loss 0.2297 Accuracy 0.2125\n",
      "Epoch 77 Batch 1100 Loss 0.2200 Accuracy 0.2213\n",
      "Epoch 77 Loss 0.2189 Accuracy 0.2221\n",
      "\n",
      "Epoch 78 Batch 0 Loss 1.2153 Accuracy 0.0156\n",
      "Epoch 78 Batch 50 Loss 0.9802 Accuracy 0.0209\n",
      "Epoch 78 Batch 100 Loss 0.9276 Accuracy 0.0260\n",
      "Epoch 78 Batch 150 Loss 0.8606 Accuracy 0.0309\n",
      "Epoch 78 Batch 200 Loss 0.7951 Accuracy 0.0356\n",
      "Epoch 78 Batch 250 Loss 0.7220 Accuracy 0.0416\n",
      "Epoch 78 Batch 300 Loss 0.6553 Accuracy 0.0479\n",
      "Epoch 78 Batch 350 Loss 0.5944 Accuracy 0.0548\n",
      "Epoch 78 Batch 400 Loss 0.5402 Accuracy 0.0624\n",
      "Epoch 78 Batch 450 Loss 0.4941 Accuracy 0.0706\n",
      "Epoch 78 Batch 500 Loss 0.4541 Accuracy 0.0795\n",
      "Epoch 78 Batch 550 Loss 0.4184 Accuracy 0.0892\n",
      "Epoch 78 Batch 600 Loss 0.3866 Accuracy 0.1005\n",
      "Epoch 78 Batch 650 Loss 0.3587 Accuracy 0.1134\n",
      "Epoch 78 Batch 700 Loss 0.3350 Accuracy 0.1270\n",
      "Epoch 78 Batch 750 Loss 0.3146 Accuracy 0.1395\n",
      "Epoch 78 Batch 800 Loss 0.2974 Accuracy 0.1539\n",
      "Epoch 78 Batch 850 Loss 0.2814 Accuracy 0.1687\n",
      "Epoch 78 Batch 900 Loss 0.2669 Accuracy 0.1817\n",
      "Epoch 78 Batch 950 Loss 0.2537 Accuracy 0.1937\n",
      "Epoch 78 Batch 1000 Loss 0.2418 Accuracy 0.2039\n",
      "Epoch 78 Batch 1050 Loss 0.2311 Accuracy 0.2130\n",
      "Epoch 78 Batch 1100 Loss 0.2212 Accuracy 0.2214\n",
      "Epoch 78 Loss 0.2201 Accuracy 0.2221\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.4844 Accuracy 0.0230\n",
      "Epoch 79 Batch 50 Loss 0.9743 Accuracy 0.0216\n",
      "Epoch 79 Batch 100 Loss 0.9347 Accuracy 0.0263\n",
      "Epoch 79 Batch 150 Loss 0.8500 Accuracy 0.0311\n",
      "Epoch 79 Batch 200 Loss 0.8044 Accuracy 0.0357\n",
      "Epoch 79 Batch 250 Loss 0.7310 Accuracy 0.0418\n",
      "Epoch 79 Batch 300 Loss 0.6630 Accuracy 0.0479\n",
      "Epoch 79 Batch 350 Loss 0.6037 Accuracy 0.0549\n",
      "Epoch 79 Batch 400 Loss 0.5493 Accuracy 0.0625\n",
      "Epoch 79 Batch 450 Loss 0.5013 Accuracy 0.0707\n",
      "Epoch 79 Batch 500 Loss 0.4582 Accuracy 0.0798\n",
      "Epoch 79 Batch 550 Loss 0.4215 Accuracy 0.0899\n",
      "Epoch 79 Batch 600 Loss 0.3891 Accuracy 0.1013\n",
      "Epoch 79 Batch 650 Loss 0.3608 Accuracy 0.1144\n",
      "Epoch 79 Batch 700 Loss 0.3373 Accuracy 0.1277\n",
      "Epoch 79 Batch 750 Loss 0.3176 Accuracy 0.1401\n",
      "Epoch 79 Batch 800 Loss 0.3003 Accuracy 0.1545\n",
      "Epoch 79 Batch 850 Loss 0.2846 Accuracy 0.1688\n",
      "Epoch 79 Batch 900 Loss 0.2701 Accuracy 0.1815\n",
      "Epoch 79 Batch 950 Loss 0.2571 Accuracy 0.1931\n",
      "Epoch 79 Batch 1000 Loss 0.2452 Accuracy 0.2036\n",
      "Epoch 79 Batch 1050 Loss 0.2343 Accuracy 0.2128\n",
      "Epoch 79 Batch 1100 Loss 0.2245 Accuracy 0.2212\n",
      "Epoch 79 Loss 0.2234 Accuracy 0.2220\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.8473 Accuracy 0.0206\n",
      "Epoch 80 Batch 50 Loss 1.0015 Accuracy 0.0212\n",
      "Epoch 80 Batch 100 Loss 0.9282 Accuracy 0.0261\n",
      "Epoch 80 Batch 150 Loss 0.8741 Accuracy 0.0306\n",
      "Epoch 80 Batch 200 Loss 0.8000 Accuracy 0.0358\n",
      "Epoch 80 Batch 250 Loss 0.7244 Accuracy 0.0417\n",
      "Epoch 80 Batch 300 Loss 0.6612 Accuracy 0.0480\n",
      "Epoch 80 Batch 350 Loss 0.6021 Accuracy 0.0546\n",
      "Epoch 80 Batch 400 Loss 0.5465 Accuracy 0.0624\n",
      "Epoch 80 Batch 450 Loss 0.4984 Accuracy 0.0705\n",
      "Epoch 80 Batch 500 Loss 0.4564 Accuracy 0.0795\n",
      "Epoch 80 Batch 550 Loss 0.4194 Accuracy 0.0895\n",
      "Epoch 80 Batch 600 Loss 0.3875 Accuracy 0.1005\n",
      "Epoch 80 Batch 650 Loss 0.3594 Accuracy 0.1135\n",
      "Epoch 80 Batch 700 Loss 0.3355 Accuracy 0.1269\n",
      "Epoch 80 Batch 750 Loss 0.3155 Accuracy 0.1393\n",
      "Epoch 80 Batch 800 Loss 0.2980 Accuracy 0.1541\n",
      "Epoch 80 Batch 850 Loss 0.2819 Accuracy 0.1684\n",
      "Epoch 80 Batch 900 Loss 0.2674 Accuracy 0.1815\n",
      "Epoch 80 Batch 950 Loss 0.2542 Accuracy 0.1930\n",
      "Epoch 80 Batch 1000 Loss 0.2423 Accuracy 0.2034\n",
      "Epoch 80 Batch 1050 Loss 0.2314 Accuracy 0.2127\n",
      "Epoch 80 Batch 1100 Loss 0.2217 Accuracy 0.2215\n",
      "Epoch 80 Loss 0.2206 Accuracy 0.2222\n",
      "\n",
      "Epoch 81 Batch 0 Loss 1.0651 Accuracy 0.0148\n",
      "Epoch 81 Batch 50 Loss 0.9879 Accuracy 0.0212\n",
      "Epoch 81 Batch 100 Loss 0.9335 Accuracy 0.0260\n",
      "Epoch 81 Batch 150 Loss 0.8468 Accuracy 0.0312\n",
      "Epoch 81 Batch 200 Loss 0.7666 Accuracy 0.0361\n",
      "Epoch 81 Batch 250 Loss 0.7071 Accuracy 0.0420\n",
      "Epoch 81 Batch 300 Loss 0.6437 Accuracy 0.0481\n",
      "Epoch 81 Batch 350 Loss 0.5846 Accuracy 0.0549\n",
      "Epoch 81 Batch 400 Loss 0.5317 Accuracy 0.0623\n",
      "Epoch 81 Batch 450 Loss 0.4862 Accuracy 0.0706\n",
      "Epoch 81 Batch 500 Loss 0.4468 Accuracy 0.0792\n",
      "Epoch 81 Batch 550 Loss 0.4110 Accuracy 0.0893\n",
      "Epoch 81 Batch 600 Loss 0.3797 Accuracy 0.1009\n",
      "Epoch 81 Batch 650 Loss 0.3528 Accuracy 0.1137\n",
      "Epoch 81 Batch 700 Loss 0.3298 Accuracy 0.1268\n",
      "Epoch 81 Batch 750 Loss 0.3102 Accuracy 0.1395\n",
      "Epoch 81 Batch 800 Loss 0.2934 Accuracy 0.1541\n",
      "Epoch 81 Batch 850 Loss 0.2778 Accuracy 0.1689\n",
      "Epoch 81 Batch 900 Loss 0.2635 Accuracy 0.1817\n",
      "Epoch 81 Batch 950 Loss 0.2506 Accuracy 0.1930\n",
      "Epoch 81 Batch 1000 Loss 0.2388 Accuracy 0.2038\n",
      "Epoch 81 Batch 1050 Loss 0.2282 Accuracy 0.2130\n",
      "Epoch 81 Batch 1100 Loss 0.2185 Accuracy 0.2213\n",
      "Epoch 81 Loss 0.2174 Accuracy 0.2222\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.9440 Accuracy 0.0156\n",
      "Epoch 82 Batch 50 Loss 0.9428 Accuracy 0.0212\n",
      "Epoch 82 Batch 100 Loss 0.9113 Accuracy 0.0262\n",
      "Epoch 82 Batch 150 Loss 0.8350 Accuracy 0.0312\n",
      "Epoch 82 Batch 200 Loss 0.7489 Accuracy 0.0362\n",
      "Epoch 82 Batch 250 Loss 0.6879 Accuracy 0.0422\n",
      "Epoch 82 Batch 300 Loss 0.6283 Accuracy 0.0484\n",
      "Epoch 82 Batch 350 Loss 0.5730 Accuracy 0.0556\n",
      "Epoch 82 Batch 400 Loss 0.5241 Accuracy 0.0630\n",
      "Epoch 82 Batch 450 Loss 0.4790 Accuracy 0.0710\n",
      "Epoch 82 Batch 500 Loss 0.4397 Accuracy 0.0799\n",
      "Epoch 82 Batch 550 Loss 0.4049 Accuracy 0.0898\n",
      "Epoch 82 Batch 600 Loss 0.3741 Accuracy 0.1010\n",
      "Epoch 82 Batch 650 Loss 0.3472 Accuracy 0.1140\n",
      "Epoch 82 Batch 700 Loss 0.3244 Accuracy 0.1273\n",
      "Epoch 82 Batch 750 Loss 0.3052 Accuracy 0.1395\n",
      "Epoch 82 Batch 800 Loss 0.2884 Accuracy 0.1539\n",
      "Epoch 82 Batch 850 Loss 0.2730 Accuracy 0.1684\n",
      "Epoch 82 Batch 900 Loss 0.2591 Accuracy 0.1814\n",
      "Epoch 82 Batch 950 Loss 0.2465 Accuracy 0.1927\n",
      "Epoch 82 Batch 1000 Loss 0.2349 Accuracy 0.2034\n",
      "Epoch 82 Batch 1050 Loss 0.2245 Accuracy 0.2128\n",
      "Epoch 82 Batch 1100 Loss 0.2149 Accuracy 0.2213\n",
      "Epoch 82 Loss 0.2138 Accuracy 0.2222\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.8514 Accuracy 0.0148\n",
      "Epoch 83 Batch 50 Loss 0.9561 Accuracy 0.0210\n",
      "Epoch 83 Batch 100 Loss 0.9412 Accuracy 0.0258\n",
      "Epoch 83 Batch 150 Loss 0.8477 Accuracy 0.0314\n",
      "Epoch 83 Batch 200 Loss 0.7735 Accuracy 0.0363\n",
      "Epoch 83 Batch 250 Loss 0.7048 Accuracy 0.0420\n",
      "Epoch 83 Batch 300 Loss 0.6411 Accuracy 0.0485\n",
      "Epoch 83 Batch 350 Loss 0.5808 Accuracy 0.0555\n",
      "Epoch 83 Batch 400 Loss 0.5275 Accuracy 0.0628\n",
      "Epoch 83 Batch 450 Loss 0.4818 Accuracy 0.0708\n",
      "Epoch 83 Batch 500 Loss 0.4408 Accuracy 0.0801\n",
      "Epoch 83 Batch 550 Loss 0.4054 Accuracy 0.0897\n",
      "Epoch 83 Batch 600 Loss 0.3748 Accuracy 0.1009\n",
      "Epoch 83 Batch 650 Loss 0.3479 Accuracy 0.1139\n",
      "Epoch 83 Batch 700 Loss 0.3253 Accuracy 0.1272\n",
      "Epoch 83 Batch 750 Loss 0.3061 Accuracy 0.1398\n",
      "Epoch 83 Batch 800 Loss 0.2891 Accuracy 0.1545\n",
      "Epoch 83 Batch 850 Loss 0.2740 Accuracy 0.1688\n",
      "Epoch 83 Batch 900 Loss 0.2600 Accuracy 0.1818\n",
      "Epoch 83 Batch 950 Loss 0.2472 Accuracy 0.1934\n",
      "Epoch 83 Batch 1000 Loss 0.2356 Accuracy 0.2036\n",
      "Epoch 83 Batch 1050 Loss 0.2252 Accuracy 0.2128\n",
      "Epoch 83 Batch 1100 Loss 0.2158 Accuracy 0.2214\n",
      "Epoch 83 Loss 0.2147 Accuracy 0.2222\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.8258 Accuracy 0.0173\n",
      "Epoch 84 Batch 50 Loss 0.9967 Accuracy 0.0213\n",
      "Epoch 84 Batch 100 Loss 0.9341 Accuracy 0.0262\n",
      "Epoch 84 Batch 150 Loss 0.8561 Accuracy 0.0310\n",
      "Epoch 84 Batch 200 Loss 0.7784 Accuracy 0.0361\n",
      "Epoch 84 Batch 250 Loss 0.7058 Accuracy 0.0419\n",
      "Epoch 84 Batch 300 Loss 0.6419 Accuracy 0.0480\n",
      "Epoch 84 Batch 350 Loss 0.5832 Accuracy 0.0548\n",
      "Epoch 84 Batch 400 Loss 0.5314 Accuracy 0.0625\n",
      "Epoch 84 Batch 450 Loss 0.4844 Accuracy 0.0705\n",
      "Epoch 84 Batch 500 Loss 0.4438 Accuracy 0.0797\n",
      "Epoch 84 Batch 550 Loss 0.4082 Accuracy 0.0898\n",
      "Epoch 84 Batch 600 Loss 0.3776 Accuracy 0.1006\n",
      "Epoch 84 Batch 650 Loss 0.3503 Accuracy 0.1142\n",
      "Epoch 84 Batch 700 Loss 0.3273 Accuracy 0.1272\n",
      "Epoch 84 Batch 750 Loss 0.3076 Accuracy 0.1400\n",
      "Epoch 84 Batch 800 Loss 0.2906 Accuracy 0.1541\n",
      "Epoch 84 Batch 850 Loss 0.2752 Accuracy 0.1684\n",
      "Epoch 84 Batch 900 Loss 0.2610 Accuracy 0.1813\n",
      "Epoch 84 Batch 950 Loss 0.2483 Accuracy 0.1931\n",
      "Epoch 84 Batch 1000 Loss 0.2367 Accuracy 0.2036\n",
      "Epoch 84 Batch 1050 Loss 0.2261 Accuracy 0.2130\n",
      "Epoch 84 Batch 1100 Loss 0.2165 Accuracy 0.2212\n",
      "Epoch 84 Loss 0.2154 Accuracy 0.2222\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.6547 Accuracy 0.0214\n",
      "Epoch 85 Batch 50 Loss 0.9761 Accuracy 0.0222\n",
      "Epoch 85 Batch 100 Loss 0.9176 Accuracy 0.0269\n",
      "Epoch 85 Batch 150 Loss 0.8332 Accuracy 0.0318\n",
      "Epoch 85 Batch 200 Loss 0.7619 Accuracy 0.0363\n",
      "Epoch 85 Batch 250 Loss 0.6964 Accuracy 0.0424\n",
      "Epoch 85 Batch 300 Loss 0.6347 Accuracy 0.0486\n",
      "Epoch 85 Batch 350 Loss 0.5776 Accuracy 0.0555\n",
      "Epoch 85 Batch 400 Loss 0.5284 Accuracy 0.0630\n",
      "Epoch 85 Batch 450 Loss 0.4824 Accuracy 0.0713\n",
      "Epoch 85 Batch 500 Loss 0.4418 Accuracy 0.0802\n",
      "Epoch 85 Batch 550 Loss 0.4073 Accuracy 0.0900\n",
      "Epoch 85 Batch 600 Loss 0.3768 Accuracy 0.1004\n",
      "Epoch 85 Batch 650 Loss 0.3501 Accuracy 0.1137\n",
      "Epoch 85 Batch 700 Loss 0.3275 Accuracy 0.1272\n",
      "Epoch 85 Batch 750 Loss 0.3082 Accuracy 0.1396\n",
      "Epoch 85 Batch 800 Loss 0.2916 Accuracy 0.1537\n",
      "Epoch 85 Batch 850 Loss 0.2763 Accuracy 0.1686\n",
      "Epoch 85 Batch 900 Loss 0.2623 Accuracy 0.1812\n",
      "Epoch 85 Batch 950 Loss 0.2495 Accuracy 0.1926\n",
      "Epoch 85 Batch 1000 Loss 0.2378 Accuracy 0.2032\n",
      "Epoch 85 Batch 1050 Loss 0.2273 Accuracy 0.2126\n",
      "Epoch 85 Batch 1100 Loss 0.2176 Accuracy 0.2212\n",
      "Epoch 85 Loss 0.2165 Accuracy 0.2221\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.2283 Accuracy 0.0238\n",
      "Epoch 86 Batch 50 Loss 0.9988 Accuracy 0.0213\n",
      "Epoch 86 Batch 100 Loss 0.8995 Accuracy 0.0265\n",
      "Epoch 86 Batch 150 Loss 0.8326 Accuracy 0.0313\n",
      "Epoch 86 Batch 200 Loss 0.7655 Accuracy 0.0360\n",
      "Epoch 86 Batch 250 Loss 0.6974 Accuracy 0.0421\n",
      "Epoch 86 Batch 300 Loss 0.6349 Accuracy 0.0482\n",
      "Epoch 86 Batch 350 Loss 0.5762 Accuracy 0.0556\n",
      "Epoch 86 Batch 400 Loss 0.5250 Accuracy 0.0629\n",
      "Epoch 86 Batch 450 Loss 0.4801 Accuracy 0.0710\n",
      "Epoch 86 Batch 500 Loss 0.4395 Accuracy 0.0800\n",
      "Epoch 86 Batch 550 Loss 0.4042 Accuracy 0.0899\n",
      "Epoch 86 Batch 600 Loss 0.3731 Accuracy 0.1013\n",
      "Epoch 86 Batch 650 Loss 0.3467 Accuracy 0.1142\n",
      "Epoch 86 Batch 700 Loss 0.3237 Accuracy 0.1273\n",
      "Epoch 86 Batch 750 Loss 0.3044 Accuracy 0.1399\n",
      "Epoch 86 Batch 800 Loss 0.2876 Accuracy 0.1542\n",
      "Epoch 86 Batch 850 Loss 0.2723 Accuracy 0.1687\n",
      "Epoch 86 Batch 900 Loss 0.2585 Accuracy 0.1815\n",
      "Epoch 86 Batch 950 Loss 0.2459 Accuracy 0.1931\n",
      "Epoch 86 Batch 1000 Loss 0.2346 Accuracy 0.2036\n",
      "Epoch 86 Batch 1050 Loss 0.2242 Accuracy 0.2128\n",
      "Epoch 86 Batch 1100 Loss 0.2145 Accuracy 0.2213\n",
      "Epoch 86 Loss 0.2134 Accuracy 0.2222\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.4761 Accuracy 0.0181\n",
      "Epoch 87 Batch 50 Loss 1.0250 Accuracy 0.0204\n",
      "Epoch 87 Batch 100 Loss 0.9488 Accuracy 0.0258\n",
      "Epoch 87 Batch 150 Loss 0.8545 Accuracy 0.0310\n",
      "Epoch 87 Batch 200 Loss 0.7700 Accuracy 0.0360\n",
      "Epoch 87 Batch 250 Loss 0.6958 Accuracy 0.0422\n",
      "Epoch 87 Batch 300 Loss 0.6298 Accuracy 0.0486\n",
      "Epoch 87 Batch 350 Loss 0.5726 Accuracy 0.0556\n",
      "Epoch 87 Batch 400 Loss 0.5196 Accuracy 0.0631\n",
      "Epoch 87 Batch 450 Loss 0.4750 Accuracy 0.0710\n",
      "Epoch 87 Batch 500 Loss 0.4349 Accuracy 0.0800\n",
      "Epoch 87 Batch 550 Loss 0.4005 Accuracy 0.0897\n",
      "Epoch 87 Batch 600 Loss 0.3709 Accuracy 0.1010\n",
      "Epoch 87 Batch 650 Loss 0.3444 Accuracy 0.1144\n",
      "Epoch 87 Batch 700 Loss 0.3220 Accuracy 0.1276\n",
      "Epoch 87 Batch 750 Loss 0.3027 Accuracy 0.1400\n",
      "Epoch 87 Batch 800 Loss 0.2863 Accuracy 0.1545\n",
      "Epoch 87 Batch 850 Loss 0.2710 Accuracy 0.1691\n",
      "Epoch 87 Batch 900 Loss 0.2570 Accuracy 0.1817\n",
      "Epoch 87 Batch 950 Loss 0.2445 Accuracy 0.1930\n",
      "Epoch 87 Batch 1000 Loss 0.2331 Accuracy 0.2035\n",
      "Epoch 87 Batch 1050 Loss 0.2227 Accuracy 0.2128\n",
      "Epoch 87 Batch 1100 Loss 0.2133 Accuracy 0.2214\n",
      "Epoch 87 Loss 0.2122 Accuracy 0.2222\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.7433 Accuracy 0.0206\n",
      "Epoch 88 Batch 50 Loss 0.9256 Accuracy 0.0214\n",
      "Epoch 88 Batch 100 Loss 0.8845 Accuracy 0.0265\n",
      "Epoch 88 Batch 150 Loss 0.8371 Accuracy 0.0313\n",
      "Epoch 88 Batch 200 Loss 0.7608 Accuracy 0.0366\n",
      "Epoch 88 Batch 250 Loss 0.6918 Accuracy 0.0422\n",
      "Epoch 88 Batch 300 Loss 0.6292 Accuracy 0.0483\n",
      "Epoch 88 Batch 350 Loss 0.5713 Accuracy 0.0553\n",
      "Epoch 88 Batch 400 Loss 0.5204 Accuracy 0.0628\n",
      "Epoch 88 Batch 450 Loss 0.4751 Accuracy 0.0708\n",
      "Epoch 88 Batch 500 Loss 0.4354 Accuracy 0.0798\n",
      "Epoch 88 Batch 550 Loss 0.4003 Accuracy 0.0899\n",
      "Epoch 88 Batch 600 Loss 0.3709 Accuracy 0.1009\n",
      "Epoch 88 Batch 650 Loss 0.3443 Accuracy 0.1139\n",
      "Epoch 88 Batch 700 Loss 0.3219 Accuracy 0.1272\n",
      "Epoch 88 Batch 750 Loss 0.3026 Accuracy 0.1396\n",
      "Epoch 88 Batch 800 Loss 0.2860 Accuracy 0.1544\n",
      "Epoch 88 Batch 850 Loss 0.2707 Accuracy 0.1691\n",
      "Epoch 88 Batch 900 Loss 0.2568 Accuracy 0.1820\n",
      "Epoch 88 Batch 950 Loss 0.2443 Accuracy 0.1935\n",
      "Epoch 88 Batch 1000 Loss 0.2329 Accuracy 0.2040\n",
      "Epoch 88 Batch 1050 Loss 0.2228 Accuracy 0.2130\n",
      "Epoch 88 Batch 1100 Loss 0.2132 Accuracy 0.2213\n",
      "Epoch 88 Loss 0.2121 Accuracy 0.2222\n",
      "\n",
      "Epoch 89 Batch 0 Loss 1.0870 Accuracy 0.0148\n",
      "Epoch 89 Batch 50 Loss 0.9258 Accuracy 0.0218\n",
      "Epoch 89 Batch 100 Loss 0.8864 Accuracy 0.0262\n",
      "Epoch 89 Batch 150 Loss 0.8163 Accuracy 0.0317\n",
      "Epoch 89 Batch 200 Loss 0.7398 Accuracy 0.0367\n",
      "Epoch 89 Batch 250 Loss 0.6771 Accuracy 0.0422\n",
      "Epoch 89 Batch 300 Loss 0.6192 Accuracy 0.0484\n",
      "Epoch 89 Batch 350 Loss 0.5647 Accuracy 0.0554\n",
      "Epoch 89 Batch 400 Loss 0.5128 Accuracy 0.0629\n",
      "Epoch 89 Batch 450 Loss 0.4662 Accuracy 0.0712\n",
      "Epoch 89 Batch 500 Loss 0.4280 Accuracy 0.0800\n",
      "Epoch 89 Batch 550 Loss 0.3945 Accuracy 0.0898\n",
      "Epoch 89 Batch 600 Loss 0.3648 Accuracy 0.1009\n",
      "Epoch 89 Batch 650 Loss 0.3390 Accuracy 0.1141\n",
      "Epoch 89 Batch 700 Loss 0.3167 Accuracy 0.1270\n",
      "Epoch 89 Batch 750 Loss 0.2978 Accuracy 0.1392\n",
      "Epoch 89 Batch 800 Loss 0.2814 Accuracy 0.1539\n",
      "Epoch 89 Batch 850 Loss 0.2664 Accuracy 0.1684\n",
      "Epoch 89 Batch 900 Loss 0.2529 Accuracy 0.1812\n",
      "Epoch 89 Batch 950 Loss 0.2406 Accuracy 0.1931\n",
      "Epoch 89 Batch 1000 Loss 0.2292 Accuracy 0.2032\n",
      "Epoch 89 Batch 1050 Loss 0.2191 Accuracy 0.2127\n",
      "Epoch 89 Batch 1100 Loss 0.2099 Accuracy 0.2213\n",
      "Epoch 89 Loss 0.2089 Accuracy 0.2222\n",
      "\n",
      "Epoch 90 Batch 0 Loss 1.1729 Accuracy 0.0123\n",
      "Epoch 90 Batch 50 Loss 0.9452 Accuracy 0.0215\n",
      "Epoch 90 Batch 100 Loss 0.8993 Accuracy 0.0259\n",
      "Epoch 90 Batch 150 Loss 0.8397 Accuracy 0.0305\n",
      "Epoch 90 Batch 200 Loss 0.7587 Accuracy 0.0360\n",
      "Epoch 90 Batch 250 Loss 0.6876 Accuracy 0.0421\n",
      "Epoch 90 Batch 300 Loss 0.6278 Accuracy 0.0483\n",
      "Epoch 90 Batch 350 Loss 0.5730 Accuracy 0.0553\n",
      "Epoch 90 Batch 400 Loss 0.5217 Accuracy 0.0627\n",
      "Epoch 90 Batch 450 Loss 0.4751 Accuracy 0.0710\n",
      "Epoch 90 Batch 500 Loss 0.4351 Accuracy 0.0799\n",
      "Epoch 90 Batch 550 Loss 0.4010 Accuracy 0.0900\n",
      "Epoch 90 Batch 600 Loss 0.3710 Accuracy 0.1012\n",
      "Epoch 90 Batch 650 Loss 0.3444 Accuracy 0.1139\n",
      "Epoch 90 Batch 700 Loss 0.3216 Accuracy 0.1272\n",
      "Epoch 90 Batch 750 Loss 0.3023 Accuracy 0.1399\n",
      "Epoch 90 Batch 800 Loss 0.2856 Accuracy 0.1544\n",
      "Epoch 90 Batch 850 Loss 0.2703 Accuracy 0.1694\n",
      "Epoch 90 Batch 900 Loss 0.2563 Accuracy 0.1820\n",
      "Epoch 90 Batch 950 Loss 0.2438 Accuracy 0.1937\n",
      "Epoch 90 Batch 1000 Loss 0.2325 Accuracy 0.2038\n",
      "Epoch 90 Batch 1050 Loss 0.2222 Accuracy 0.2132\n",
      "Epoch 90 Batch 1100 Loss 0.2128 Accuracy 0.2213\n",
      "Epoch 90 Loss 0.2117 Accuracy 0.2222\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.3793 Accuracy 0.0189\n",
      "Epoch 91 Batch 50 Loss 0.9219 Accuracy 0.0207\n",
      "Epoch 91 Batch 100 Loss 0.8892 Accuracy 0.0263\n",
      "Epoch 91 Batch 150 Loss 0.8118 Accuracy 0.0311\n",
      "Epoch 91 Batch 200 Loss 0.7478 Accuracy 0.0362\n",
      "Epoch 91 Batch 250 Loss 0.6853 Accuracy 0.0418\n",
      "Epoch 91 Batch 300 Loss 0.6208 Accuracy 0.0483\n",
      "Epoch 91 Batch 350 Loss 0.5633 Accuracy 0.0552\n",
      "Epoch 91 Batch 400 Loss 0.5113 Accuracy 0.0629\n",
      "Epoch 91 Batch 450 Loss 0.4666 Accuracy 0.0710\n",
      "Epoch 91 Batch 500 Loss 0.4283 Accuracy 0.0797\n",
      "Epoch 91 Batch 550 Loss 0.3941 Accuracy 0.0895\n",
      "Epoch 91 Batch 600 Loss 0.3647 Accuracy 0.1005\n",
      "Epoch 91 Batch 650 Loss 0.3384 Accuracy 0.1134\n",
      "Epoch 91 Batch 700 Loss 0.3159 Accuracy 0.1268\n",
      "Epoch 91 Batch 750 Loss 0.2973 Accuracy 0.1392\n",
      "Epoch 91 Batch 800 Loss 0.2807 Accuracy 0.1535\n",
      "Epoch 91 Batch 850 Loss 0.2658 Accuracy 0.1680\n",
      "Epoch 91 Batch 900 Loss 0.2520 Accuracy 0.1809\n",
      "Epoch 91 Batch 950 Loss 0.2397 Accuracy 0.1927\n",
      "Epoch 91 Batch 1000 Loss 0.2284 Accuracy 0.2032\n",
      "Epoch 91 Batch 1050 Loss 0.2183 Accuracy 0.2127\n",
      "Epoch 91 Batch 1100 Loss 0.2091 Accuracy 0.2215\n",
      "Epoch 91 Loss 0.2080 Accuracy 0.2223\n",
      "\n",
      "Epoch 92 Batch 0 Loss 1.1260 Accuracy 0.0173\n",
      "Epoch 92 Batch 50 Loss 0.8740 Accuracy 0.0222\n",
      "Epoch 92 Batch 100 Loss 0.8742 Accuracy 0.0266\n",
      "Epoch 92 Batch 150 Loss 0.8067 Accuracy 0.0313\n",
      "Epoch 92 Batch 200 Loss 0.7360 Accuracy 0.0364\n",
      "Epoch 92 Batch 250 Loss 0.6671 Accuracy 0.0422\n",
      "Epoch 92 Batch 300 Loss 0.6063 Accuracy 0.0485\n",
      "Epoch 92 Batch 350 Loss 0.5531 Accuracy 0.0553\n",
      "Epoch 92 Batch 400 Loss 0.5050 Accuracy 0.0626\n",
      "Epoch 92 Batch 450 Loss 0.4622 Accuracy 0.0708\n",
      "Epoch 92 Batch 500 Loss 0.4254 Accuracy 0.0797\n",
      "Epoch 92 Batch 550 Loss 0.3910 Accuracy 0.0899\n",
      "Epoch 92 Batch 600 Loss 0.3621 Accuracy 0.1013\n",
      "Epoch 92 Batch 650 Loss 0.3368 Accuracy 0.1141\n",
      "Epoch 92 Batch 700 Loss 0.3145 Accuracy 0.1276\n",
      "Epoch 92 Batch 750 Loss 0.2959 Accuracy 0.1398\n",
      "Epoch 92 Batch 800 Loss 0.2799 Accuracy 0.1541\n",
      "Epoch 92 Batch 850 Loss 0.2652 Accuracy 0.1687\n",
      "Epoch 92 Batch 900 Loss 0.2515 Accuracy 0.1817\n",
      "Epoch 92 Batch 950 Loss 0.2391 Accuracy 0.1932\n",
      "Epoch 92 Batch 1000 Loss 0.2280 Accuracy 0.2036\n",
      "Epoch 92 Batch 1050 Loss 0.2178 Accuracy 0.2130\n",
      "Epoch 92 Batch 1100 Loss 0.2086 Accuracy 0.2215\n",
      "Epoch 92 Loss 0.2076 Accuracy 0.2222\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.4904 Accuracy 0.0189\n",
      "Epoch 93 Batch 50 Loss 0.9597 Accuracy 0.0212\n",
      "Epoch 93 Batch 100 Loss 0.9112 Accuracy 0.0263\n",
      "Epoch 93 Batch 150 Loss 0.8312 Accuracy 0.0313\n",
      "Epoch 93 Batch 200 Loss 0.7522 Accuracy 0.0365\n",
      "Epoch 93 Batch 250 Loss 0.6774 Accuracy 0.0422\n",
      "Epoch 93 Batch 300 Loss 0.6203 Accuracy 0.0485\n",
      "Epoch 93 Batch 350 Loss 0.5622 Accuracy 0.0554\n",
      "Epoch 93 Batch 400 Loss 0.5123 Accuracy 0.0627\n",
      "Epoch 93 Batch 450 Loss 0.4663 Accuracy 0.0708\n",
      "Epoch 93 Batch 500 Loss 0.4264 Accuracy 0.0797\n",
      "Epoch 93 Batch 550 Loss 0.3922 Accuracy 0.0900\n",
      "Epoch 93 Batch 600 Loss 0.3632 Accuracy 0.1010\n",
      "Epoch 93 Batch 650 Loss 0.3371 Accuracy 0.1143\n",
      "Epoch 93 Batch 700 Loss 0.3150 Accuracy 0.1275\n",
      "Epoch 93 Batch 750 Loss 0.2962 Accuracy 0.1400\n",
      "Epoch 93 Batch 800 Loss 0.2800 Accuracy 0.1543\n",
      "Epoch 93 Batch 850 Loss 0.2651 Accuracy 0.1687\n",
      "Epoch 93 Batch 900 Loss 0.2517 Accuracy 0.1814\n",
      "Epoch 93 Batch 950 Loss 0.2395 Accuracy 0.1930\n",
      "Epoch 93 Batch 1000 Loss 0.2285 Accuracy 0.2036\n",
      "Epoch 93 Batch 1050 Loss 0.2184 Accuracy 0.2129\n",
      "Epoch 93 Batch 1100 Loss 0.2093 Accuracy 0.2215\n",
      "Epoch 93 Loss 0.2083 Accuracy 0.2222\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.4148 Accuracy 0.0206\n",
      "Epoch 94 Batch 50 Loss 0.9631 Accuracy 0.0214\n",
      "Epoch 94 Batch 100 Loss 0.9083 Accuracy 0.0262\n",
      "Epoch 94 Batch 150 Loss 0.8301 Accuracy 0.0313\n",
      "Epoch 94 Batch 200 Loss 0.7620 Accuracy 0.0362\n",
      "Epoch 94 Batch 250 Loss 0.6962 Accuracy 0.0419\n",
      "Epoch 94 Batch 300 Loss 0.6347 Accuracy 0.0481\n",
      "Epoch 94 Batch 350 Loss 0.5768 Accuracy 0.0551\n",
      "Epoch 94 Batch 400 Loss 0.5246 Accuracy 0.0625\n",
      "Epoch 94 Batch 450 Loss 0.4801 Accuracy 0.0705\n",
      "Epoch 94 Batch 500 Loss 0.4401 Accuracy 0.0799\n",
      "Epoch 94 Batch 550 Loss 0.4055 Accuracy 0.0898\n",
      "Epoch 94 Batch 600 Loss 0.3753 Accuracy 0.1011\n",
      "Epoch 94 Batch 650 Loss 0.3484 Accuracy 0.1144\n",
      "Epoch 94 Batch 700 Loss 0.3254 Accuracy 0.1278\n",
      "Epoch 94 Batch 750 Loss 0.3060 Accuracy 0.1402\n",
      "Epoch 94 Batch 800 Loss 0.2890 Accuracy 0.1549\n",
      "Epoch 94 Batch 850 Loss 0.2736 Accuracy 0.1691\n",
      "Epoch 94 Batch 900 Loss 0.2597 Accuracy 0.1823\n",
      "Epoch 94 Batch 950 Loss 0.2468 Accuracy 0.1942\n",
      "Epoch 94 Batch 1000 Loss 0.2353 Accuracy 0.2042\n",
      "Epoch 94 Batch 1050 Loss 0.2249 Accuracy 0.2129\n",
      "Epoch 94 Batch 1100 Loss 0.2153 Accuracy 0.2213\n",
      "Epoch 94 Loss 0.2142 Accuracy 0.2222\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.9398 Accuracy 0.0156\n",
      "Epoch 95 Batch 50 Loss 0.8818 Accuracy 0.0214\n",
      "Epoch 95 Batch 100 Loss 0.8355 Accuracy 0.0261\n",
      "Epoch 95 Batch 150 Loss 0.7665 Accuracy 0.0316\n",
      "Epoch 95 Batch 200 Loss 0.7160 Accuracy 0.0364\n",
      "Epoch 95 Batch 250 Loss 0.6621 Accuracy 0.0420\n",
      "Epoch 95 Batch 300 Loss 0.6039 Accuracy 0.0484\n",
      "Epoch 95 Batch 350 Loss 0.5527 Accuracy 0.0554\n",
      "Epoch 95 Batch 400 Loss 0.5050 Accuracy 0.0628\n",
      "Epoch 95 Batch 450 Loss 0.4614 Accuracy 0.0708\n",
      "Epoch 95 Batch 500 Loss 0.4240 Accuracy 0.0793\n",
      "Epoch 95 Batch 550 Loss 0.3904 Accuracy 0.0894\n",
      "Epoch 95 Batch 600 Loss 0.3613 Accuracy 0.1005\n",
      "Epoch 95 Batch 650 Loss 0.3355 Accuracy 0.1132\n",
      "Epoch 95 Batch 700 Loss 0.3135 Accuracy 0.1262\n",
      "Epoch 95 Batch 750 Loss 0.2947 Accuracy 0.1391\n",
      "Epoch 95 Batch 800 Loss 0.2784 Accuracy 0.1538\n",
      "Epoch 95 Batch 850 Loss 0.2636 Accuracy 0.1684\n",
      "Epoch 95 Batch 900 Loss 0.2500 Accuracy 0.1812\n",
      "Epoch 95 Batch 950 Loss 0.2376 Accuracy 0.1931\n",
      "Epoch 95 Batch 1000 Loss 0.2265 Accuracy 0.2037\n",
      "Epoch 95 Batch 1050 Loss 0.2165 Accuracy 0.2129\n",
      "Epoch 95 Batch 1100 Loss 0.2074 Accuracy 0.2213\n",
      "Epoch 95 Loss 0.2063 Accuracy 0.2222\n",
      "\n",
      "Epoch 96 Batch 0 Loss 1.2049 Accuracy 0.0123\n",
      "Epoch 96 Batch 50 Loss 0.9847 Accuracy 0.0211\n",
      "Epoch 96 Batch 100 Loss 0.9226 Accuracy 0.0257\n",
      "Epoch 96 Batch 150 Loss 0.8222 Accuracy 0.0309\n",
      "Epoch 96 Batch 200 Loss 0.7686 Accuracy 0.0358\n",
      "Epoch 96 Batch 250 Loss 0.6980 Accuracy 0.0419\n",
      "Epoch 96 Batch 300 Loss 0.6337 Accuracy 0.0480\n",
      "Epoch 96 Batch 350 Loss 0.5778 Accuracy 0.0549\n",
      "Epoch 96 Batch 400 Loss 0.5244 Accuracy 0.0626\n",
      "Epoch 96 Batch 450 Loss 0.4780 Accuracy 0.0711\n",
      "Epoch 96 Batch 500 Loss 0.4389 Accuracy 0.0802\n",
      "Epoch 96 Batch 550 Loss 0.4044 Accuracy 0.0898\n",
      "Epoch 96 Batch 600 Loss 0.3738 Accuracy 0.1007\n",
      "Epoch 96 Batch 650 Loss 0.3469 Accuracy 0.1139\n",
      "Epoch 96 Batch 700 Loss 0.3242 Accuracy 0.1272\n",
      "Epoch 96 Batch 750 Loss 0.3049 Accuracy 0.1398\n",
      "Epoch 96 Batch 800 Loss 0.2881 Accuracy 0.1546\n",
      "Epoch 96 Batch 850 Loss 0.2728 Accuracy 0.1690\n",
      "Epoch 96 Batch 900 Loss 0.2586 Accuracy 0.1816\n",
      "Epoch 96 Batch 950 Loss 0.2459 Accuracy 0.1932\n",
      "Epoch 96 Batch 1000 Loss 0.2345 Accuracy 0.2036\n",
      "Epoch 96 Batch 1050 Loss 0.2240 Accuracy 0.2128\n",
      "Epoch 96 Batch 1100 Loss 0.2146 Accuracy 0.2214\n",
      "Epoch 96 Loss 0.2135 Accuracy 0.2222\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.3791 Accuracy 0.0255\n",
      "Epoch 97 Batch 50 Loss 0.9535 Accuracy 0.0221\n",
      "Epoch 97 Batch 100 Loss 0.9055 Accuracy 0.0265\n",
      "Epoch 97 Batch 150 Loss 0.8163 Accuracy 0.0315\n",
      "Epoch 97 Batch 200 Loss 0.7516 Accuracy 0.0360\n",
      "Epoch 97 Batch 250 Loss 0.6816 Accuracy 0.0419\n",
      "Epoch 97 Batch 300 Loss 0.6206 Accuracy 0.0481\n",
      "Epoch 97 Batch 350 Loss 0.5660 Accuracy 0.0549\n",
      "Epoch 97 Batch 400 Loss 0.5153 Accuracy 0.0625\n",
      "Epoch 97 Batch 450 Loss 0.4699 Accuracy 0.0707\n",
      "Epoch 97 Batch 500 Loss 0.4302 Accuracy 0.0795\n",
      "Epoch 97 Batch 550 Loss 0.3961 Accuracy 0.0893\n",
      "Epoch 97 Batch 600 Loss 0.3661 Accuracy 0.1003\n",
      "Epoch 97 Batch 650 Loss 0.3396 Accuracy 0.1133\n",
      "Epoch 97 Batch 700 Loss 0.3171 Accuracy 0.1264\n",
      "Epoch 97 Batch 750 Loss 0.2977 Accuracy 0.1390\n",
      "Epoch 97 Batch 800 Loss 0.2813 Accuracy 0.1537\n",
      "Epoch 97 Batch 850 Loss 0.2661 Accuracy 0.1689\n",
      "Epoch 97 Batch 900 Loss 0.2524 Accuracy 0.1818\n",
      "Epoch 97 Batch 950 Loss 0.2400 Accuracy 0.1933\n",
      "Epoch 97 Batch 1000 Loss 0.2286 Accuracy 0.2036\n",
      "Epoch 97 Batch 1050 Loss 0.2184 Accuracy 0.2131\n",
      "Epoch 97 Batch 1100 Loss 0.2092 Accuracy 0.2215\n",
      "Epoch 97 Loss 0.2081 Accuracy 0.2223\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.7215 Accuracy 0.0230\n",
      "Epoch 98 Batch 50 Loss 0.9815 Accuracy 0.0218\n",
      "Epoch 98 Batch 100 Loss 0.8974 Accuracy 0.0263\n",
      "Epoch 98 Batch 150 Loss 0.8237 Accuracy 0.0316\n",
      "Epoch 98 Batch 200 Loss 0.7430 Accuracy 0.0368\n",
      "Epoch 98 Batch 250 Loss 0.6798 Accuracy 0.0427\n",
      "Epoch 98 Batch 300 Loss 0.6169 Accuracy 0.0489\n",
      "Epoch 98 Batch 350 Loss 0.5618 Accuracy 0.0557\n",
      "Epoch 98 Batch 400 Loss 0.5094 Accuracy 0.0633\n",
      "Epoch 98 Batch 450 Loss 0.4647 Accuracy 0.0716\n",
      "Epoch 98 Batch 500 Loss 0.4250 Accuracy 0.0804\n",
      "Epoch 98 Batch 550 Loss 0.3909 Accuracy 0.0900\n",
      "Epoch 98 Batch 600 Loss 0.3616 Accuracy 0.1010\n",
      "Epoch 98 Batch 650 Loss 0.3354 Accuracy 0.1144\n",
      "Epoch 98 Batch 700 Loss 0.3135 Accuracy 0.1279\n",
      "Epoch 98 Batch 750 Loss 0.2947 Accuracy 0.1403\n",
      "Epoch 98 Batch 800 Loss 0.2784 Accuracy 0.1546\n",
      "Epoch 98 Batch 850 Loss 0.2639 Accuracy 0.1689\n",
      "Epoch 98 Batch 900 Loss 0.2502 Accuracy 0.1813\n",
      "Epoch 98 Batch 950 Loss 0.2381 Accuracy 0.1930\n",
      "Epoch 98 Batch 1000 Loss 0.2269 Accuracy 0.2032\n",
      "Epoch 98 Batch 1050 Loss 0.2168 Accuracy 0.2129\n",
      "Epoch 98 Batch 1100 Loss 0.2076 Accuracy 0.2214\n",
      "Epoch 98 Loss 0.2065 Accuracy 0.2223\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.7335 Accuracy 0.0214\n",
      "Epoch 99 Batch 50 Loss 0.9376 Accuracy 0.0211\n",
      "Epoch 99 Batch 100 Loss 0.8615 Accuracy 0.0267\n",
      "Epoch 99 Batch 150 Loss 0.7840 Accuracy 0.0318\n",
      "Epoch 99 Batch 200 Loss 0.7215 Accuracy 0.0368\n",
      "Epoch 99 Batch 250 Loss 0.6626 Accuracy 0.0427\n",
      "Epoch 99 Batch 300 Loss 0.6037 Accuracy 0.0488\n",
      "Epoch 99 Batch 350 Loss 0.5482 Accuracy 0.0557\n",
      "Epoch 99 Batch 400 Loss 0.4990 Accuracy 0.0631\n",
      "Epoch 99 Batch 450 Loss 0.4542 Accuracy 0.0715\n",
      "Epoch 99 Batch 500 Loss 0.4163 Accuracy 0.0806\n",
      "Epoch 99 Batch 550 Loss 0.3832 Accuracy 0.0903\n",
      "Epoch 99 Batch 600 Loss 0.3549 Accuracy 0.1013\n",
      "Epoch 99 Batch 650 Loss 0.3300 Accuracy 0.1143\n",
      "Epoch 99 Batch 700 Loss 0.3083 Accuracy 0.1277\n",
      "Epoch 99 Batch 750 Loss 0.2899 Accuracy 0.1405\n",
      "Epoch 99 Batch 800 Loss 0.2739 Accuracy 0.1553\n",
      "Epoch 99 Batch 850 Loss 0.2595 Accuracy 0.1694\n",
      "Epoch 99 Batch 900 Loss 0.2462 Accuracy 0.1824\n",
      "Epoch 99 Batch 950 Loss 0.2339 Accuracy 0.1939\n",
      "Epoch 99 Batch 1000 Loss 0.2231 Accuracy 0.2039\n",
      "Epoch 99 Batch 1050 Loss 0.2131 Accuracy 0.2132\n",
      "Epoch 99 Batch 1100 Loss 0.2041 Accuracy 0.2215\n",
      "Epoch 99 Loss 0.2031 Accuracy 0.2223\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.9084 Accuracy 0.0189\n",
      "Epoch 100 Batch 50 Loss 0.9063 Accuracy 0.0217\n",
      "Epoch 100 Batch 100 Loss 0.8849 Accuracy 0.0264\n",
      "Epoch 100 Batch 150 Loss 0.8208 Accuracy 0.0311\n",
      "Epoch 100 Batch 200 Loss 0.7465 Accuracy 0.0363\n",
      "Epoch 100 Batch 250 Loss 0.6787 Accuracy 0.0423\n",
      "Epoch 100 Batch 300 Loss 0.6190 Accuracy 0.0484\n",
      "Epoch 100 Batch 350 Loss 0.5613 Accuracy 0.0555\n",
      "Epoch 100 Batch 400 Loss 0.5089 Accuracy 0.0632\n",
      "Epoch 100 Batch 450 Loss 0.4638 Accuracy 0.0712\n",
      "Epoch 100 Batch 500 Loss 0.4255 Accuracy 0.0801\n",
      "Epoch 100 Batch 550 Loss 0.3917 Accuracy 0.0898\n",
      "Epoch 100 Batch 600 Loss 0.3625 Accuracy 0.1006\n",
      "Epoch 100 Batch 650 Loss 0.3364 Accuracy 0.1138\n",
      "Epoch 100 Batch 700 Loss 0.3144 Accuracy 0.1269\n",
      "Epoch 100 Batch 750 Loss 0.2954 Accuracy 0.1395\n",
      "Epoch 100 Batch 800 Loss 0.2794 Accuracy 0.1542\n",
      "Epoch 100 Batch 850 Loss 0.2646 Accuracy 0.1689\n",
      "Epoch 100 Batch 900 Loss 0.2511 Accuracy 0.1819\n",
      "Epoch 100 Batch 950 Loss 0.2388 Accuracy 0.1933\n",
      "Epoch 100 Batch 1000 Loss 0.2275 Accuracy 0.2035\n",
      "Epoch 100 Batch 1050 Loss 0.2174 Accuracy 0.2127\n",
      "Epoch 100 Batch 1100 Loss 0.2081 Accuracy 0.2214\n",
      "Saved weights for epoch 100 at weights/transformer_weights_epoch_100.weights.h5\n",
      "Epoch 100 Loss 0.2070 Accuracy 0.2223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        enc_input = inp['inputs']\n",
    "        target = tar['outputs']\n",
    "        train_step(enc_input, target)\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    # Save model weights every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        weights_path = f'weights/transformer_weights_epoch_{epoch+1}.weights.h5'\n",
    "        transformer.save_weights(weights_path)\n",
    "        print(f'Saved weights for epoch {epoch+1} at {weights_path}')\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804896d-08fa-4c10-bf80-7459352fd1b6",
   "metadata": {},
   "source": [
    "* As you can see from above that training is completed\n",
    "* Now let's write a code to load saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "527cd195-cc15-4936-b8dd-59c294ecb897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built successfully!\n"
     ]
    }
   ],
   "source": [
    "# Build the model by calling it once with sample data\n",
    "sample_input = tf.zeros((1, 20), dtype=tf.int32)\n",
    "sample_target = tf.zeros((1, 19), dtype=tf.int32)\n",
    "\n",
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(sample_input, sample_target)\n",
    "\n",
    "# This actually builds the model\n",
    "_ = transformer(\n",
    "    sample_input, \n",
    "    sample_target, \n",
    "    training=False,\n",
    "    enc_padding_mask=enc_padding_mask,\n",
    "    look_ahead_mask=combined_mask,\n",
    "    dec_padding_mask=dec_padding_mask\n",
    ")\n",
    "\n",
    "print(\"Model built successfully!\")\n",
    "\n",
    "transformer.load_weights(f'./weights/transformer_weights_epoch_{EPOCHS}.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26812675-458a-4407-bdfe-614decb2ac95",
   "metadata": {},
   "source": [
    "* Now let's write a function for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68e266f8-34a3-4f27-9390-0ab2abd16ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    # Encoding input\n",
    "    encoded = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    encoder_input = tf.expand_dims([START_TOKEN] + encoded + [END_TOKEN], axis=0)\n",
    "    # Starting decoder with start token\n",
    "    decoder_input = tf.expand_dims([START_TOKEN], 0)\n",
    "    # Generating tokens one by one\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # Creating masks\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, decoder_input\n",
    "        )\n",
    "        # Getting predictions\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            decoder_input,\n",
    "            training=False,\n",
    "            enc_padding_mask=enc_padding_mask,\n",
    "            look_ahead_mask=combined_mask,\n",
    "            dec_padding_mask=dec_padding_mask\n",
    "        )\n",
    "        # Getting last positions predictions\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        # Stopping if end token\n",
    "        if tf.equal(predicted_id, END_TOKEN):\n",
    "            break\n",
    "        # Adding predicted token to decoder input\n",
    "        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n",
    "    return tf.squeeze(decoder_input, axis=0), attention_weights\n",
    "\n",
    "def predict(sentence):\n",
    "    # Geting prediction\n",
    "    prediction, attention_weights = evaluate(sentence)\n",
    "    # Filtering out special tokens\n",
    "    sequence = [i for i in prediction.numpy().tolist() \n",
    "                if i != START_TOKEN and i != END_TOKEN and i > 0]\n",
    "    # Converting to text\n",
    "    predicted_sentence = tokenizer.sequences_to_texts([sequence])[0]\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "    return predicted_sentence, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87e920-c953-44c2-a69d-c025be980214",
   "metadata": {},
   "source": [
    "* Let's now test the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6782ca43-b2dd-4df0-bfad-617b88f1e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: There is no purer love than the love of a student for their professor\n",
      "Output: is no love than the love of a dateless lively heat nor my five no longer than thy love\n"
     ]
    }
   ],
   "source": [
    "output, attention = predict(\"There is no purer love than the love of a student for their professor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12bd30-cf0a-4d3c-85c8-b2be0749abd7",
   "metadata": {},
   "source": [
    "* As you can see from above our prediction function is working as expected\n",
    "* Now let's write a function to generate poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46022eef-a57f-46ed-a13c-1798c9025ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(seed_text, num_lines=4, words_per_line=8):\n",
    "    \"\"\"Generate poem given text\"\"\"\n",
    "    poem_lines = []\n",
    "    current_text = seed_text\n",
    "    for line_num in range(num_lines):\n",
    "        # Generating next sequence\n",
    "        predicted, _ = evaluate(current_text)\n",
    "        # Converting to words\n",
    "        sequence = [i for i in predicted.numpy().tolist() \n",
    "                    if i != START_TOKEN and i != END_TOKEN and i > 0]\n",
    "        predicted_words = tokenizer.sequences_to_texts([sequence])[0]\n",
    "        words = predicted_words.split()\n",
    "        # Taking words for this line\n",
    "        line_words = words[:words_per_line]\n",
    "        line = ' '.join(line_words)\n",
    "        poem_lines.append(line)\n",
    "        # Using end of this line as seed for next line\n",
    "        if len(words) >= 3:\n",
    "            current_text = ' '.join(words[-3:])  # Last 3 words as new seed\n",
    "        else:\n",
    "            current_text = predicted_words\n",
    "    poem = '\\n'.join(poem_lines)\n",
    "    return poem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18605a77-13ba-44ac-aa81-575d0220f53e",
   "metadata": {},
   "source": [
    "* Let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "886a0496-96ba-4711-a197-395ecdc34180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i compare thee\n",
      "thee\n",
      "thee how to have years be new\n",
      "be new made when a new pride\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem(\"shall i compare thee\", num_lines=4, words_per_line=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
